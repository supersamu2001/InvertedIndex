30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation Franklin Zhang Bellevue College Bellevue, WA, USA franklin.zhang@bellevuecollege.edu Sonya Zhang Eastside Preparatory School Kirkland, WA, USA szhang@eastsideprep.org Alon Halevy Google Cloud Mountain View, CA, USA alonhalevy@gmail.com Abstract In this paper, we present 30 Day Me, a habit- formation application that leverages Large Lan- guage Models (LLMs) to help users break down their goals into manageable, actionable steps and track their progress. Central to the app is the 30DAYGEN system, which gener- ates 3,531 unique 30-day challenges sourced from over 15K webpages, and enables run- time search of challenge ideas aligned with user-defined goals. We showcase how LLMs can be harnessed to rapidly construct domain- specific content corpora for behavioral and educational purposes, and propose a practi- cal pipeline that incorporates effective LLM- enhanced approaches for content generation and semantic deduplication. 1 Introduction Repetition is the key to mastery. Each time we practice, we add another layer of myelin, making the pathway more efficient. - Dr. George Bartzokis, UCLA Neuropsychiatrist We developed 30 Day Me, a mobile app de- signed to help people achieve their long-term goals by turning them into manageable 30-day chal- lenges. Each challenge begins with a personal goal, or “wish,” and pairs it with a simple daily action that consistently moves the user closer to that goal. The app helps users stay motivated and accountable by tracking their progress over time (see screenshot in Figure 1(a)). By focusing on daily, achievable steps, 30 Day Me promotes habit formation, skill development, and sustained per- sonal growth through incremental progress. A successful 30-day challenge hinges on hav- ing a clear and effective action plan that guides progress toward a meaningful goal. Research in goal-setting has shown that structured approaches, such as SMART goals,1 lead to better outcomes 1https://www.samhsa.gov/sites/default/files/ nc-smart-goals-fact-sheet.pdf compared to vague or unstructured intentions. However, despite their benefits, crafting well- defined SMART goals can be difficult and time- consuming for many users. To help users identify a SMART action plan, we built 30DAYGEN, a search engine that recom- mends 30-day challenges based on a user’s goal. In the offline stage, 30DAYGEN constructs a corpus of 30-day challenges. We started with posing 25 web search queries asking for a diverse assortment of challenges, and ultimately extracting and gener- ated 3,531 unique challenges from online blogs and articles. At runtime, when a user submits a wish, the system searches this corpus and suggests chal- lenges that are most likely to help users progress towards their goal. (see screenshot in Figure 1(b)). In this process, LLMs have played a crucial role across all components of the system, transforming traditionally labor-intensive and error-prone tasks into more tractable issues, for better collection and search. First, initial web search with 25 queries resulted in 14,746 unique webpages, containing a signifi- cant amount of noise, where manual review can be tedius. We employed LLM-based URL-level filtering to rapidly assess the likelihood that a page contains valid 30-day challenge ideas, which effec- tively selected 953 promising articles, and achieved a filter precision of 94%. Second, different webpages structure challenges in various ways and traditional information extrac- tion methods (Chang et al., 2006) can fall short. We invoked an LLM to easily extract 11,792 chal- lenges (average 12.4 per page) from the 953 pages, and generated the wishes and daily actions from original content following a defined schema. Third, there is excessive content overlap across different web sources and it requires a nuanced understanding of action plans to identify duplicates. We prompted LLMs to consider challenges that suggest mostly similar daily actions as duplicates. arXiv:2505.02851v1  [cs.CL]  2 May 2025 Figure 1: 30 Day Me provides progress tracking and runtime challenge search, exhibiting high search quality (overall hit@3=85%). Specifically, we reduced the challenge set from 11,792 to 3,531 unique challenges, achieving a F- measure of 0.890. Finally, at runtime, the typical retrieval-then- ranking 2-step search pipeline is inadequate to ensure that the returned challenges are perfectly aligned. We invoke an LLM to validate that top recommendations are helpful to the user’s wish, adding a layer of intelligent filtering. To the best of our knowledge, this is the first paper to describe the use of LLMs for building a content corpus specifically aimed at habit forma- tion. We make three contributions. 1. We developed 30 Day Me, an app that sup- ports habit formation and goal achievement by breaking down long-term ambitions into man- ageable 30-day challenges. The app can be accessed at https://30day.me, and has both individual users and school users for educa- tional purposes, receiving feedback like "this app has really helped me become more con- scious and intentional about my behavior," "being able to see my completion rate at a glance is incredibly motivating." 2. At the core of the app is the 30DAYGEN system, which constructs a corpus of 3,531 challenges sourced from online webpages and provides a search interface to recom- mend relevant challenges to users with 85% hit@3 (Figure 1(c)). We detail the end-to-end pipeline for collecting, formalizing, cleaning, and deduplicating challenges from the web. 3. We demonstrate that with minimal effort, LLMs can be leveraged to rapidly construct a high-quality content corpus. We built 30DAY- GEN—including data collection, indexing, and search—in under two weeks. We hope our experience can offer practical insights for building similar educational systems. 2 Related Work LLMs in education: LLMs have seen wide adop- tion across a variety of educational applications. In behavioral education, there has been a growing body of research exploring how LLMs can act as coaches to facilitate lifestyle changes. LLMs incor- porate established behavioral science frameworks (like COM-B (Michie et al., 2011)) to improve physical activity coaching through methods such as prompt priming and dialogue re-ranking, aim- ing to enhance coach empathy and effectiveness (Hegde et al., 2024). Beyond behavioral coaching, LLMs have also been used to power conversational intelligent tutoring systems (ITS) (Liu et al., 2025) for providing adaptive and personalized learning at scale. Whereas the aforementioned systems focus on user dialog to promote changes, 30DAYGEN harnesses LLMs from a content generation and search perspective. Knowledge extraction: The practice of automat- ically building specialized text corpora from the web is well-established (Kilgarriff and Grefenstette, 2003; Gatto, 2014), with early tools like WebBoot- Cat (Baroni and Bernardini, 2004; Symseridou, 2018) demonstrating automated collection, albeit often requiring significant post-processing or re- lying on keyword limitations, and later practice of Knowledge Extraction (Weikum and Theobald, 2010), targeting rigid (subject, predicate, object) triples (Dong, 2023). LLMs significantly en- hance this process, enabling the integration of unstructured and diverse web data into a desired schema (Wu et al., 2024; Dagdelen et al., 2024). Our approach, 30DAYGEN, applies this web- harvesting concept to the specific domain of 30- day challenges to extract less rigidly defined con- cepts like a ’wish’ and the associated ’daily action’. LLMs allow us to perform this extraction, plus gen- eration and formatting based on the core challenge actions, effectively transforming unstructured web content into a structured challenge corpus. Data integration: Another related area is Data Integration, combining data from different sources to form a unified corpus (Dong and Srivastava, 2015; Golshan et al., 2017). Key issues addressed include schema heterogeneity through schema map- ping, entity heterogeneity through entity linkage, and value heterogeneity through data fusion. Entity linkage is critical to 30DAYGEN (Dong and Srivas- tava, 2015). The LLM enhances entity resolution using semantic comparisons, which is shown to significantly increase performance over traditional string matching methods (Peeters et al., 2024). We proposed a unique deduplication pipeline that relies on embedding similarity for initial matching and leverages LLMs to refine the results for difficult pairs. 3 Overview 3.1 Problem definition We start by describing what is a 30-day challenge. A 30-day challenge is an action plan for a change that a user wishes to make to their lives everyday during a month-long period. It consists of two components: the wish and the daily action. The wish sets the goal that a user wishes to achieve (e.g., "feel less stressed"); the daily action suggests what one shall conduct to progress towards the goal (e.g., "meditate for 5 minutes daily"). We develop the 30DAYGEN system that helps users find 30-day challenges relevant to their goals. The system takes a user query Q describing their wish, searches a ChallengeDB corpus with web- sourced challenges, and outputs a list of challenges C, best suited to the user wish. Consider someone who struggles with low energy levels throughout the day and wishes to be more energetic. The top-2 suggestions are shown as examples in Figure 1. A critical step in building the 30DAYGEN sys- tem is to populate ChallengeDB with high-quality challenges. We build ChallengeDB by extracting 30-day challenge ideas from resources available on the web. 3.2 30DAYGEN architecture We now describe the overall architecture of our system, depicted in Figure 2. Our system has two parts: the offline system generates 30-day chal- lenges by leveraging ideas from the web, and uses them to populate the ChallengeDB; the runtime search system takes the user’s wish and searches the ChallengeDB to suggest challenges. The offline system is responsible for generating the challenge idea corpus. It has four components. First, the Web Collection component collects a set of web-pages containing concrete suggestions for 30-day challenges; for example, Appendix B shows a blog with 101 30-day challenges for self-care. Second, the Challenge Generation component ex- tracts ideas from the scraped pages, generating and formatting challenges in the required format. Third, the deduplication component identifies and elimi- nates challenges deemed duplicates or too similar to another. Finally, the Indexing component popu- lates the ChallengeDB with generated challenges and their semantic embedding representations. The Runtime Search system takes a user query and returns a list of relevant 30-day challenges. It has 3 key components. First, the Input Encoding component creates a semantic representation of the user’s input. Next, the Challenge Retrieval compo- nent matches the query embedding and challenge embeddings to identify relevant challenges. Finally, the Ranking and Validation component validates relevance of retrieved results and re-ranks them accordingly. 4 Offline Challenge Generation In this section, we describe each component of the offline pipeline in detail: web collection, challenge generation, deduplication, and indexing. 4.1 Web Collection The goal of the Web Collection component is to find a broad selection of blog and article URLs that suggest quality 30-day challenges. We look for web pages meeting the following criteria. 1) Pages should contain specific 30-day challenges rather than high-level promotion or discussing the usefulness of habit formation. 2) Pages must pro- vide challenges with repeatable, daily action items. 3) Pages should offer challenges that represent a diverse assortment of potential user wishes and appeal to a wide range of interests. Our approach to Web Collection consists of three Figure 2: A high-level overview of the system architecture, illustrating the key components and their interactions. steps: Composing search queries, collecting the web-pages from the searches, and filtering pages that fail our criteria. Step 1. Search query composition. We tested a variety of search queries suggested by GPT-4o and identified 25 unique ones that are effective for finding 30-day challenges: 11 general and 14 tailored to specific themes. The full list of queries is included in Appendix C. Step 2. Search result collection: We com- piled search results using Bright Data’s SERP API (Search Engine Results Page) to collect 25,000 web-pages (500 from each query), resulting in 14,746 unique results after deduplication. Step 3. Webpage filtering: The filter compo- nent takes each search result and decides whether it provides useful 30-day challenges following the criteria outlined above. We first remove blocked base domains including social media sites (e.g. YouTube, Pinterest) and popular e-commerce sites (e.g. Amazon, eBay), hard to be used for 30-day challenges (complete list of blocked base domains in Appendix D). We then prompted Google’s Gem- ini 2.0 Flash with in-context learning to determine a Likelihood Score between 0–10 that signifies the probability that a webpage is useful. The com- ponent resulted with 953 URLs. Prompt in Ap- pendix E. 4.2 Challenge Idea Generation This component receives an input list of URLs and outputs a structured list of challenges with generated wish and daily action fields parsed from the webpage content. We prompt LLMs to conduct step-by-step challenge generation. Step 1. Scraping and parsing: First, we used Puppeteer2 to crawl and retrieve the HTML of the pages, then scraped and parsed each web-page to obtain the text content from the page. To mini- mize token inputs when extracting challenges, we removed header and footer elements and saved the rest of the text content. Step 2. Challenge extraction: Next, we invoked Gemini 2.0 Flash to analyze each article, empha- sizing two important tasks: 1) extract the exact text from article content to formulate challenge titles and descriptions; 2) create a 30-day challenge with the wish and daily action. (Prompt in Appendix F) 4.3 Deduplication A major challenge with retrieving challenges from various web-pages and blogs is that many articles share overlapping ideas in their content. The dedu- plication component receives a set of challenges C, identifies and removes duplicates, and outputs a list of deduplicated challenges Cu prioritizing ideas with better descriptions. We begin by defining what constitutes a dupli- cate. Challenges that suggest largely similar daily actions are considered duplicates. For example, "cooking a new meal every day" and "trying a new recipe every day" are duplicates because they both essentially propose cooking something new daily. We aim to optimize for well-balanced performance on two key metrics: 1) Precision, which is the percentage of challenges removed that are in fact 2https://pptr.dev/ duplicates. 2) Recall, the percentage of duplicates that are identified and removed. In the literature (Christen, 2012) deduplication typically proceeds in three stages. First, Block- ing groups items using fast, approximate similarity comparison to identify potential duplicates with high recall. Second, Matching analyzes every pair in the same block to detect duplicates. Fi- nally, Clustering groups matched pairs into clusters, where each cluster represents a unique entity. We transform the standard framework into a four- step system. As the number of challenges is rela- tively small, and embedding-based similarity com- parison is efficient at this scale, we bypass the tra- ditional Blocking stage. Our approach focuses on refining the Matching stage. First, we preliminarily filter our challenge idea dataset to remove obvi- ous duplicates. Second, we utilize FAISS (Johnson et al., 2019), an indexing library that provides fast similarity search for high-dimensional vectors, to find duplicate-pair candidates based on embedding similarity. Third, we analyze moderate-confidence pairs using an LLM to accurately determine similar- ity. These three steps are effectively integrated into a comprehensive pairwise matching process. Fi- nally, we perform correlation clustering to finalize a deduplicated collection of 30-day challenges. Step 1: Preliminary filtering We first eliminate obvious duplicates. This step helps save computa- tional costs in subsequent steps while using min- imal resources. We focus on the title and daily action fields in this step because of their consis- tent format and significant role in our duplicate definition. We normalize titles and daily actions and remove stop-words. We compile the stop-word list by using a combination of preexisting lists and extensive custom entries such as 30day, challenge, day, improve, etc. Step 2: Pair-wise similarity computation Next, we compute pairwise similarity scores to identify potential near-duplicate challenge pairs. We first convert challenges into their vector representation. Since daily action is the primary factor in determin- ing duplicates, we use OpenAI’s text-embedding-3- large model with the daily action of each challenge as input to generate embeddings. We then find vector similarity for every possi- ble pair within the challenge list. We use FAISS’s IndexFlatL2, as its brute-force search approach pro- vides the highest possible accuracy with sufficient efficiency for relatively small datasets. Step 3: LLM-based matching With a list of po- tential challenge idea duplicates, we sample pairs in each similarity range and manually examine the percentage of true duplicates. Accordingly we de- cide a high threshold and a low threshold, divid- ing the pairs into three segments: pairs above the high threshold are considered matches, pairs below the low threshold are considered non-matches, and pairs between the thresholds undergo LLM-based matching for further evaluation. We prompted Google’s Gemini 2.0 Flash model and its ability to understand the nuance of action plans to accurately determine whether a pair is a duplicate. (Prompt in Appendix G) Step 4: Clustering Using the map of all high- accuracy pairs, we cluster similar challenges to- gether to co-locate duplicates and create a dedupli- cated list of challenges. Our intuition is that imperfect duplicate determi- nation means that similarity is not transitive. For example, if A and B are a pair, and B and C are a pair, it is entirely possible for A and C to be insuf- ficiently similar. Ideally, we would utilize correla- tion clustering to create optimal groups given pair- wise constraints. However, correlation clustering is an NP-complete problem (Bansal et al., 2004). We therefore employ a greedy algorithm as an approx- imation. Each challenge idea is treated as a node, evaluated sequentially to determine whether to join a cluster or to create its own individual cluster. This decision is made using the following criteria: if a challenge idea does not match at least half of the nodes in any existing cluster, it is allocated into a new cluster; otherwise, the node is inserted into the cluster with which it has the highest number of matches. Finally, With duplicate challenges co-located within their own clusters, we choose one idea from each cluster to create a challenge list. For single- item clusters, that challenge idea is selected. For clusters with multiple ideas, we prioritize the chal- lenge idea with the longest description, with the assumption that its detail makes it more specific and usable. 4.4 Indexing The Indexing component is responsible for insert- ing generated challenges into the ChallengeDB, providing access to the Runtime Search suite. We reuse the embeddings and the index generated for deduplication purpose, and upload challenges to the ChallengeDB. 5 Runtime Search The runtime system enables users to discover chal- lenges tailored to their personal goals. Given a user’s input wish, the system retrieves and ranks relevant 30-day challenges from the ChallengeDB. It does so by encoding the input query, performing a semantic similarity search against stored chal- lenge embeddings, and then ranking and validating the retrieved results. In this section, we detail the key components of the runtime system. 5.1 Challenge Retrieval The Challenge Retrieval component finds chal- lenges most semantically similar to a user wish. It takes a user query as input and outputs a list of potential challenge idea results. We do this in two steps: 1. Input encoding: We first encode the user’s wish into a vector representation using Ope- nAI’s text-embedding-3-large model. 2. Similarity search: We use cosine-similarity to find top-k challenges most semantically similar with user input, querying Chal- lengeDB with the input embedding. 5.2 Rerank While cosine-similarity retrieval effectively iden- tifies challenges semantically similar to a user’s input, it lacks the precision needed for accurate relevance-based ranking. To refine the results, the Rerank component leverages the bge-reranker-v2- m3 model via a Pinecone API endpoint. This model receives the daily action field of candidate chal- lenges and returns a re-ranked list sorted by textual relevance to the user’s query. 5.3 Validation The goal of the Validation component is to finalize and refine challenges recommended to the user. Determining best-fit challenges based on a user’s wish requires a nuanced understanding of the effect of various lifestyle adjustments. By themselves, the Challenge Retrieval and Rerank components can sometimes provide irrelevant suggestions that are ineffective for helping a user accomplish their wish. We notice this occurring in two conditions: 1. Insufficient data in ChallengeDB: The Chal- lenge Database lacks generated challenge idea data relevant to the user’s expressed desire. For example, a user wishes to "prepare for the SAT." However, no challenges in the sources used to generate the ChallengeDB pertain to standardized testing improvement. Conse- quently, related but unhelpful challenges such as "Sit at the breakfast table" or "Make a wish everyday" are returned. 2. Intentional contradiction despite thematic overlap: The challenge idea and the user’s wish share a thematic overlap, but directly contradict each other in terms of intent. For example, a user wants to "wake up feeling re- freshed in the morning." The challenge "Wake up 30 minutes earlier" might exhibit high vec- tor proximity due to the shared theme of wak- ing up. However, this challenge suffers in utility because waking up earlier will likely worsen fatigue, directly conflicting with the user’s desire to feel refreshed. We thus utilize the Validation component, lever- aging an LLM which can better assess user intent and accurately verify relevance of challenge idea suggestions. We again use Google’s Gemini 2.0 Flash model to analyze the retrieved challenges and remove those which are irrelevant. (Prompt in Ap- pendix H) 6 Benchmarks and Experiments Setup Our 30DayGen system gathers challenge ideas through 25 search queries, resulting in 14,746 unique webpages. LLM URL filtering narrows this to 953 promising articles and blogs, which are then crawled to extract an initial 11,792 formatted challenge ideas. Finally, a four-step deduplication process refines this list in less than 15 minutes, gen- erating a finalized set of 3,531 unique challenge ideas. To evaluate the quality of our 30-day challenge corpus, as well as the end-to-end search perfor- mance, we conducted experiments to answer two questions: 1. Q1: How well does 30DayGen retrieve rel- evant and helpful challenges based on user wish input? 2. Q2: Can we effectively leverage LLMs to identify duplicate challenges to build a high- quality challenge corpus? 6.1 Challenge search 6.1.1 Search Evaluation Setup Query set: We prompted Gemini 2.5 Pro Exp to compose a list of 100 search queries, comprising of 30 general wishes like "I want to boost my en- ergy levels," 40 fairly specific wishes like "I want to stay properly hydrated," and 30 ultra-specific wishes like "I want to be able to hold a plank for 2 minutes." Ground truth answers: We queried Challenge DB with an embedded representation of each of the generated queries and compiled 50 of the most semantically similar challenges. We then manually reviewed these potential answers for each query, marking each result as correct (relevant and helpful) and incorrect, thereby forming a ground truth of highly relevant challenges. Metrics: We evaluate the ranking of our search results through five metrics. (1) NDCG (Järvelin and Kekäläinen, 2002) evaluates the relevance and ranking of the results, weighing correct results at the front of compiled answers more heavily than those at the end: DCG measures the correctness of a document based on its position in the result list; NDCG is the ratio of DCG to the best possi- ble DCG with perfect ranking, otherwise known as iDCG. (2) Hit@3: Percentage of queries with at least 1 correct answer in its top 3 output chal- lenges. (3) Precision@K: Percentage of output challenges that are correct in the top-K search re- sults. We measure Precision@3 and Precision@20. (4) Recall@K: Percentage of correct results that are included in top-K search results. We measure Recall@3 and Recall@20. (5) F1-measure@K: Harmonic mean between precision and recall. We compute it using the equation: F1 = 2·Precision·Recall Precision+Recall Methods: We compare our search solution with an ablation without LLM filtering that decides use- fulness of a challenge to achieve the user’s wish. 6.1.2 Evaluation results Table 1 shows our experimental results for Runtime Search system benchmarks and Appendix A shows the PR-curves. We have three observations: 1. Our results demonstrate effective perfor- mance, with Hit@3 results of 0.848 overall, Table 1: Runtime Search Performance: Metrics with and without LLM Filtering Metric Overall General Fairly Ultra Specific Specific Hit@3 0.848 0.983 0.900 0.644 - Filtering 0.818 0.983 0.862 0.594 Precision@3 0.770 0.977 0.866 0.433 - Filtering 0.740 0.977 0.833 0.377 Recall@3 0.778 0.977 0.858 0.472 - Filtering 0.763 0.977 0.833 0.455 F-msr@3 0.774 0.977 0.862 0.452 - Filtering 0.751 0.977 0.833 0.412 Precision@20 0.738 0.970 0.832 0.380 - Filtering 0.652 0.965 0.745 0.216 Recall@20 0.721 0.916 0.752 0.484 - Filtering 0.821 0.965 0.826 0.671 F-msr@20 0.729 0.946 0.790 0.426 - Filtering 0.727 0.965 0.783 0.327 NDCG 0.797 0.970 0.852 0.551 - Filtering 0.774 0.966 0.814 0.530 meaning for 85% of questions we show rele- vant and helpful challenges in top-3 results, in particular high for general questions (0.983) and fairly specific questions (0.900). Addi- tionally, we have high precision and recall for top-3 results, and even reasonable precision and recall for top-20 results. 2. The model performance generally worsens as queries grow in specificity. We can see this with our NDCG values in particular, as the score drops from a high 0.970 to a medium 0.852 and to a mere 0.551, from general to fairly specific to ultra specific queries. This is due to the small number of challenges suitable for a narrow topic within our Challenge DB corpus. However, we still maintain a 0.644 Hit@3 score for ultra specific queries, show- ing that even for highly specific queries we still return helpful suggestions for two thirds of the questions. 3. Finally, we note that our solution utilizing LLM-filtering outperforms the ablated solu- tion without it in all holistic categories. The only regression is recall@20, where the LLM- filtering may be aggressive in filtering and hurts recall; however, it still exhibits higher F-measure on more specific questions by re- moving unhelpful search results. 6.2 Challenge corpus construction 6.2.1 Evaluation Setup Metrics: We compute the precision and recall of our challenge deduplication component. • Precision: Precision computes the percentage of removed challenges that are indeed dupli- cates. We randomly sampled 100 removed challenges, identified the representative chal- lenge that remained in the final corpus, and manually annotated if they are duplicates. • Recall: Recall computes the percentage of du- plicate challenges that are removed. We ran- domly sampled 100 challenges that remained after the deduplication process, and decided if it is a duplicate as follows: for each chal- lenge, we found the top-5 similar challenges based on embedding similarity from the origi- nal challenge list; we then manually decided if any of these challenges is a duplicate of the ex- amined challenge. Let m be the percentage of challenges that have an unremoved duplicate. We compute recall as: prec · removed% prec · removed% + m · (1 −removed%) Methods: We compared our deduplication solu- tion with the following methods. • Our solution: Vector pairwise matching, re- finement with LLM matching for pairs where the similarities falling between 0.625 (low threshold) and 0.7 (high threshold), followed by correlation clustering. • Baseline 1: MinHash pairwise matching3, fol- lowed by transitive-closure clustering. • Baseline 2: Vector pairwise matching with threshold 0.7 (high threshold), followed by transitive-closure clustering. • Ablation 1: Remove LLM refinement step, in- stead only performing vector pairwise match- ing and correlation clustering. • Ablation 2: Replace correlation clustering with transitive-closure clustering. 6.2.2 Deduplication Performance Table 2 shows our experimental results for the Deduplication pipeline. We have made the fol- lowing observations. (1) Our deduplication shows high effectiveness on our corpus, achieving a pre- cision of 93% and a recall of 85% for removing duplicate challenges. It significantly outperforms 3https://pypi.org/project/datasketch/ Table 2: Deduplication Performance. Our solution ob- tains the highest F1-Score. Method Precision Recall F1-Score MinHash baseline 0.970 0.688 0.805 Vector-sim baseline 0.100 0.999 0.182 Our Solution 0.930 0.853 0.890 - LLM matching 0.740 0.685 0.711 - Correlation Clus. 0.040 0.999 0.077 baseline solutions (by 8% and 71%) and ablated solutions (by 18% and 81%), showing importance of each component in our deduplication solution. (2) MinHash based on syntactic string matching achieves the highest precision (0.970), but is too conservative in removing duplicates, thus obtains a very low recall (0.688). (3) Transitive closure clustering, used in Vector-sim baseline and Abla- tion w/o correlation clustering, suffers from poor precision. This confirms that similarity between challenges is not transitive and demonstrates im- portance of a more sophisticated clustering method, like correlation clustering. (4) The LLM-matching step significantly improves pairwise matching, in- creased precision by 19% and recall by 17%. URL filtering: Finally, we evaluate the accuracy of our LLM webpage filtering, which reduced an initial set of 14,746 unique pages to only 953 pages. We randomly sampled 100 removed webpages and manually checked if each page contains valid 30- day challenge ideas. Our filtering precision is as high as 94%, showing effectiveness of our LLM- based filtering. 7 Conclusion In this paper, we presented 30DAYGEN, a novel system that leverages LLMs to efficiently construct a specialized content corpus for habit formation. Our system successfully processed 14,746 web- pages, harvested 3,531 unique, high-quality chal- lenges, and provided search over the corpus with high quality (hit@3=85%). Our 30DAYGEN sys- tem demonstrates how LLMs can enhance the de- velopment of content-rich educational and behav- ioral applications, shedding light on efficiently cre- ating similar data corpora. Beyond web-based corpus creation, there are many possibilities to further leverage LLMs to im- prove the app and conduct future research. First, how to make the app more conversational and invit- ing. Second, how to adapt the challenges to be more tailored to specific users. Finally, how to suggest new and finer-granular day-by-day ideas. Limitations 30DAYGEN offers a promising approach to gen- erating 30-day habit formation challenges and in- cludes a search function for user personalization. However, several limitations should be noted: • Fixed challenge structure: The system is designed exclusively for challenges requiring the repetition of a single daily action for 30 days. It cannot generate or support challenges with varying day-by-day plans. This design constraint meant that data sources detailing structured, multi-step plans were necessarily excluded, limiting the scope and diversity of our training data. • Data representativeness: Our web data ac- quisition, while extensive, may not fully cap- ture the spectrum of user demographics, in- terests, and cultural backgrounds. This could result in a lack of diversity in the generated challenges, potentially omitting culturally or religiously specific practices. We plan to miti- gate this in future iterations by incorporating expert curation to broaden the system’s appli- cability. • English-only operation: Currently, 30DAY- GEN is limited to English. Its effectiveness and output quality in other languages remain uninvestigated. References Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004. Correlation clustering. Mach. Learn., 56(1–3):89–113. Marco Baroni and Silvia Bernardini. 2004. BootCaT: Bootstrapping corpora and terms from the web. In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC‘04), Lisbon, Portugal. European Language Resources As- sociation (ELRA). Chia-Hui Chang, M. Kayed, M.R. Girgis, and K.F. Shaalan. 2006. A survey of web information ex- traction systems. IEEE Transactions on Knowledge and Data Engineering, 18(10):1411–1428. Peter Christen. 2012. Data matching: Concepts and techniques for record linkage, entity resolution, and duplicate detection, 2012 edition. Springer, Berlin, Germany. John Dagdelen, Alexander Dunn, Sanghoon Lee, Nicholas Walker, Andrew S Rosen, Gerbrand Ceder, Kristin A Persson, and Anubhav Jain. 2024. Struc- tured information extraction from scientific text with large language models. Nat. Commun., 15(1):1418. Xin Luna Dong. 2023. Generations of knowledge graphs: The crazy ideas and the business impact. Preprint, arXiv:2308.14217. Xin Luna Dong and Divesh Srivastava. 2015. Big data integration. Springer, Berlin, Germany. Maristella Gatto. 2014. Web as corpus: Theory and practice. Continuum Publishing Corporation, New York, NY. Behzad Golshan, Alon Halevy, George Mihaila, and Wang-Chiew Tan. 2017. Data integration: After the teenage years. In Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, PODS ’17, page 101–106, New York, NY, USA. Association for Computing Machin- ery. Narayan Hegde, Madhurima Vardhan, Deepak Nathani, Emily Rosenzweig, Cathy Speed, Alan Karthike- salingam, and Martin Seneviratne. 2024. Infusing behavior science into large language models for ac- tivity coaching. PLOS Digit. Health, 3(4):e0000431. Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumu- lated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4):422–446. Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535–547. Adam Kilgarriff and Gregory Grefenstette. 2003. In- troduction to the special issue on the web as cor- pus. American Journal of Computational Linguistics, 29(3):333–348. Ben Liu, Jihan Zhang, Fangquan Lin, Xu Jia, and Min Peng. 2025. One size doesn’t fit all: A personal- ized conversational tutoring agent for mathematics instruction. Preprint, arXiv:2502.12633. Susan Michie, Maartje M van Stralen, and Robert West. 2011. The behaviour change wheel: a new method for characterising and designing behaviour change interventions. Implementation Science, 6. Ralph Peeters, Aaron Steiner, and Christian Bizer. 2024. Entity matching using large language mod- els. Preprint, arXiv:2310.11244. Elina Symseridou. 2018. The web as a corpus and for building corpora in the teaching of specialised trans- lation: The example of texts in healthcare. FITISPos International Journal, 5. Gerhard Weikum and Martin Theobald. 2010. From information to knowledge: harvesting entities and re- lationships from web sources. In Proceedings of the Figure 3: PR-curve’s according to our three question types: general, fairly-specific, and ultra-specific Twenty-Ninth ACM SIGMOD-SIGACT-SIGART Sym- posium on Principles of Database Systems, PODS ’10, page 65–76, New York, NY, USA. Association for Computing Machinery. Haolun Wu, Ye Yuan, Liana Mikaelyan, Alexander Meulemans, Xue Liu, James Hensman, and Bhaskar Mitra. 2024. Learning to extract structured entities using language models. Preprint, arXiv:2402.04437. A PR-Curve’s PR-Curve Per Question Type Figure 3 shows the PR-curves by different question types. We observed very high precision (between 0.9 and 1) for general questions, slightly lower but still relatively high precision (between 0.7 and 0.9) for fairly specific questions, and much lower precision (below 0.6) for ultra-specific questions, showing much higher difficulties in finding chal- lenges for ultra-specific wishes. Note that since we only examine top-20 answers, the recall for the first two types of questions stops at 40% (we labeled only 50 answers) but gets higher for ultra-specific questions, which typically have much fewer correct answers. PR-Curve Overall Figure 4 shows the overall PR-curves of all dif- ferent question types combined, with and without filtering. Both versions begin with mediocre pre- cision (0.82 and 0.77 for filtered and unfiltered respectively), but this is mainly caused by lower performance with ultra-specific queries. Still, the figure demonstrates that our filtered version de- livers higher overall precision than the unfiltered version until past 10 results, before performance suffers from aggressive filtering. Figure 4: PR-curve’s according to our three question types: general, fairly-specific, and ultra-specific B Example Challenge Ideas Source One example of a website containing 30-day chal- lenge ideas is the blog post titled "101 30-Day Self-Care Challenge Ideas", which can be found at: Sarah Steckler’s Blog: 101 30-Day Self-Care Chal- lenge Ideas C Search Query List General Queries 1. fun and simple 30 day challenge ideas 2. unique monthly challenge list for personal growth 3. 30 day self improvement challenge ideas 4. 30 day challenge ideas 5. easy monthly challenges to try at home 6. personal growth monthly challenge inspira- tion 7. daily habit building 30 day challenge 8. creative and productive monthly challenges 9. motivational 30 day life improvement chal- lenge 10. list of fun challenges to do each month 11. ideas for a different 30 day challenge each month Theme-Specific Queries 1. 30 day fitness challenge ideas 2. monthly wellness challenge for healthy habits 3. monthly learning challenge for self-education 4. 30 day study challenge ideas for students 5. monthly art challenge prompts for creativity 6. 30 day writing challenge for creative practice 7. monthly productivity challenge for better habits 8. 30 day organization challenge for time man- agement 9. monthly sustainability challenge for eco- friendly living 10. 30 day low waste lifestyle challenge 11. monthly money saving challenge ideas 12. 30 day no spend challenge for budgeting 13. monthly kindness challenge for better relation- ships 14. 30 day social skills improvement challenge D Blocked Base Domains • YouTube • Pinterest • Facebook • Instagram • Amazon • Reddit • eBay • LinkedIn • Etsy • Yelp • TikTok • Quora E LLM Prompt for Webpage Filtering The following prompt was provided to the LLM to filter and remove webpages unlikely to provide 30-day challenges based on URLs. You are an AI system designed to analyze URLs and determine their likelihood of containing useful challenge ideas across various domains. Your goal is to identify URLs that might provide multiple, re- peatable challenge ideas, not just single-challenge plans, product promotions, or fixed 30-day sched- ules. Here is the array of URLs you need to analyze: <url_array>{Webpage URLs}</url_array> Instructions: Analyze each URL in the array without access- ing the actual websites. Provide a comprehensive analysis for all URLs in a single response. For each URL, use the following structure: URL Evaluation: • url: Insert URL here • breakdown: Break down the URL into its components: domain, path • keywords_and_flags: List all relevant key- words and potential red flags found in the URL • arguments: Provide arguments for and against the URL containing useful challenge ideas • indicator_count: Count the number of pos- itive and negative indicators • idea_estimate: Estimate the potential num- ber of challenge ideas based on URL structure • isHabitLike: true or false — Does this URL likely describe a challenge that promotes do- ing the same or similar action every day (e.g., journal daily, walk daily)? • isIdeaGenerator: true or false — Does this URL likely offer general, reusable challenge ideas, as opposed to a fixed 30-day plan with different tasks each day? • assumptions: Explicitly state any assump- tions made about the content • analysis: Consider the following: – Is this likely to be a brand challenge or competition? – Are there indicators of repeatable ac- tions (e.g., "daily", "30-day")? (THIS IS THE MOST IMPORTANT FACTOR. URLs that do not signify repeatable chal- lenges should receive low scores. Sim- ply including the word "challenge" is not enough.) – Could this be a one-time event rather than a source of multiple challenge ideas? – Might this be a single structured chal- lenge plan rather than a collection of gen- eral ideas? – Does it appear to promote habit-like be- havior or structured progression? Explain your reasoning clearly based on what can be inferred from the URL. • likelihood_score: Provide a score from 0 to 10 based on the criteria below • reasoning: Summarize why you assigned this score, focusing on the most important factors Scoring Rule: • If either isHabitLike or isIdeaGenerator is false, the likelihood_score must not ex- ceed 5. Steps for Evaluation: 1. Break down the URL structure (protocol, do- main name, subdomains, path, query parame- ters). 2. Identify and list all relevant keywords and potential red flags in the URL. 3. Consider and explicitly state arguments for and against the URL containing useful chal- lenge ideas. 4. Count and list the number of positive and neg- ative indicators. 5. Estimate the potential number of challenge ideas based on URL structure and language. 6. Determine: • If the challenge promotes habit-like rep- etition (isHabitLike) • If the URL likely provides a source of reusable ideas (isIdeaGenerator) 7. Explicitly state any assumptions made about the content. 8. Analyze the URL based on this information and the criteria listed above. 9. Assign a likelihood score (0–10), and apply the scoring rule if needed. Look for indicators that the content might be related to challenges or ideas in any domain. These may include, but are not limited to: • "challenge" • "ideas" • "30-day" • "monthly" • Numeric indicators of multiple items (e.g., "15 challenge ideas") Watch for red flags that may indicate low useful- ness: • Specific challenge names (e.g., "30-day plank challenge") • Product-oriented terms (e.g., "products", "buy", "shop") • Signs of a linear plan or single-event guide (e.g., "Day 1: do X, Day 2: do Y") • Brand or competition-related terminology Apply the following scoring criteria: • 9–10: Very likely to contain multiple, reusable challenge ideas with habit-forming, repeatable actions • 7–8: Likely to contain multiple challenge ideas with some repeatability, but may be lim- ited in scope or diversity • 5–6: Contains challenge-related content, but likely a single plan or not habit-based • 3–4: Possibly challenge-related, but too spe- cific, structured, branded, or one-time in na- ture • 0–2: Likely irrelevant, promotional, or lack- ing challenge-related content Important: Base your analysis solely on the in- formation provided in the URLs. Do not access the actual websites or assume content beyond what is reasonably suggested by the structure and language of the URL. Consider challenge ideas from any domain, not just fitness. Be especially cautious about assigning high scores to URLs that appear to be: • Brand-sponsored challenges • Fixed 30-day challenge calendars • Product promotions • Competitions or one-off events Only assign high scores to URLs that clearly indicate a broad, reusable source of challenge ideas that support daily repetition or habit-building. F LLM Prompt for Challenge Extraction The following prompt was provided to the LLM to generate challenge ideas according to unstructured webpage content: You are an AI system designed to extract high-quality 30-day challenge ideas from arti- cles. Your goal is to identify and format challenge ideas that can be adapted into 30-day challenges. <article_text> {Article Content} </arti- cle_text> Instructions: 1. Title Extraction: - Extract the exact wording of the challenge title from the article - If no explicit title exists, create a concise descrip- tive title based on the challenge content - Remove any extra whitespace or newlines 2. Idea Retrieval: - Identify and extract every high-quality challenge idea - Include ideas that can be adapted as 30-day chal- lenges, even if presented as shorter challenges - Ignore challenge plans that break down actions by individual days (e.g., “Day 1: jumping jacks, Day 2: bench press”) 3. For Each Idea, Extract: - title: The exact title of the challenge if available (single line, no newlines) - wish: A 10-word summary describing what some- one hopes to achieve (single line, no newlines) - dailyAction: A SMART daily action (Specific, Measurable, Achievable, Relevant, Time-bound) Specific: Clearly state what needs to be done Measurable: Include a quantifiable element (time, reps, distance, etc.) Achievable: Something that can realistically be done daily Relevant: Directly related to achieving the wish Time-bound: Must be completable within one day - description: Use the exact description text from the article without modification (single line, no newlines) Examples: For a “Squat Challenge”: • BAD Wish: “To improve strength and fitness at home with squats” • BAD Wish: “To improve strength and fitness at home” • GOOD Wish: “Improve strength and fitness at home” • BAD dailyAction: “Do squats” • BAD dailyAction: “Exercise legs” • GOOD dailyAction: “Complete 3 sets of 15 bodyweight squats” For a “Reading Challenge”: • BAD Wish: “To enjoy a fun reading habit and learn more” • BAD Wish: “To read more books and expand knowledge base daily” • GOOD Wish: “Read more books and expand knowledge” • BAD dailyAction: “Read more” • BAD dailyAction: “Read books” • GOOD dailyAction: “Read one chapter or 20 pages of current book” For a “Meditation Challenge”: • BAD Wish: “To become more mindful and present in daily life” • GOOD Wish: “To practice meditation and reduce stress levels regularly” • GOOD Wish: “Develop mindfulness and re- duce daily stress through meditation” • BAD dailyAction: “Meditate” • BAD dailyAction: “Do some mindfulness” • GOOD dailyAction: “Complete one 10- minute guided meditation session” 4. Validation Rules: - Exclude any challenge that is structured as a day- by-day plan - Include adaptable ideas even if they’re 7-day or general habit ideas - Return an empty array if no valid ideas are found - ALL fields must be single lines with no newlines - wish and dailyAction must be exactly 15 words or less - dailyAction must be SMART and include a mea- surable component (number, duration, distance, etc.) - wish must be concise and action-oriented, avoid- ing “to” at the start Output Format: Return an array of challenge objects, each follow- ing this structure: { "title": "Exact Challenge Title", "description": "EXACT description text from the article", "wish": "10-word summary of the goal", "dailyAction": "10-word summary of daily action" } Important: - Use EXACT text from the article for descriptions - Keep wish and dailyAction summaries to exactly 10 words - Only include ideas that can be adapted to 30-day challenges - Exclude any day-by-day structured plans - Return an empty array if no valid ideas are found - ALL fields must be single lines with no newlines or extra whitespace G LLM Prompt for Duplicate Judgment The following prompt was provided to the LLM to judge whether challenge pairs are compatible with our duplicate definition. You are an AI assistant that analyzes pairs of daily actions to determine if they are duplicates or not. A pair is considered a duplicate if the actions are essentially the same, even if worded differently. Consider the context, intent, and nature of the activities, such as similar types of exercise or volunteering, when determining similarity. Pay special attention to the methods, outcomes, and perspectives involved in the actions, as different methods, outcomes, or perspectives may indicate non-similarity. Provide the output in the format: { pairId: number, isSimilar: boolean, explanation: string }[] Similarity Assessment Guidelines for Daily Action Pairs Use the following rules to decide whether two daily actions are too similar or sufficiently distinct. As- sess based on goal, method, context, and intent. Include examples where helpful. Mark pairs as too similar if they meet ANY of the following: 1. Same Core Outcome If both actions aim to achieve the same final result, they are too similar, even if phrasing differs. Example: A: “Avoid all online shopping for the entire month” B: “Avoid all online shopping for 24 hours each day” →Same outcome (no online shopping), just differ- ent framing. 2. Same Method or Strategy If both actions use the same mechanism, they’re too similar. Example: A: “Take a 20-minute nap in the afternoon” B: “Take a 30-minute nap daily” →Only duration changes; method and intent are the same. 3. Same Contextual Setup If both actions occur in the same setting with mi- nor variations (like TV vs. devices), they are too similar. Example: A: “Eat dinner with no devices” B: “Eat dinner with no TV for 60 minutes” →Both are about distraction-free dinners. Mark pairs as not too similar if they meet ANY of the following: 1. Different Goals or Intent If the end purpose differs significantly, they’re not too similar. Example: A: “Do something that makes you laugh every day” B: “Infuse your daily activities with fun” →Laughing is a specific outcome; infusing fun is about mindset and engagement. 2. Different Methods to Similar Goals If they use distinct systems, even with the same goal, they’re not too similar. Example: A: “Save daily into a holiday fund” B: “Use the envelope method to save money” →One is goal-based; the other is system-based. 3. Different Participants or Perspective If one action is self-focused and the other involves others, they are distinct. Example: A: “Watch scary videos and record your reaction” B: “Scare someone and record their reaction” →One’s about your reaction; the other is about theirs. 4. Different Emotional or Reflective Framing If journaling or reflection differs in theme or emo- tional depth, they’re distinct. Example: A: “Write in a themed journal for 15 minutes” B: “Write in a passion journal for 15 minutes” →One is structured/exploratory; the other is per- sonal/emotional. Quick Checklist Quick Checklist: • Question: Do they result in the same exact behavior or outcome? If YES, then: Too Similar • Question: Do they use the same process or technique? If YES, then: Too Similar • Question: Do they happen in the same setting with only superficial changes? If YES, then: Too Similar • Question: Do they differ in why someone is doing them? If YES, then: Not Too Similar • Question: Do they use different systems or strategies? If YES, then: Not Too Similar • Question: Do they involve different people or focus (self vs. others)? If YES, then: Not Too Similar • Question: Do they reflect different emotional or thematic purposes? If YES, then: Not Too Similar Compare the Following: <pair_array> {Challenge Pairs} </pair_array> H LLM Prompt for Search Validation Your job is to validate the challenge ideas pro- vided by the user. Remove only those challenge ideas that are entirely irrelevant or unhelpful in contributing to a user’s wish. (DO NOT be overly strict. Only remove those that are obviously un- helpful). <string_input> {Input} </string_input> <challenge_array> {Challenge List} </chal- lenge_array> Provide the response as an array of challenge idea IDs that should be removed. [ id1, id2, id3 ] 