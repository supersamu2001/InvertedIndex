3D Can Be Explored In 2D : Pseudo-Label Generation for LiDAR Point Clouds Using Sensor-Intensity-Based 2D Semantic Segmentation Andrew Caunes12, Thierry Chateau1, Vincent Fr´emont2 Abstract— Semantic segmentation of 3D LiDAR point clouds, essential for autonomous driving and infrastructure manage- ment, is best achieved by supervised learning, which demands extensive annotated datasets and faces the problem of domain shifts. We introduce a new 3D semantic segmentation pipeline that leverages aligned scenes and state-of-the-art 2D segmen- tation methods, avoiding the need for direct 3D annotation or reliance on additional modalities such as camera images at inference time. Our approach generates 2D views from LiDAR scans colored by sensor intensity and applies 2D semantic segmentation to these views using a camera-domain pretrained model. The segmented 2D outputs are then back-projected onto the 3D points, with a simple voting-based estimator that merges the labels associated to each 3D point. Our main contribution is a global pipeline for 3D semantic segmentation requiring no prior 3D annotation and not other modality for inference, which can be used for pseudo-label generation. We conduct a thorough ablation study and demonstrate the potential of the generated pseudo-labels for the Unsupervised Domain Adaptation task. I. INTRODUCTION Semantic segmentation of 3D point clouds, gathered by LiDAR sensors, is pivotal in various domains, notably in Autonomous Driving (AD) and infrastructure management. This segmentation is crucial for tasks such as environment understanding and object interaction—key components for autonomous system navigation and decision-making. How- ever, the majority of 3D semantic segmentation work, par- ticularly for LiDAR, has been tailored to AD. This spe- cialization focuses on dynamic objects like vehicles, often neglecting the full potential of dense scenes of aligned point clouds for static objects. Moreover, the emphasis on real- time processing limits the scope of computational resources, impacting the reach of segmentation achievable. The highest performance in 3D semantic segmentation has been achieved through supervised learning [31] [4], requiring extensive annotated data. This approach is notably hindered by the significant resources required for annotation, as well as the inherent difficulty in annotating complex 3D data [2] [1]. Additionally, existing methods often suffer from substantial domain shift when applied to diverse or unseen environments, which hampers the model’s ability to generalize across diverse environments. In response to these challenges, various approaches have been proposed to avoid or reduce the need for point cloud annotation. Weak and semi-supervised learning strategies aim to maximize performance with minimal annotated data [11]. Domain adaptation methods aim to overcome the 1Logiroad 2LS2N - Ecole Centrale de Nantes domain shift problem, enabling the use of existing datasets [5] [23] [16]. Active learning strategies aim to minimize this need by carefully selecting data samples for annotation. While all of these methods have shown promising results, they still require a certain level of annotation, either for the target domain or for the source domain. Encouraged by the rapid advancements in 2D image segmentation in recent years, novel solutions have been proposed to transfer knowledge from 2D to 3D, thus circumventing the need for 3D annotations as well as avoiding the 3D domain shift problem [24] [12] [33] [8]. Our approach, while falling into this category, uniquely does not rely on additional modalities such as camera images for inference. We propose a pseudo-label generation pipeline leveraging aligned scenes, higher computing time and state of the art 2D methods to generate high quality labels. These labels can then be used either on their own or as training material e.g. in domain adaptation. The only requirements are that input point clouds must be provided in sequences and either a pretrained 2D image segmentation model or a 2D annotated dataset must be available for the desired classes at training time. It should be highlighted that our method is optimal for use with static classes, unaffected by movement artifacts in aligned scenes. We plan to investigate solutions for dynamic classes in future work. First, all scans of a 3D scene are aligned using input sensor poses which are either provided or computed. A large number of views of the scene are then generated as 2D RGB images, using only the LiDAR intensities as colors. This differs from prior arts where camera images are used for direct inference or point coloring [15] [29] [7]. Ideally, the views of the point cloud as well as the augmented training images of the 2D model are selected with the objective of minimizing the domain shift between them. These views are then processed through the 2D segmentation model, and the resulting labels are projected onto the 3D points. The 3D segmentation masks are accumulated, effectively being used as “votes”. Each point’s final class is then determined based on a simple election estimator. In this paper, we make two key contributions to the field of 3D semantic segmentation. First, we show that direct inference of 2D image models on views of raw dense point clouds with only sensor-intensity-based coloring can be used for 3D semantic segmentation without requiring prior 3D annotations or other modality at inference time. Second, we show that this method can be used to produce quality pseudo-labels. We demonstrate the potential of our pseudo- labels firstly by comparing them to ground-truth labels and arXiv:2505.03300v1  [cs.CV]  6 May 2025 secondly by using them in the 3D Unsupervised Domain Adaptation (UDA) setting and comparing the resulting mod- els to state-of-the-art methods. II. RELATED WORKS Most of the existing approaches for 3D semantic segmen- tation utilize 3D annotations, either in the source or the target domain. When 3D labels are available in the target domain, fully-supervised 3D models achieve the best performance [4] [31] with models either using sparse 3D convolutions or 2D Bird’s Eye View representations. When the target data comes with few annotated samples, semi-supervised methods can be used to leverage unlabelled target data. In that case, it has been shown that self-training on pseudo-labels can significantly improve performance by approximating entropy maximization [32] [11]. Some domain adaptation methods also leverage a few target labels to improve performance [5]. With only source domain 3D labels, unsupervised domain adaptation methods have been proposed to mitigate the domain shift problem [16] [23], [28], [6], [26], [10]. Most recently, zero-shot learning methods have been shown to perform well in 3D semantic segmentation. [30] and [14] align 2D, 3D and language features in a common latent space in order to segment unseen classes in a target domain. Substantial progress has been made recently towards 3D semantic segmentation without any 3D annotation. This implies using other modalities for training and / or inference. Most recently, self-supervised methods have been proposed where a model is pretrained on unlabelled data using a pretext task to obtain relevant feature representations. 2D- to-3D knowledge distillation [24] [12] leverages 2D image models to directly teach a student 3D model. Though very efficient, this approach still requires downstream fine-tuning on 3D labelled data to achieve a given task. We propose to segment multiple 2D views of a 3D point cloud, therefore requiring no 3D annotation but only 2D annotations at training time. Close to our proposed method, [25] provides a comprehensive review of the state-of-the-art techniques that use multiple views of a scene for inference with Convolutional Neural Networks (CNN). These methods can be divided into different categories, depending on the stage at which information from multiple views is fused. Our method falls into the category that uses score fusion, which involves performing inference independently on each view first and then fusing the resulting scores. The choice of the fusion method is investigated in [25] where they experiment with adding, compounding and taking a maximum of the scores. The summing estimator can be seen as a simple majority election system where each projected classification of a point (or pixel) constitutes a vote. [20] uses this method with colored point clouds for heritage building 3D segmentation. The method proposed in [7] is very close to ours except that posed camera RGB images are used for inference instead of generated views of the point cloud. [29] and [15] use a more complex graph-based diffusion mechanism accounting for neighboring points to project and fuse the 2D labels to 3D. [22] and [21] experiment with using neural networks to correspond 2D features to 3D, a process that requires 3D annotations for training. Very recently, [33] and [8] applied a 2D-to-3D label propagation method to non-semantic segmentation labels from Vision Foundation Models (VFM) such as Segment-Anything [9]. To the best of our knowledge, our method is the first to perform 2D-to-3D label propagation to generated 2D views thereby eliminating the need for camera images at inference time. We experiment with various fusion estimators and show that a soft summing estimator can yield satisfactory results. III. METHOD Our proposed pipeline is illustrated in Fig. 1. As a preliminary step , a trained 2D image segmentation model must be obtained. Ideally, the model is trained using image augmentations in order to minimize the domain gap with the target images that will be generated subsequently. We train our own 2D models in Section IV and include the influence of the augmentations in the ablation study. Given a sequence of raw LiDAR scans S = {s1,s2,...,sM}, where ∀m, sm ∈RNm×4 (each point has 3 coordinates and an intensity), we colorize each point pj,m in scan sm based on its intensity (LiDAR reflectivity). We apply two transformations to these intensities with the objective to increase contrast and to close the domain gap between the training images of the 2D model and the generated views. The intensity Ij,m of each point is first clipped to a range [βmin,βmax] to mitigate the effect of extremely high or low values: I′ j,m = min(max(Ij,m,βmin),βmax), The clipped intensity is then scaled to a range [ηmin,ηmax]. I′′ j,m = I′ j,m −min j(I′ j,m) max j(I′ j,m)−min j(I′ j,m) ×(ηmax −ηmin)+ηmin. Each scan sm is then aligned into a common reference frame using transformation Tm, obtained from sensor poses. Sdense = M [ m=1 Tm(sm). When sensor poses are not readily available, they can be estimated using odometry methods such as Iterative Closest Point (ICP). Experimentation on our own data shows that a 64-beam LiDAR sensor with a 20Hz acquisition rate can be aligned automatically using [27] with enough precision for our method to work with satisfying performance. From the sensor poses, K multiple virtual camera poses {(R′ i,T ′ i )}1≤i≤K are generated by simulating a vehicle-like perspective at different positions and orientations around Sdense, as described in Alg. 1. We add random noise to the original poses to enhance the diversity of viewpoints. We then obtain views V = {v1,v2,...,vK} corresponding to 2D projections of the 3D scene: ∀k, vk = π3D→2D(Sdense; R′ k,T ′ k), votes {ŷipoint}1≤i≤M / {Lipoint}1≤i≤M 2D segmentation model Output Labels (C classes) + 2D augmented camera images Augmentations : RGB -> grey Noise Color distortion Crop Resize ... Preliminary phase Inference phase Project  π2D→3D 3D Aligned point cloud (scene) with normalized LiDAR intensities as colors 2D segmentation model Train + Input 3D pointclouds (scans) M Generate K views {vi}1≤i≤K from virtual camera poses Count votes Align Project π3D→2D . . . . . . 2D segmentation masks {ŷi}1≤i≤K and logits {Li}1≤i≤K Sample & Add random noise Virtual camera poses {(R'i, T'i)}1≤i≤K Inference counted votes ŷ / Lcumulative / Lcompound Elect classes Sensor poses {(Ri, Ti)}1≤i≤M Sensor poses {(Ri, Ti)}1≤i≤M Fig. 1. Pipeline. As a preliminary step, a 2D semantic segmentation model is trained on augmented camera images. An input sequence of LiDAR 3D scans and sensor poses is aligned and colorized using sensor intensity. A large number of 2D greyscale images are then generated from virtual camera poses along the sensor’s trajectory. The 2D model is applied to obtain segmentation logits and labels for each view which are then projected back onto the 3D points. These votes are counted and each point is assigned a class based on the chosen election estimator. Algorithm 1 Generate virtual camera poses 1: for all i in J1,KK do 2: R′ i, T ′ i = RandomChoice({Ri,Ti}1<i<M) 3: R′ i = R′ i ·RotationY(U (−θ,θ)) 4: T ′ i = T ′ i + U (−λ,λ)∗(R′ i)X 5: T ′ i = T ′ i + U (−λ,λ)∗(R′ i)Z 6: T ′ i = T ′ i + U (−γ,γ)∗(R′ i)Y 7: end for During the inference stage, for each generated view vk, the 2D segmentation model f2D outputs logits Lpixel k and a segmentation mask ˆypixel k . Lpixel k = f2D(vk) ∈Rwidth×height×C, ˆypixel k = argmax(Softmax(f2D(vk))) ∈{0,1}width×height×C The projection step maps the predicted 2D segmentation masks ˆypixel k and logits Lpixel k back onto the 3D points in Sdense. For each view vk, we obtain point logits Lpoint k and a point segmentation mask ˆypoint k by projecting using the pose (R′ k,T ′ k) : Lpoint k = π2D→3D(Lpixel k ; R′ k,T ′ k) ∈RM×C, ˆypoint k = π2D→3D(ˆypixel k ; R′ k,T ′ k) ∈{0,1}M×C We accumulate and compound the obtained point segmen- tation masks and logits from all views, a process that can be understood as counting votes. We experiment with three different fusion methods (estimators): ˆypoint = K ∑ k=1 ˆypoint k ∈NM×C,, Lpoint cumulative = K ∑ k=1 Lpoint k ∈RM×C, Lpoint compound = K ∏ k=1 Lpoint k ∈RM×C. A final class is then picked for each point according to the chosen election estimator: • Summing hard votes : ˆypoint = argmax(ˆypoint) ∈J1,CKM • Summing Soft Votes : ˆypoint = argmax(Lpoint cumulative) ∈J1,CKM • Compounding Soft Votes : ˆypoint = argmax(Lpoint compound) ∈J1,CKM IV. EXPERIMENTS To demonstrate the effectiveness of our approach, we generate pseudo-labels for the nuScenes autonomous driving dataset [2], and then evaluate the quality of these labels in two ways. First, we compare the pseudo-labels generated directly to the ground-truth annotations. We conduct an ablation study to verify the importance of various features of our pipeline. Second, we utilize the obtained pseudo-labels to train 3D semantic segmentation models in the Unsupervised Domain Adaptation setting and compare their performance to that of state-of-the-art methods. A. Pseudo-labels Generation 1) Implementation details: For our first experiment, we generate pseudo-labels on the nuScenes autonomous driving dataset [2] and compare them to the ground-truth annota- tions. nuScenes is a large-scale dataset providing semantic segmentation annotations for more than 34,000 LiDAR scans acquired using a 32-beam LiDAR sensor at 20Hz. The dataset is split into 1,000 scenes, with 850 annotated scenes of which 150 are for validation. There are a total of 32 annotated classes for 3D semantic segmentation, but we will focus on a subset of 5 classes: driveable surface, sidewalk, manmade, vegetation and terrain. We select these classes for their static nature which make them unaffected by motion- related artifacts in an aligned sequence of scans. For each scene, nuScenes provides the full sequence of LiDAR scans of which about 1/10 is annotated. The sensor poses which we use to align the scans are also provided. We use the Intersection over Union (IoU) metric for the comparison. We compute the IoU for each class as well as the mean IoU (mIoU) over all classes. We run our pipeline over all 850 annotated scenes of the nuScenes dataset. Points further than 30 meters on each side and 10 meters in height are cropped for the comparison since the generated views do not explore these areas. In the future, we plan to adress this issue by generating more diverse poses and adapting the 2D model to other domains than the classical vehicle perspective. The sidewalk and terrain classes are also merged for this comparison, as they are very close in LiDAR intensity domain. We sample K = 600 poses from the original sensor poses for each scene and add random noise to them following Alg. 1 with θ = 30◦, λ = 1m, γ = 1m. Examples of the obtained view can be seen in Fig. 1. We set a limit on the maximum depth for the back-projection to 30 meters and a minimum depth of 1 meter. We discuss the influence of all parameters in Table I. We train our own 2D semantic segmentation model. For this, we require a dataset annotating RGB images with the same classes as our target dataset. We use the Mapillary Vistas dataset [19] which provides 20,000 annotated street images of high diversity. We believe this diversity can help bridge the domain gap between camera images and generated views. For the same reason, we augment the images in multiple ways including random cropping, random scaling, random horizontal flipping, random color jittering, random gaussian noise, random gaussian blur and rotations. Most im- portantly, we convert the RGB images to greyscale images to match the LiDAR intensity domain. We also experiment with the influence of these augmentations in Table I. Examples of the images can be seen in Fig. 1 We train a Mask2Former [3] architecture with a swin-l backbone [13] using [18] for 9 epochs with a batch size of 1 on 4 Nvidia V100 GPUs. We use a learning rate of 0.0001 with polynomial decay and a momentum of 0.9 and a weight decay of 0.0001. 2) Results: The comparison of our pseudo-labels with ground-truth annotations is displayed in Fig. 2. The best 0 200 400 600 800 1000 0.0 0.5 1.0 road Average IoU: 0.78 0 200 400 600 800 1000 0.00 0.25 0.50 0.75 sidewalk/terrain Average IoU: 0.48 0 200 400 600 800 1000 0.00 0.25 0.50 0.75 manmade Average IoU: 0.37 0 200 400 600 800 1000 0.00 0.25 0.50 0.75 vegetation Average IoU: 0.48 0 200 400 600 800 1000 0.00 0.25 0.50 0.75 mIoU Average IoU: 0.49 Fig. 2. Comparison of the pseudo-labels generated on the nuScenes dataset [2]. IoU per class and mIoU for all annotated scenes. Transformations such as cropping are applied to obtain a relevant comparison. performance is obtained for the driveable surface class with an average IoU of 0.81. This is explained by the higher contrast between the road and the rest of the scene. The manmade class is the least well predicted with a mIoU score of 0.37. This and other low scores may be caused by three factors: the occlusion and depth problem and the lack of exploration. Indeed, buildings tend to be further away from the sensor. This leads to some points being ouf of reach for many views given our depth limit, and exploration isn’t sufficient because only views close to the sensor trajectory are rendered. There is also an occlusion problem where background hidden points receive votes from 2D segmentation masks of foreground objects. This problem is most prevalent for buildings which are further away. We plan to address these issues in the future by using depth maps to deal with occlusions and by generating more diverse poses. Note that some scenes yield notably low mIoU scores because the ego vehicle remains stationary. This results in an unexplored environment and therefore a sparse aligned point cloud. We conduct an ablation study to verify the influence of various features of our pipeline in Table I. Each line corresponds to an experiment, the first experiment (a) serving as a reference. Related experiments from (b) to (h) are grouped by color and compared to (a). Various settings are explored where the presence of a feature is indicated by a checkmark. Presented features are, respectively, number of generated views (with varying K), 2D model image aug- mentation (either all augmentations or none), randomization of the virtual camera poses (either low or high values for θ, λ and γ), election estimator. The green area shows that the number of generated views has a significant impact on performance, with the mIoU score dropping more than 86% when decreasing the number of views from 600 to 100. Removing the image augmentations for the 2D model has a modest negative impact. High variation in the poses generation and the choice of election estimator, seen in the red and yellow areas, seem to have a strong impact on performance. In particular, “summing soft votes” seems to be the best election estimator. Interestingly, [25] found that the “coumpounding soft votes” election estimator yielded the best results, while we observe a large drop in performance. This may be caused by a missing prior normalization of the scores. TABLE I ABLATION STUDY. K=100 300 600 900 img. aug. rdm. vws. high rdm. vws. + hrd. vts. + sft. vts. × sft. vts. mIoU ↗% (a) ✓✓ ✓ ✓ 50.06 ref (b) ✓ ✓ ✓ ✓ 6.89 -86.23 (c) ✓ ✓ ✓ ✓ 39.5 -21.09 (d) ✓ ✓ ✓ ✓ 49.95 -0.22 (e) ✓ ✓ ✓ 49.77 -0.58 (f) ✓✓ ✓ ✓ 53.14 +6.15 (g) ✓✓ ✓ ✓ 53.12 +6.12 (h) ✓✓ ✓ ✓35.77 -28.54 B. Training 3D Models for Domain Adaptation 1) Implementation Details: As a more practical approach for evaluating the quality of our method and pseudo-labels, we conduct experiments in the Unsupervised Domain Adap- tation setting. A 3D model is pretrained on a labelled source semantic segmentation dataset and then re-trained or fine- tuned on an unlabelled target dataset using the pseudo-labels generated by our method. We use the SemanticKITTI dataset [1] as the source dataset as it also provides 3D semantic seg- mentation for the classes of interest. We compare our method to two approaches: weak-supervision baselines, which are models pretrained on SemanticKITTI and fine-tuned directly on a subset of nuScenes’ annotations, and state-of-the-art UDA methods. For the unsupervised domain adaptation methods, we com- pare our approach to SWD [10], (M+A)Ent [28], CORAL [26], UDASSGA [23] and the recent T-UDA [6]. For a fair comparison, we follow [6] and use a Minkowski U-net architecture [4]. The results for the compared methods are taken from [6]. Note that the Na¨ıve setting (”no da”) in [6] obtains a mIoU score of 34.18 on the classes of interest which is significantly higher than our own Na¨ıve implemen- tation. We believe this may be due to a difference in the augmentations used during pretraining on SemanticKITTI, but the comparison should still hold since our results are lower. We use [17] and train each model for 36 epochs on the source dataset, then fine-tune on the target dataset for 36 epochs. We use a batch size of 8 for SemanticKITTI and 32 for nuScenes on a single Nvidia V100 GPU. 2) Results: The results are displayed in Table II. The weak-supevision baselines and UDA methods are at the top and middle of the table respectively. We split our results in two experiments depending on whether the model is pretrained on SemanticKITTI or not. The best overall results are in bold while the best and second best unsupervised results are in blue and red respectively. As a first observation, both results are close to each other. This is satisfactory as it shows that our method can be used without the need for 3D semantic segmentation annotations. Secondly, the pretrained method is only 6.8 mIoU points below the 1% weak-supervision baseline, which, for nuScenes, represents about 300 manually annotated scans. Interestingly, in both settings, our method performs the worst on the vegetation class, although the pseudo-labels obtained better IoU than for the manmade class on average in Fig. 2. We believe this is due to the fact that even with better annotations, identifying leafs in a sparse 3D scan is a harder task than simple plane detection which can be used to segment a large proportion of the manmade class. Compared to state-of-the-art methods in unsupervised do- main adaptation, our approach is on-par with T-UDA [6] and reaches better mIoU than all other methods in the pretrained setting. Notably, this last statement is still true even without pretraining on SemanticKITTI. TABLE II IOU PER CLASS AND MIOU FOR THE DOMAIN ADAPTATION SETTING. road sidewalk manmadeterrain vegetation mIoU SK →NS Na¨ıve 9.7 0.1 24.7 0.7 0.3 7 1h 78.7 0.2 47.2 43.7 48.6 43.7 1% 90.2 44 79.9 70.1 56.8 68.2 10% 94.9 66.6 86.5 83 72 80.5 SWD [10] 80.7 26.5 60.2 30.1 43.9 48.28 (M+A)Ent [28] 83.5 32.6 62.3 31.8 43.3 50.7 CORAL [26] 82.6 27.1 56.7 27 55.3 49.74 UDASSGA [23] 82.2 29.6 65.7 34 57.9 53.88 T-UDA [6] 87.8 45.8 72.6 46.1 70.3 64.52 Ours 86.2 30 67.9 64.9 22 54.23 Ours pt SK 87.5 40 73 69 37.1 61.4 full. sup. 96.6 73.9 89.6 86 75 84.3 C. Conclusion We have proposed a method for 3D semantic segmentation of sequences of point clouds requiring zero 3D annota- tions. Unlike prior works, we do not require any additional modality such as camera images for inference but only 2D annotations for training. We demonstrated the potential of the method for producing pseudo-labels which can be used in the Unsupervised Domain Adaptation setting to obtain competitive results. While promising, our initial findings also highlight several challenges, such as the occlusion problem and the limited domain in which the views can be generated without increas- ing the domain shift with the 2D dataset. Future efforts will focus on mitigating these issues to enhance the robustness and applicability of our method. Avenues for improvement also include extending our technique to other classes. In particular, non-static classes such as vehicles and pedestrians could also be reached by adapting the 2D model to the domain of generated views, which includes motion-related artifacts. V. ACKNOWLEDGEMENTS This project was provided with AI computing and storage resources by GENCI at IDRIS thanks to the grant 2024- AD011012128 on the supercomputer Jean Zay’s V100 par- tition. REFERENCES [1] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Juergen Gall. SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences, August 2019. arXiv:1904.01416 [cs]. [2] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuScenes: A multimodal dataset for autonomous driving, May 2020. arXiv:1903.11027 [cs, stat]. [3] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kir- illov, and Rohit Girdhar. Masked-attention Mask Transformer for Universal Image Segmentation, June 2022. arXiv:2112.01527 [cs]. [4] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4D Spatio- Temporal ConvNets: Minkowski Convolutional Neural Networks, June 2019. arXiv:1904.08755 [cs] version: 4. [5] Eduardo R. Corral-Soto, Mrigank Rochan, Yannis Y. He, Xingxin Chen, Shubhra Aich, and Liu Bingbing. Domain Adaptation in LiDAR Semantic Segmentation via Hybrid Learning with Alternating Skip Connections. In 2023 IEEE Intelligent Vehicles Symposium (IV), pages 1–7, Anchorage, AK, USA, June 2023. IEEE. [6] Awet Haileslassie Gebrehiwot, David Hurych, Karel Zimmermann, Patrick P´erez, and Tom´aˇs Svoboda. T-UDA: Temporal Unsupervised Domain Adaptation in Sequential Point Clouds. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 7643–7650, October 2023. ISSN: 2153-0866. [7] Kyle Genova, Xiaoqi Yin, Abhijit Kundu, Caroline Pantofaru, For- rester Cole, Avneesh Sud, Brian Brewington, Brian Shucker, and Thomas Funkhouser. Learning 3D Semantic Segmentation with only 2D Image Supervision, October 2021. arXiv:2110.11325 [cs]. [8] Rui Huang, Songyou Peng, Ayca Takmaz, Federico Tombari, Marc Pollefeys, Shiji Song, Gao Huang, and Francis Engelmann. Seg- ment3D: Learning Fine-Grained Class-Agnostic 3D Segmentation without Manual Labels. 2023. Publisher: arXiv Version Number: 1. [9] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexan- der C. Berg, Wan-Yen Lo, Piotr Doll´ar, and Ross Girshick. Segment Anything, April 2023. arXiv:2304.02643 [cs]. [10] Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation, March 2019. [11] Dong-Hyun Lee. Pseudo-Label : The Simple and Efficient Semi- Supervised Learning Method for Deep Neural Networks. ICML 2013 Workshop : Challenges in Representation Learning (WREPL), July 2013. [12] Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wenwei Zhang, Liang Pan, Kai Chen, and Ziwei Liu. Segment Any Point Cloud Sequences by Distilling Vision Foundation Models, October 2023. arXiv:2306.09347 [cs]. [13] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, August 2021. arXiv:2103.14030 [cs]. [14] Yuhang Lu, Qi Jiang, Runnan Chen, Yuenan Hou, Xinge Zhu, and Yuexin Ma. See More and Know More: Zero-shot Point Cloud Segmentation via Multi-modal Visual Data. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 21617– 21627, Paris, France, October 2023. IEEE. [15] Ruben Mascaro, Lucas Teixeira, and Margarita Chli. Diffuser: Multi- View 2D-to-3D Label Diffusion for Semantic Scene Segmentation. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 13589–13595, Xi’an, China, May 2021. IEEE. [16] Bjoern Michele, Alexandre Boulch, Gilles Puy, Tuan-Hung Vu, Re- naud Marlet, and Nicolas Courty. SALUDA: Surface-based Au- tomotive Lidar Unsupervised Domain Adaptation, November 2023. arXiv:2304.03251 [cs]. [17] MMDetection3D Contributors. OpenMMLab’s Next-generation Plat- form for General 3D Object Detection, July 2020. original-date: 2020- 07-08T03:39:45Z. [18] MMSegmentation Contributors. OpenMMLab Semantic Segmenta- tion Toolbox and Benchmark, July 2020. original-date: 2020-06- 14T04:32:33Z. [19] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The Mapillary Vistas Dataset for Semantic Understand- ing of Street Scenes. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 5000–5009, Venice, October 2017. IEEE. [20] Eug´enio Pellis, Arnadi Murtiyoso, Andrea Masiero, Grazia Tucci, Michele Betti, and Pierre Grussenmeyer. 2D to 3D Label propagation for the semantic segmentation of Heritage building point clouds. In XXIV ISPRS Congress “Imaging today, foreseeing tomorrow”, Commission II 2022 edition, 6–11 June 2022, Nice, France, volume XLIII-B2-2022, pages 861–867, Nice, France, June 2022. ISPRS. [21] Torben Peters, Claus Brenner, and Konrad Schindler. Semantic segmentation of mobile mapping point clouds via multi-view label transfer. ISPRS Journal of Photogrammetry and Remote Sensing, 202:30–39, August 2023. [22] Damien Robert, Bruno Vallet, and Loic Landrieu. Learning Multi- View Aggregation In the Wild for Large-Scale 3D Semantic Segmen- tation, July 2022. arXiv:2204.07548 [cs]. [23] Mrigank Rochan, Shubhra Aich, Eduardo R. Corral-Soto, Amir Nabatchian, and Bingbing Liu. Unsupervised Domain Adaptation in LiDAR Semantic Segmentation with Self-Supervision and Gated Adapters, March 2022. arXiv:2107.09783 [cs] version: 3. [24] Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre Boulch, Andrei Bursuc, and Renaud Marlet. Image-to-Lidar Self- Supervised Distillation for Autonomous Driving Data, March 2022. arXiv:2203.16258 [cs]. [25] Marco Seeland and Patrick M¨ader. Multi-view classification with convolutional neural networks. PLOS ONE, 16(1):e0245230, January 2021. Publisher: Public Library of Science. [26] Baochen Sun and Kate Saenko. Deep CORAL: Correlation Alignment for Deep Domain Adaptation, July 2016. arXiv:1607.01719 [cs]. [27] Ignacio Vizzo, Tiziano Guadagnino, Benedikt Mersch, Louis Wies- mann, Jens Behley, and Cyrill Stachniss. KISS-ICP: In Defense of Point-to-Point ICP – Simple, Accurate, and Robust Registration If Done the Right Way. IEEE Robotics and Automation Letters, 8(2):1029–1036, February 2023. arXiv:2209.15397 [cs]. [28] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick P´erez. ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation, April 2019. arXiv:1811.12833 [cs]. [29] Brian H. Wang, Wei-Lun Chao, Yan Wang, Bharath Hariharan, Kil- ian Q. Weinberger, and Mark Campbell. LDLS: 3-D Object Segmen- tation Through Label Diffusion From 2-D Images. IEEE Robotics and Automation Letters, 4(3):2902–2909, July 2019. arXiv:1910.13955 [cs, eess]. [30] Yuanbin Wang, Shaofei Huang, Yulu Gao, Zhen Wang, Rui Wang, Kehua Sheng, Bo Zhang, and Si Liu. Transferring CLIP’s Knowledge into Zero-Shot Point Cloud Semantic Segmentation, December 2023. arXiv:2312.07221 [cs]. [31] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point Transformer V3: Simpler, Faster, Stronger, December 2023. arXiv:2312.10035 [cs] version: 1. [32] Jihan Yang, Shaoshuai Shi, Zhe Wang, Hongsheng Li, and Xiaojuan Qi. ST3D: Self-training for Unsupervised Domain Adaptation on 3D Object Detection, March 2021. arXiv:2103.05346 [cs]. [33] Yunhan Yang, Xiaoyang Wu, Tong He, Hengshuang Zhao, and Xi- hui Liu. SAM3D: Segment Anything in 3D Scenes, June 2023. arXiv:2306.03908 [cs]. 