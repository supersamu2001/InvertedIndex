arXiv:2505.05474v1  [cs.CV]  8 May 2025 1 3D Scene Generation: A Survey Beichen Wen*, Haozhe Xie*, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu Abstract—3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation. Index Terms—3D Scene Generation, Generative Models, AI Generated Content, 3D Vision ✦ 1 INTRODUCTION T HE goal of generating 3D scenes is to create a spatially structured, semantically meaningful, and visually real- istic 3D environment. As a cornerstone of computer vision, it supports a wide range of applications, from immersive filmmaking [1], [2] and expansive game worlds [3], [4], [5] to architectural visualization [6], [7]. It also plays a crucial role in AR/VR [8], [9], [10], robotics simulation [11], [12], and autonomous driving [13], [14] by providing high-fidelity environments for training and testing. Beyond these appli- cations, 3D scene generation is vital for advancing embodied AI [15], [16], [17] and world models [18], [19], [20], which depend on diverse, high-quality scenes for learning and evaluation. Realistic scene synthesis enhances AI agents’ ability to navigate, interact, and adapt, driving progress in autonomous systems and virtual simulations. As shown in Figure 1, 3D scene generation has gained significant attention in recent years. Early scene generation methods relied on procedural generation using rule-based algorithms [21] and manually designed assets [22], offering scalability and control in game design [23], urban plan- ning [24], [25], and architecture [26], [27]. However, their reliance on predefined rules and deterministic algorithms limits diversity, requiring extensive human intervention for realistic or varied scenes [28]. Advances in deep genera- tive models (e.g., GANs [29], Diffusion models [30]), en- able neural networks to synthesize diverse, realistic spatial • This study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOET2EP20221- 0012, MOE-T2EP20223- 0002), and under the RIE2020 Industry Alignment Fund – Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). (Corresponding author: Ziwei Liu.) • The authors are with S-Lab, Nanyang Technological University, Sin- gapore 637335 (email: beichen002@ntu.edu.sg; haozhe.xie@ntu.edu.sg; zhaoxi001@ntu.edu.sg; fangzhou001@ntu.edu.sg; ziwei.liu@ntu.edu.sg) * denotes equal contribution. Before 2018 2019 2020 2021 2022 2023 2024 2025 0 20 40 60 80 100 120 61 93 40 14 20 9 7 5 29 # Papers Procedural Generation Neural 3D-based Generation Image-based Generation Video-based Generation Fig. 1. Annual statistics of 3D scene generation papers in computer vision conferences, journals, and preprints. The notable rise in pub- lications and the evolving trends in recent years highlight the need for a comprehensive survey. Note that the data for 2025 reflects papers published up until April 30th. structures by learning real-world distributions. Combined with innovations in 3D representations like NeRF [31] and 3D Gaussians [32], neural 3D-based generation methods enhance geometry fidelity, rendering efficiency, and view consistency, making them ideal for photorealistic scene syn- thesis and immersive virtual environments. Starting from a single image, image-based scene generation methods leverage camera pose transformations and image outpaint- ing to iteratively synthesize perpetual views [33], [34] or panoramic local environments [35], [36]. Benefit from the rapid advancement of video diffusion models [37], [38], video generation quality has significantly improved, leading to a surge in 3D scene generation research over the past two years. These methods formulate 3D scene generation as a form of video generation and enhance view consistency through temporal modeling [39]. The integration of dynamic 3D representations [40], [41] further facilitates the synthesis of immersive and dynamic environments [42], [43]. 2 3D Scene Generation Introduction (§ 1) Preliminaries (§ 2) Task Definition and Formulation (§ 2.1) 3D Scene Representa- tions(§ 2.2) Voxel Grid Point Cloud Mesh Neural Fields 3D Gaussians Image Sequence Generative Models (§ 2.3) Autoregressive Model VAE GAN Diffusion Model Procedural Generator Methods: A Hierarchical Taxonomy (§ 3) Procedural Gen. (§ 3.1) Rule-based Gen. (§ 3.1.1) Optm.-based Gen. (§ 3.1.2) LLM-based Gen. (§ 3.1.3) Neural 3D-based Gen. (§ 3.2) Scene Parameters (§ 3.2.1) Scene Graph (§ 3.2.2) Semantic Layout (§ 3.2.3) Implicit Layout (§ 3.2.4) Image-based Gen. (§ 3.3) Holistic Gen. (§ 3.3.1) Iterative Gen. (§ 3.3.2) Video-based Gen. (§ 3.4) Two-stage Gen. (§ 3.4.1) One-stage Gen. (§ 3.4.2) Datasets and Evaluation (§ 4) Datasets (§ 4.1) Indoor Datasets (§ 4.1.1) Natural Datasets (§ 4.1.2) Urban Datasets (§ 4.1.3) Evaluation (§ 4.2) Metrics-based Eval. (§ 4.2.1) Benchmark-based Eval. (§ 4.2.2) Human Eval. (§ 4.2.3) Applications and Tasks (§ 5) 3D Scene Editing (§ 5.1) Human-Scene Interaction (§ 5.2) Embodied AI (§ 5.3) Robotics (§ 5.4) Autonomous Driving (§ 5.5) Challenges and Future Directions (§ 6) Challenges (§ 6.1) Future Directions (§ 6.2) Fig. 2. The overall structure of our comprehensive survey. Our survey presents three core contributions: 1) a summary of key repre- sentations and generative models in 3D scene generation, 2) a hierar- chical taxonomy systematically organizing intertwined papers with in- depth analysis, and 3) an exploration of datasets, evaluation metrics, applications, along with an outlook on challenges and future directions. Compared to generating 3D objects and avatars, gen- erating 3D scenes presents significantly greater challenges across several dimensions. 1) Scale: Objects and avatars typically exist within a fixed, limited spatial extent, while scenes must accommodate multiple entities across a much larger and more variable spatial scale. 2) Structural com- plexity: Scenes involve complex spatial and semantic re- lationships among diverse objects, requiring the model to ensure both functional coherence and overall plausibility. 3) Data availability: While large-scale datasets for object- and avatar-level generation are abundant, high-quality, an- notated 3D scene datasets remain scarce and expensive to collect. 4) Fine-grained control: Scene generation often demands user control over attributes like object placement, zoning, and style, which remain difficult to incorporate in a flexible and interpretable way. Despite rapid progress in 3D scene generation, the field lacks a comprehensive survey that systematically catego- rizes existing approaches, highlights key challenges, and identifies future directions. Prior surveys focus on narrow domains such as procedural generation [44], [45], indoor scenes [46], [47], autonomous driving [48], and text-driven generation [49], [50], offering limited perspectives. Broader surveys on general 3D or 4D content generation [51], [52], [53], [54], [55], [56] often treat scene generation only pe- ripherally, leading to fragmented coverage. Although many existing works explore aspects of scene generation, their broader focus often leads to a fragmented understanding that overlooks critical components. Some works focus on specific subdomains, such as diffusion models [55], text- driven scene generation [52], or 4D generation [56], while others neglect key representations like 3D Gaussians [51] and image sequences [53], [54], as well as important paradigms like procedural and video-based generation [51], [53], [54]. Surveys on world models [18], [57], [58] primarily address video prediction in driving scenarios, offering only a partial view. These gaps call for a comprehensive, up-to- date survey that consolidates recent advances and maps out the evolving landscape of 3D scene generation. Contributions. This survey offers a structured overview of recent advances in 3D scene generation. We categorize existing methods into four types: procedural, neural 3D- based, image-based, and video-based generation, highlight- ing their paradigms and trade-offs. We also review key applications in scene editing, human-scene interaction, em- bodied AI, robotics, and autonomous driving. Additionally, we examine commonly used scene representations, datasets, and evaluation protocols, and identify current limitations in generative capacity, controllability, and realism. Finally, we outline future directions including higher fidelity, physics- aware and interactive generation, and unified perception- generation models. Scope. This survey primarily focuses on approaches for generating 3D scenes in 3D scene representations. Notably, these generative methods aim to synthesize diverse 3D scenes, whereas 3D reconstruction methods can only gen- erate a single scene from a given input. For a review of reconstruction approaches, readers may refer to [59], [60]. Furthermore, this survey excludes general video genera- tion [38], [61] and general 3D object generation [62], [63], [64] methods, even though they have demonstrated some capability in 3D scene generation. This survey complements existing reviews on 3D generative models [51], [52], [53], [54], [55], as none provide a comprehensive overview of 3D scene generation or its relevant insights. Organization. A summary of this survey’s structure is presented in Figure 2. Section 2 provides the foundational concepts, including task definition and formulation, 3D scene representations, and generative models. Section 3 cat- egorizes existing approaches into four types, detailing each category’s paradigm, strengths, and weaknesses. Section 4 introduces relevant datasets and evaluation metrics. Sec- tion 5 reviews various downstream tasks related to 3D scene generation. Finally, Section 6 discusses current challenges, future directions, and concludes the survey. 2 PRELIMINARIES 2.1 Task Definition and Formulation 3D scene generation maps an input x (e.g., random noise, text, images, or other conditions) to a 3D scene representa- 3 tion S (Sec. 2.2) using a generative model G (Sec. 2.3). G : x →S (1) The generated scene S is spatially coherent, implicitly or explicitly defines 3D geometry, and enables multi-view ren- dering or 3D reconstruction. 2.2 3D Scene Representations Various 3D scene representations have been developed and utilized in computer vision and graphics. In this section, we provide an overview of the key 3D scene representations, discussing their structures, properties, and suitability for 3D scene generation. Voxel Grid. A voxel grid is a 3D array V ∈RH×W ×D, where H, W, and D represent the height, width, and depth of the grid, respectively. Each voxel stores properties such as occupancy or signed distance values [65], enabling struc- tured volumetric scene representation. Point Cloud. A point cloud is an unordered set of N 3D points P = {pi | pi ∈R3}N i=1 that approximates an object’s surface. Unlike voxel grids, point clouds are sparse, unstruc- tured, memory-efficient, and are commonly generated from depth sensors, LiDAR, and structure-from-motion [66]. Mesh. A polygonal mesh M = {MV , ME, MF } defines a 3D surface through vertices MV (points in space), edges ME (pairwise connections between vertices), and faces MF (flat polygons, such as triangles or quads). It provides explicit connectivity information, making them ideal for modeling the surfaces of 3D scenes. Neural Fields. Signed Distance Field (SDF) [67] and Neural Radiance Field (NeRF) [31] are continuous implicit functions that can be parameterized by neural networks. SDF maps a spatial position x ∈R3 to a signed distance s(x) ∈R, defin- ing a surface as its zero-level set. NeRF maps x and a view direction r ∈R3 to a volume density σ(x, r) ∈R+ and color c(x, r) ∈R3. SDF is rendered using sphere tracing [68], while NeRF uses differentiable volume rendering [69], [70]. 3D Gaussians. 3D Gaussians [32] represent 3D scenes using N 3D Gaussian primitives G = {(µi, Σi, ci, αi)}N i=1, where µi ∈R3 is the center, Σi ∈R3×3 defines the anisotropic shape, ci ∈R3 is the RGB color, and αi ∈[0, 1] is the opacity. The image can be rendered by rasterizing 3D Gaussians onto a 2D plane. Image Sequence. An image sequence, implicitly encoding the scene’s 3D structure with N images from different viewpoints, e.g., C = {Ii ∈RH×W ×3}N i=1, is a crucial 3D scene representation widely used in image- and video-based generation methods, where the 3D structure can be inferred through multi-view reconstruction. 2.3 Generative Models Generative models synthesize data by either learning sta- tistical patterns (e.g., AR models, VAEs [71], GANs [29], diffusion models [30]) or applying predefined rules (e.g., procedural generators). While the former approximates data distributions for novel outputs, the latter constructs struc- tured 3D scenes through deterministic or stochastic logic without learned priors. In this section, we briefly introduce representative generative models in 3D scene generation, highlighting their characteristics and mechanisms. Autoregressive Models (AR models) generate data sequen- tially, where each element is conditioned on the previously generated elements. A common formalization of AR models involves the factorization of the joint probability distri- bution of data into a product of conditional probabilities p(x) = QT t=1 p(xt|x<t). This decomposition follows di- rectly from the chain rule of probability and ensures that each element xt is generated sequentially, conditioned on all previous elements. The probability p(xt|x<t) is modeled by deep generative networks [72], [73], which learn to capture the dependencies between the data. Variational Autoencoders (VAEs) [71] are generative mod- els that encode data into a probabilistic latent space and decode it back. Given an input x, the encoder maps it to a latent distribution q(z|x) parameterized by a mean µ and variance σ2, where z = µ+σ ·ϵ, ϵ ∼N(0, I). The decoder reconstructs x from z. Using the reparameterization trick, VAEs enable backpropagation through stochastic sampling. The loss function combines reconstruction loss (to preserve input features) and KL divergence (to regularize the latent space), which allows VAEs to generate smooth and mean- ingful data variations. However, since VAEs optimize like- lihood, they often spread probability mass beyond the true data manifold, causing blurry and less detailed generated samples [74], [75]. Generative Adversarial Networks (GANs) [29] consist of two networks – the Generator G and the Discriminator D – that compete in a minimax game. The Generator G takes random noise z and generates fake data G(z), while the Discriminator D tries to distinguish real data x from fake data G(z). The objective is to optimize the Generator to cre- ate realistic data that the Discriminator cannot distinguish from real data, and to train the Discriminator to classify real and fake data correctly, which can be represented by the objective function min G max D Ex∼pdata(x)[log D(x)] + Ez∼pz(z)[log(1 −D(G(z)))] (2) where pdata(x) is the real data distribution and pz(z) is the random noise distribution. A key drawback of GANs is that they can be difficult to train, often suffering from issues like mode collapse and instability [76]. Diffusion Models [30] are generative models that operate by gradually adding noise to data in a forward process, transforming it into pure noise, and then learning to reverse this process by denoising to recover the original data. The forward process is modeled as a Markov chain, where each step xt is obtained by adding Gaussian noise to the previous step xt−1, defined by xt = √1 −βtxt−1 + √βtϵt, where ϵt is Gaussian noise and βt controls the noise schedule. The reverse process aims to model p(xt−1|xt), learning how to reverse the added noise and regenerate the original data. While these models generate high-quality data and are more stable than GANs, they are computationally expensive and slow due to the iterative denoising process [77]. Procedural Generators [44] are algorithmic systems that synthesize 3D scenes through iterative application of para- metric rules and mathematical operations. These generators transform an initial state S0 (e.g., a geometric primitive or empty scene) into a structured output Sn via recursive or iterative processes governed by St+1 = R(St, Θ), where 4 TABLE 1 General comparison of 3D scene generation categories across key characteristics. Individual methods may vary. Characteristic Procedural Gen. Neural 3D-based Gen. Image-based Gen. Video-based Gen. Realism ⋆⋆⋆: Stylized or repeti- tive textures ⋆⋆⋆: Limited by the quality of 3D datasets ⋆⋆⋆: Photorealistic but lacks accurate depth ⋆⋆⋆: High-quality tem- poral coherence Diversity ⋆⋆⋆: Limited variations due to predefined assets ⋆⋆⋆: Diversity depends on training data ⋆⋆⋆: Rich variations from real-world images ⋆⋆⋆: Rich variations from real-world videos View Consistency ⋆⋆⋆: 3D-consistent rep- resentations/rendering ⋆⋆⋆: 3D-consistent rep- resentations ⋆⋆⋆: Usually adopts ex- plicit 3D representation ⋆⋆⋆: Implicit geometry estimation, less reliable Semantic Consistency ⋆⋆⋆: Procedure ensures cross-view coherence ⋆⋆⋆: 3D priors preserve cross-view coherence ⋆⋆⋆: No global context; lack cross-view coherence ⋆⋆⋆: Frame-level coher- ence but possible drift Efficiency ⋆⋆⋆: Usually slow; can be faster for lower quality ⋆⋆⋆: Costly due to com- plex representations ⋆⋆⋆: Efficient per frame but lacks reuse ⋆⋆⋆: Costly due to se- quential inference Controllability ⋆⋆⋆: Limited by prede- fined rules or constraints ⋆⋆⋆: Rarely support text or image conditions ⋆⋆⋆: Controlled mainly by text or images ⋆⋆⋆: Controlled by di- verse conditions Physical Plausibility ⋆⋆⋆: Guaranteed by physics engines ⋆⋆⋆: Constrained by 3D geometry ⋆⋆⋆: Hard to infer from the static context ⋆⋆⋆: Achieved through temporal modeling R represents a set of predefined rules (e.g., subdivision, perturbation, or spatial partitioning), and Θ denotes tun- able parameters (e.g., seed values, perturbation amplitudes, or recursion depth). The rules R define deterministic or constrained stochastic operations, ensuring reproducibility when Θ is fixed. 3 METHODS: A HIERARCHICAL TAXONOMY We classify existing methods into four categories based on their generation paradigms illustrated in Figures 3 to 6: • Procedural Generation creates 3D scenes using pre- defined rules, enforced constraints, or prior knowl- edge from LLMs, resulting in high-quality outputs that integrate seamlessly with graphics engines. • Neural 3D-based Generation employs 3D-aware generative architectures to synthesize scene layouts for object placement or directly generate 3D rep- resentations such as voxels, point clouds, meshes, NeRFs, and 3D Gaussians. • Image-based Generation uses 2D image generators to synthesize images either in one step or iteratively, sometimes followed by 3D reconstruction for geo- metric consistency. • Video-based Generation uses video generators to create both 3D scenes with spatial movement and 4D scenes that evolve over time, capturing dynamic changes in both space and time. 3.1 Procedural Generation Procedural generation methods automatically generate 3D scenes by following predefined rules or constraints. They are widely used in computer graphics to create diverse environments, including terrains, vegetation, rivers, roads, rooms, buildings, and entire cities. As shown in Table 1, pro- cedural generation methods offer high efficiency and spatial consistency, but often require careful tuning to achieve real- ism and user control. The paradigms of these methods are illustrated in Figure 3, which can be further categorized into rule-based, optimization-based, and LLM-based generation. OR 3D Scene  Representation 3D Scene  Representation 3D Scene  Representation Code Layout Constraints Rules LLM Grammar Fractal Example Simulation a b c Text ①SG./ Ret. ②API ① ② Optm. Layout Ret. Parameters  Interaction Parameters  Interaction Fig. 3. The paradigms of procedural methods for 3D scene gen- eration. (a) Rule-based generation methods follow predefined rules to generate 3D scenes. (b) Optimization-based generation finds an optimized scene under different constraints. (c) LLM-based generation uses large language models (LLMs) for tasks like layout design and object selection, or to generate code that controls other generators. Note that dashed arrows denote optional operations. “Optm.”, “Ret.”, and “SG.” denote “Optimization”, “Retrieval”, and “Shape Generation”, respectively. “Interaction” refers to user actions such as click, drag, or selection during the generation process. 3.1.1 Rule-based Generation Rule-based procedural generation encompasses a range of approaches that construct 3D scenes through explicit rules and algorithms. These methods directly generate scene geometry, which is then rendered for visualization. Common techniques include fractal-based, grammar-based, simulation-driven, and example-based generation. Fractals [121], [122], [123] are mathematical structures that exhibit self-similarity across scales. Fractal-based meth- ods are widely applied in terrain modeling and texture synthesis, as they efficiently generate visually complex pat- terns while requiring minimal storage. Techniques such as midpoint displacement [124], [125] and fractional Brownian motion [126] (fBM) generate multi-scale details that resem- ble natural landscapes. Grammar-based methods consist of an alphabet of sym- bols, an initial axiom, and a set of rewriting rules. Each gen- erated symbol encodes geometric commands for complex shape generation. CityEngine [3] extends L-systems [127] for the generation of road networks and building geometry to create cities. M¨uller et al. [6] build upon shape grammars 5 TABLE 2 Summary and comparison of representative works for 3D scene generation. The table compares scene types (e.g., I indoor, N nature, U urban) and conditioning modalities (e.g., X unconditioned, T text, I image, ID top-down image, C constraint, M motion, G scene graph, V semantic volume, SD semantic map, L LiDAR, B bounding box, C camera pose, A user action) across various 3D scene representations. Note that “Optm.”, “Gen.”, “Rep.”, and “Seq.” are short for “Optimization”, “Generative”, “Representation”, and “Sequence”, respectively. Category Method Venue Gen. Model Scene Type Condition 3D Scene Rep. Procedural Rule-based Musgrave et al. [78] SIGGRAPH’89 Procedural N X Mesh CityEngine [3] SIGGRAPH’01 Procedural U ID Mesh Cordonnier et al. [79] TOG’17 Procedural N V Mesh Infinigen [80] CVPR’23 Procedural N X Mesh Optm.-based Make it home [28] TOG’11 Procedural I X Mesh Wu et al. [27] CGF’18 Procedural I C Mesh ProcTHOR [15] NeurIPS’22 Procedural I ID Mesh Infinigen Indoors [81] CVPR’24 Procedural I C Mesh LLM-based LayoutGPT [82] NeurIPS’23 Procedural I T Mesh 3D-GPT [83] 3DV’25 Procedural N T Mesh SceneX [84] AAAI’25 Procedural N U T Mesh Neural 3D-based Scene Parameters DeepSynth [85] TOG’18 GAN I ID Mesh ATISS [86] NeurIPS’21 Autoregressive I ID Mesh MIME [87] CVPR’23 Autoregressive I ID + M Mesh DiffuScene [88] CVPR’24 Diffusion I T Mesh Scene Graph PlanIT [89] TOG’19 Autoregressive I ID Mesh GRAINS [90] TOG’19 VAE I X Mesh Graph-to-3D [91] ICCV’21 VAE I G SDF CommonScenes [92] NeurIPS’23 Diffusion I G Mesh InstructScene [93] ICLR’24 Diffusion I T Mesh Semantic Layout GANcraft [94] ICCV’21 GAN N V NeRF CC3D [95] ICCV’23 GAN I U SD NeRF InfiniCity [96] ICCV’23 GAN U X NeRF SceneDreamer [97] TPAMI’23 GAN N X NeRF CityDreamer [98] CVPR’24 GAN U X NeRF Comp3D [99] 3DV’24 Diffusion N T + V NeRF BlockFusion [100] TOG’24 Diffusion I N U SD SDF Implicit Layout GSN [101] ICCV’21 GAN I X / I NeRF GAUDI [102] NeurIPS’22 Diffusion I X / T / I NeRF NeuralField-LDM [103] CVPR’23 Diffusion U X / I NeRF X 3 [104] CVPR’24 VAE&Diffusion U X / L Voxel Grid Director3D [105] NeurIPS’24 Diffusion I N U T 3D Gaussians Image-based Holistic ImmerseGAN [106] 3DV’22 GAN I N U T / I Image Seq. MVDiffusion [36] NeurIPS’23 Diffusion I N U T Image Seq. PanFusion [107] CVPR’24 Diffusion I N U T Image Seq. PERF [108] TPAMI’24 Diffusion I I NeRF LayerPano3D [109] SIGGRAPH’25 Diffusion I N U T 3D Gaussians Iterative PixelSynth [110] ICCV’21 VAE I I Point Cloud GFVS [111] ICCV’21 GAN I N I Image Seq. Infinite Nature [33] ICCV’21 GAN N I Image Seq. 3D Cinemagraphy [112] CVPR’23 GAN N I Point Cloud Text2Room [113] ICCV’23 Diffusion I T / I Mesh Text2NeRF [114] CVPR’24 Diffusion I N U T / I NeRF WonderJourney [115] CVPR’24 Diffusion I N U T / I Point Cloud LucidDreamer [116] arXiv’23 Diffusion I N U T / I 3D Gaussians Video-based Two-stage 4Real [117] NeurIPS’24 Diffusion I N U T 3D Gaussians DimensionX [42] arXiv’24 Diffusion I N U T / I 3D Gaussians One-stage MagicDrive [39] ICLR’24 Diffusion U T + SD + B + C Image Seq. Vista [118] NeurIPS’24 Diffusion U T / I / A Image Seq. GenXD [119] ICLR’25 Diffusion I N U I / C Image Seq. 4K4DGen [43] ICLR’25 Diffusion N U I 3D Gaussians GameGen-X [120] ICLR’25 Diffusion N U T / A Image Seq. [128] to model highly detailed 3D buildings. Simulation-based procedural generation creates realistic 3D environments by modeling natural and artificial pro- cesses. Some methods simulate erosion effects [78], [129], [130] and hydrology [131], [132], [133] to generate ter- rain with high fidelity. Vegetation simulations model plant growth under resource competition [79], [134], [135] and cli- mate change [136]. In urban contexts, ecosystem-based ap- proach populates cities with vegetation [137], while others 6 simulate city growth and resource distribution to generate settlements that evolve organically over time [138], [139]. Example-based procedural methods are proposed to im- prove controllability. These techniques take a small user- provided example and generate a larger scene by expanding its boundary [140], [141] or matching features [142], [143]. Inverse procedural generation attempts to provide high- level control over the generation process. These methods apply optimization functions to infer parameters from pro- cedural algorithms [26], [144] or learn a global distribution for scene arrangement [145]. The aforementioned techniques are often combined to harness their complementary strengths for generating large- scale, diverse scenes. For example, Citygen [146] integrates road networks and building generation for cityscapes, while Infinigen [80] combines material, terrain, plant, and creature generators for infinite natural scenes. 3.1.2 Optimization-based Generation Optimization-based generation formulates scene synthesis as an optimization problem that minimizes objectives en- coding predefined constraints. These constraints, typically derived from physics rules, functionality, or design prin- ciples, are embedded into cost functions and optimized using stochastic or sampling-based methods. Alternatively, statistical approaches learn spatial relationships from data and guide the layout process through probabilistic sam- pling. Some systems support user-defined constraints and user interactions to enable controllable and semantically meaningful generation. Some approaches formulate physical and spatial con- straints as cost functions and apply stochastic optimization methods for scene generation. Physical-level constraints include object interpenetration, stability, and friction [147]. Layout-level constraints, including functional relationships (e.g., co-occurrence, accessibility), interior design guide- lines(e.g., symmetry, alignment, co-circularity), and human behavior patterns, have also been considered [28], [148], [149]. High-level constraints such as scene type, size, and layout can be specified by users [15], [27], [150], enabling more controllable and semantically meaningful scene syn- thesis. Leveraging existing procedural generation pipelines, Infinigen Indoors [81] introduces a constraint specifica- tion API, allowing users to define custom constraints and achieve highly controllable scene generation. Other methods adopt data-driven models to learn object arrangement patterns from annotated data, transforming scene generation into a probabilistic sampling problem. Bayesian networks are commonly used [151], [152], [153] to capture conditional dependencies between objects, while graph-based models [154], [155], [156] model spatial hierar- chies or relational structures to improve spatial reasoning and object placement accuracy. 3.1.3 LLM-based Generation Large Language Models [157] (LLMs) and Vision-language models [158] (VLMs) have introduced a new paradigm in procedural generation by enabling text-driven scene synthe- sis, allowing users to specify environments through natural language descriptions, offering greater flexibility and user control over scene design. Several approaches use LLMs to generate scene layouts, such as object parameters [82], [159], [160], [161], [162], [163], [164], [165], [166] and scene graph [167], [168], [169], [170], [171], [172]. Based on these layouts, 3D geometries can be obtained through object retrieval or shape generation. Specifically, LayoutGPT [82] guides LLMs using generation prompts and structural templates to produce object param- eters for retrieving assets. CityCraft [161] guides land-use planning with LLMs and retrieves building assets from a database to construct detailed urban environments. I- Design [167] and Deng et al. [168] use graph-based ob- ject representations to model inter-object semantics more effectively. To support more stylized and versatile scene generation, GraphDreamer [170] and Cube [172] generate scene graphs via LLMs, treating nodes as objects and en- abling compositional scene generation through 3D object generation models. The Scene Language [165] introduces a language-based scene representation composed of a pro- gram, words, and embeddings, which can be generated by LLMs and rendered using traditional, neural, or hybrid graphics pipelines. Other methods utilize LLMs as agents to control pro- cedural generation by adjusting parameters of rule-based system or modifying operations within procedural gener- ation software. Liu et al. [173] employ LLMs to fine-tune parameters in rule-based landscape generation, optimizing procedural workflows with learned priors. 3D-GPT [83] and SceneCraft [174] generate Python scripts to control existing procedural frameworks, such as Infinigen [80] and Blender1, allowing direct manipulation of procedural assets. Holodeck [175] generates 3D environment through multiple rounds of conversation with an LLM, including floor and wall texturize, door and window generation, object selection and placement. CityX [24] and SceneX [84] use a multi- agent system for different stages of generation, producing Python codes for layout, terrain, building, and road gener- ation through Blender rendering. WorldCraft [176] further incorporates object generation and animation modules. 3.2 Neural 3D-based Generation Neural 3D-based methods generate 3D scene representa- tions using generative models trained on datasets with 3D annotations. Recent advancements in NeRF and 3D Gaus- sians have further enhanced the fidelity and realism. As shown in Table 1, these methods achieve high view and semantic consistency, but their controllability and efficiency remain limited. As shown in Figure 4, the methods are categorized into four types based on the spatial arrangement that controls the layout of generated 3D scenes: scene pa- rameters, scene graph, semantic layout, and implicit layout. 3.2.1 Scene Parameters Scene parameters offer a compact way to represent object arrangements, implicitly capturing inter-object relationships without relying on explicit scene graphs. These parameters typically encompass an object’s location, size, orientation, class, and shape latent code. As illustrated in Figure 4a, 1. https://www.blender.org/ 7 Scene Graph Parameter  Generator a 3D Scene  Representation Text  Image Parameters Loc. Size Rot. 6D Pose Shape Cls. Vec. Retrieval Shape Gen. Semantic Layout OR Neural 3D  Generator c Text Implicit Layout d Text  Image OR b Text Retrieval Shape Gen. 3D Scene  Representation 3D Scene  Representation 3D Scene  Representation Fig. 4. The paradigms of neural 3D-based methods for 3D scene generation. These paradigms use (a) scene parameters, (b) scene graphs, (c) semantic layouts, and (d) implicit layouts as intermediate representations to control the spatial arrangement of generated 3D scenes. These representations, either user-provided or produced by generative models, are then converted into 3D scene representations (e.g., voxel grid, mesh, NeRF, or 3D Gaussians) via retrieval or decoding. Note that dashed arrows denote optional operations. “Enc.” and “Dec.” stand for “Encoder” and “Decoder”, respectively. “Shape Gen.” repre- sents “Shape Generation”. these methods first generate scene parameters as an inter- mediate representation, which is then used to synthesize the final 3D scene. DeepSynth [85], FastSynth [177], Zhang et al. [178], and Sync2Gen [179] adopt CNN-based architectures that utilize top-down image-based scene representations, sequentially inserting objects by predicting their parameters. Subsequent works explore more advanced models, such as transform- ers and diffusion models. ATISS [86], SceneFormer [180], COFS [181], and Nie et al. [182] use transformers to au- toregressively generate object parameters. RoomDesigner [183] refines this process by decoupling layout and shape generation, ensuring shape compatibility in indoor scenes. CASAGPT [184] leverages cuboids as intermediate ob- ject representations to better avoid object collisions. De- BaRA [185] adopts a diffusion model for object parameter generation, while PhyScene [186] further integrates physical constraints for physical plausibility and interactivity. To improve controllability in text-driven scene genera- tion, RelScene [187] employs BERT [188] to align spatial relationships with textual descriptions in latent space. Dif- fuScene [88] leverages latent diffusion models [189] to gen- erate object parameters from text inputs, followed by object retrieval. Ctrl-Room [190] and SceneFactor [191] employ LDMs to generate coarse object layouts from text prompts, with fine-grained appearance obtained via panorama gen- eration and geometric diffusion model, respectively. Epstein et al. [192], SceneWiz3D [193], and DreamScene [194] adopt a multi-stage approach, first generating an initial object lay- out, then refining object geometry using Score Distillation Sampling (SDS) [195], followed by a global refinement step to improve compositional consistency. Human movement and interactions often influence the organization of environments, where motion patterns and physical contact inform the arrangement of objects and scene layouts. Pose2Room [196] introduces an end-to-end generative model that predicts the bounding boxes of fur- niture in a room from human motion. SUMMON [197] and MIME [87] further improve semantic consistency and physical affordances by generating objects with meshes that align with human-scene contact. Vuong et al. [198] propose a multi-conditional diffusion model that integrates text prompts to enhance controllability. To ensure physically plausible layouts free from contact or collisions, INFER- ACT [199] optimizes scene layout generation while simul- taneously simulating human movement in a physics-based environment using reinforcement learning. 3.2.2 Scene Graph Scene graphs offer a structured, symbolic representation of 3D scenes, with nodes representing objects and edges capturing their spatial relationships. Incorporating scene graphs allows generative models to enforce spatial con- straints and preserve relational consistency, facilitating the creation of well-structured 3D environments. Following the paradigm illustrated in Figure 4b, scene graphs, whether generated by models or provided as input, function as lay- out priors that guide the decoding process to create 3D scene representations by object retrieval or shape generation. Early data-driven approaches [200], [201], [202], [203] represent spatial relationships between objects using scene graphs, which serve as a blueprint for 3D scene generation through object retrieval and placement. Subsequent works enhance graph representations and introduce advanced generative models. PlanIT [89] employs a deep graph gen- erative model to synthesize scene graphs, followed by an image-based network for object instantiation. GRAINS [90] adopts a recursive VAE to learn scene structures as hierar- chical graphs, which can be decoded into object bounding boxes. 3D-SLN [204] utilizes scene graphs as a structural prior for 3D scene layout generation, ensuring spatial co- herence, and further incorporates differentiable rendering to synthesize realistic images. Meta-Sim [205] and Meta- Sim2 [206] use scene graphs to structure scene generation, optimizing parameters for visual realism and synthesizing diverse 3D scenes using rendering engines. Previous methods enable scene generation from scene graphs but rely on object retrieval or direct synthesis, lim- iting geometric diversity. To address this, Graph-to-3D [91] introduces a graph-based VAE that jointly optimizes layout and shape. SceneHGN [207] represents scenes as hierarchi- cal graphs spanning from high-level layout to fine-grained object geometry, using a hierarchical VAE for structured generation. CommonScenes [92] and EchoScene [208] pro- pose scene graph diffusion models with a dual-branch de- sign for layout and shape, capturing both global scene-object relationships and local inter-object interactions. MMG- Dreamer [209] introduces a mixed-modality graph for metic- ulous control of object geometry. Recent methods improve controllability by integrating human input. SEK [210] encodes scene knowledge as a scene graph within a conditioned diffusion model for sketch- driven scene generation. InstructScene [93] integrates text encoders with graph-based generative models for text- driven scene synthesis. To generalize scene-graph-based generation to broader scenes, Liu et al. [211] map scene graphs onto a Bird’s Eye View (BEV) embedding map, which guides a diffusion model for large-scale outdoor 8 scene synthesis. HiScene [212] leverages VLM-guided oc- clusion reasoning and video diffusion-based amodal com- pletion to generate editable 3D scenes with compositional object identities from a single isometric view. 3.2.3 Semantic Layout Semantic layouts serve as an intermediate representation that encodes the structural and semantic organization of a 3D scene. It provides high-level guidance for 3D scene gen- eration, ensuring controllability and coherence in the place- ment of objects and scene elements. As shown in Figure 4c, semantic layouts, whether user-provided or generated, act as precise constraints for generative models, guiding 3D scene generation while enabling optional textural prompts for style control. A 2D semantic layout consists of a 2D semantic map, sometimes including additional maps such as height maps, viewed from a top-down perspective. CC3D [95] gener- ates a 3D feature volume conditioned on a 2D semantic map, which serves as a NeRF for neural rendering. Berf- Scene [213] incorporates positional encoding and low-pass filtering to make the 3D representation equivariant to the BEV map, enabling controllable and scalable 3D scene gen- eration. Frankenstein [214] encodes scene components into a compact triplane [215], generated via a diffusion process conditioned on a 2D semantic layout. BlockFusion [100] introduces a latent triplane extrapolation mechanism for unbounded scene expansion. Incorporating a height map with the semantic map enables the direct conversion of 2D layouts into 3D voxel worlds, essential for urban and natu- ral scenes where building structures and terrain elevation provide important priors. InfiniCity [96] utilizes Infinity- GAN [216] to generate infinite-scale 2D layouts, which are then used to create a watertight semantic voxel world, with textures synthesized through neural rendering. For natural scene generation, SceneDreamer [97] employs a neural hash grid to capture generalizable features across various land- scapes, modeling a space- and scene-varied hyperspace. To address the diversity of buildings in urban environments, CityDreamer [98] and GaussianCity [217] break down the generation process into distinct background and building components. CityDreamer4D [218] further integrates dy- namic traffic systems to generate an expansive 4D city. A 3D semantic layout offers enhanced capability to rep- resent more complex 3D layouts compared to 2D, improving controllability, typically by using voxels or 3D bounding boxes. GANcraft [94] uses voxels as the 3D semantic lay- out, optimizing a neural field with pseudo-ground truth and adversarial training. UrbanGIRAFFE [219] and Dis- CoScene [220] break down the scene into stuff, objects, and sky, and adopt compositional neural fields for scene generation. By incorporating score distillation sampling (SDS) [195], 3D semantic layouts offer better control over text-guided scene generation, improving the alignment of generated scenes with textual descriptions. Comp3D [99], CompoNeRF [221], Set-the-Scene [222], and Layout-your- 3D [223] generate 3D scenes with compositional NeRFs using pre-defined customizable layouts as object proxies. SceneCraft [224] and Layout2Scene [225] generate indoor scenes by distilling the pretrained diffusion models. Ur- ban Architect [226] integrates geometric and semantic con- straints with SDS, leveraging the scalable hash grid to ensure better view-consistency in urban scene generation. 3.2.4 Implicit Layout Implicit layouts are feature maps that encode the spatial structure of a 3D scene. As shown in Figure 4d, these layouts manifest as latent features of different dimensions. Encoders learn to embed 3D scene layout information into latent feature maps, which are then used by the decoder to generate 3D scenes in the form of NeRF, 3D Gaussians, or voxel grids. Recent advances in representations like NeRFs and 3D Gaussians have enabled neural networks to directly gener- ate and render high-fidelity RGB images from latent feature maps. Some methods leverage these representations to pro- duce appearance-consistent 3D scenes with photorealistic quality. NeRF-VAE [227] encodes shared information across multiple scenes using a VAE. GIRAFFE [228] represents scenes as compositional generative neural fields to disen- tangle objects from background. GSN [101] and Persistent Nature [229] adopt GAN-based architectures to generate 2D latent grids as implicit scene layouts, which are sampled along camera rays to guide NeRF rendering. GAUDI [102] employs a diffusion model to learn scene features and cam- era poses jointly, decoding them into a tri-plane and pose for NeRF-based rendering control. NeuralField-LDM [103] decomposes NeRF scenes into a hierarchical latent struc- ture that includes 3D voxel, 2D BEV, and 1D global repre- sentations. Hierarchical diffusion models are then trained on this tri-latent space for generation. Director3D [105] uses a Gaussian-driven multi-view latent diffusion model to generate pixel-aligned and unbounded 3D Gaussians along a generated trajectory, followed by SDS refinement. Prometheus [230] and SplatFlow [231] learn a compressed latent space from multi-view images, and decode this latent space into pixel-aligned 3DGS representations. Another branch of work focuses more on generating semantic structure and scene geometry, typically using voxel grids as representations. These methods are not im- mediately renderable but can be textured through exter- nal rendering pipelines. Lee et al. [232] introduce discrete and latent diffusion models to generate and complete 3D scenes consisting of multiple objects, represented as seman- tic voxel grids. Due to the computational challenges posed by voxel grids, DiffInDScene [233], PDD [234], X 3 [104], and LT3SD [235] use a hierarchical diffusion pipeline to generate large-scale and fine-grained 3D scenes efficiently. SemC- ity [236] employs a tri-plane representation for 3D semantic scenes, allowing for generation and editing by manipulating the tri-plane space during diffusion. NuiScene [237] encodes the local scene chunks into vector sets, and uses a diffu- sion model to generate neighboring chunks for unbounded outdoor scenes. DynamicCity [238] tackles dynamic scene generation by employing Padded Rollout to unfold Hex- plane [239] into 2D feature maps and applying diffusion for denoising, enabling 4D scene generation. 3.3 Image-based Generation The limited availability of annotated 3D datasets constrains the generation of 3D scenes. Image-based generation at- tempts to bridge the gap between 2D and 3D generation. As 9 a Image  Generator Image  Generator •  Text •  Image Render Camera Shift b 3D Scene  Representation •  Text •  Image 3D Scene  Representation Fig. 5. The paradigms of image-based methods for 3D scene gener- ation. (a) Holistic generation creates an entire scene image in one step. (b) Iterative generation progressively extends the scene by extrapolating a sequence of images. shown in Table 1, they offer photorealism and diversity with efficient per-frame processing but struggle with depth accu- racy, long-range semantic consistency, and view coherence. The methods fall into two categories: holistic and iterative generation, as illustrated in Figure 5. Holistic generation produces a complete scene image in a single step, while iterative generation gradually expands the scene through extrapolation, generating a sequence of images. 3.3.1 Holistic Generation As shown in Figure 5a, holistic generation in 3D scene gen- eration often relies on panoramic images, which provide a full 360° × 180° field of view, ensuring spatial continuity and explicit geometric constraints. This makes them particularly effective in mitigating scene inconsistencies that arise in perspective views. Given an RGB image, early methods [240], [241], [242], [243], [244], [245] use GANs for image outpaint- ing to fill masked regions in panoramas. More recent ap- proaches employ advanced generative models (e.g., Co- ModGAN [246] and VQGAN [247]) for greater diversity and content control. ImmerseGAN [106] leverages CoModGAN for user-controlled generation. OmniDreamer [248] and Dream360 [249] use VQGAN to generate diverse and high- resolution panoramas. Leveraging advances in latent dif- fusion models (LDM) [189], PanoDiffusion [250] enhances scene structure awareness by integrating depth into a bi- modal diffusion framework. Text-to-image models (e.g., CLIP [251], LDM [189]) en- able text-driven panorama generation. Text2Light [35] uses CLIP for text-based generation and hierarchical samplers to extract and piece together panoramic patches based on the input text. Some approaches [252], [253] leverage diffu- sion models to generate high-resolution planar panoramas. However, they fail to guarantee the continuity at image boundaries, which is essential in creating a seamless view- ing experience. To address this, MVDiffusion [36], DiffCol- lage [254], and CubeDiff [255] generate multi-view consis- tent images and align them into a closed-loop panorama for smooth transitions. StitchDiffusion [256], Diffusion360 [257], PanoDiff [258], and PanFusion [107] adopt padding and cropping strategies at boundaries to maintain the continuity. Recent methods extend single-view panorama genera- tion to multi-view for immersive scene exploration, fol- lowing two main strategies: one directly generates multi- view panoramic images with diffusion models [259], while the other applies 3D reconstruction (e.g., surface recon- struction [190], [260], [261], NeRF [108], and 3D Gaussian Splatting [109], [262], [263], [264], [265]) as post-processing. In this context, LayerPano3D [109] breaks the generated panorama into depth-based layers, filling in unseen content to help create complex scene hierarchies. Another research direction focuses on generating geo- metrically consistent street-view panoramas from satellite images. Some methods [266], [267], [268] integrate geomet- ric priors into GAN-based frameworks to learn cross-view mappings. Others [269], [270], [271] estimate 3D structures from satellite images and synthesize textures for rendered street-view panoramas. 3.3.2 Iterative Generation As shown in Figure 5b, iterative generation starts with an initial 2D image, either provided by the user or generated from text prompts. To generate large-scale 3D scenes, these methods progressively extrapolate the scene along a prede- fined trajectory. By expanding and refining content step by step, they continuously optimize the 3D scene representa- tion, enhancing geometric and structural coherence. Given a single image, early methods infer 3D scene representations and use them to render novel views. These representations include point clouds [110], [272], [273], [274], multi-plane images [275], [276], depth maps [277], and meshes [278]. Despite enabling fast rendering, these repre- sentations limit camera movement due to their finite spatial extent. To enable unrestricted camera movement, Infinite Nature [33], InfiniteNature-Zero [34], Pathdreamer [279], and SGAM [280] follow a “render-refine-repeat” manner, iteratively warping previous views and outpainting missing regions. DiffDreamer [281] improves multi-view consistency by conditioning on multiple past and future frames using a diffusion model. Rather than using explicit 3D represen- tations, GFVS [111] and LOTR [282] encode images and camera poses directly, using transformers to generate novel views. Tseng et al. [283], Photoconsistent-NVS [284], and ODIN [285] improve long-term view synthesis consistency with a pose-guided diffusion model. CAT3D [286] uses a multi-view LDM to generate novel views from input im- ages, followed by 3D reconstruction for interactive render- ing. Similarly, Bolt3D [287] generates scene appearance and geometry through multi-view diffusion but directly outputs 3D Gaussians to avoid time-consuming optimization. Text-driven scene generation boosts diversity and con- trollability by leveraging pretrained text-to-image diffu- sion models [189], [288]. Without requiring extensive domain-specific training, these methods iteratively shift the camera view, outpaint images based on text prompts. PanoGen [289], AOG-Net [290], PanoFree [291], OPa- Ma [292], and Invisible Stitch [293] iteratively outpaint images in perspective view and seamlessly stitch them into a panoramic scene. Other approaches leverage depth estimator [294], [295], [296] to merge RGB images into a unified 3D scene. SceneScape [297], Text2Room [113], and iControl3D [298] use 3D meshes as an intermediary proxy to fuse diffusion-generated images into a coherent 3D scene representation iteratively. WonderJourney [115] adopts a point cloud representation and leverages a VLM- guided re-generation strategy to ensure visual fidelity. 10 3D Scene  Representation a Generator 1 Generator 2 b Video  Generator Single- or Multi-view Video Ctrl. BEV       Action •  Text •  Image •  Text •  Image 3D Scene  Representation Multi-view Video Single-view Video OR Multi-view Static Video Fig. 6. The paradigms of video-based methods for 3D scene gen- eration. (a) Two-stage methods employ two generators, at least one being a video generator, to synthesize multi-view videos while utilizing dynamic 3D scene representations for consistency and exploration. (b) One-stage methods produce single or multi-view videos within a unified process and model, optionally optimizing dynamic 3D scene representa- tions. Note that dashed arrows denote optional operations. “Ctrl.” stands for “Control”. Text2NeRF [114] and 3D-SceneDreamer [299] adopt NeRF- based representations to mitigate error accumulation in geometry and appearance, improving adaptability across scenarios. Scene123 [300] further enhances photorealism by using a GAN framework, where the discriminator compares outputs from the video generator with those from the scene generator. By introducing 3D Gaussian Splatting [32], Lucid- Dreamer [116], Text2Immersion [301], WonderWorld [302], RealmDreamer [303], BloomScene [304], and Wonder- Turbo [305] adopt 3D Gaussians as 3D scene representa- tions for higher quality and faster rendering. Leveraging recent advancements in powerful large reconstruction mod- els [306], [307], [308], [309], [310], SynCity [311] enables training-free generation of high-quality 3D scenes by itera- tively performing image outpainting, 3D object generation, and stitching. Another research direction conducts iterative view syn- thesis and image animation simultaneously to build a dynamic 3D scene from a single image. 3D Cinemagra- phy [112] and Make-It-4D [312] use layered depth images (LDIs) to build feature point clouds and animate scenes via motion estimation and 3D scene flow. 3D-MOM [313] first optimizes 3D Gaussians by generating multi-view images from a single image, and then optimizes 4D Gaussians [40] by estimating consistent motion across views. 3.4 Video-based Generation Recent advances in video diffusion models [38], [61] have demonstrated significant progress in generating high- quality video content. Building on these advancements, video-based 3D scene generation methods produce image sequences, enabling the synthesis of immersive and dy- namic environments. As shown in Table 1, they provide high realism and diversity through sequential generation, ben- efiting from temporal coherence across frames. However, they face challenges in ensuring consistent view alignment. These methods can be divided into two-stage and one-stage categories, with their paradigms illustrated in Figure 6. 3.4.1 Two-stage Generation As shown in Figure 6a, two-stage generation divides the generation into two stages, each targeting multi-view spatial consistency and multi-frame temporal coherence separately. To further improve view consistency, these generated se- quences are subsequently used to optimize a dynamic 3D scene representation (e.g., 4D Gaussians [40], Deformable Gaussians [41]). VividDream [314] first constructs a static 3D scene through iterative image outpainting, then ren- ders multi-view videos covering the entire scene and ap- plies time-reversal [315] to animate them, creating dynamic videos across viewpoints. PaintScene4D [316] first generates a video from a text description using video diffusion, then refines it through iterative warping and inpainting at each timestamp to maintain multi-view consistency. Similarly, 4Real [117], DimensionX [42], and Free4D [317] first gener- ate a coherent reference video and then extend view angles using frame-conditioned video generation. 3.4.2 One-stage Generation As shown in Figure 6b, one-stage generation consolidates generation into a single process, implicitly capturing spatio- temporal consistency to produce single- or multi-view videos from any viewpoint and timestep within a unified model. Some approaches [318], [319], [320], [321], [322], [323], [324], [325] adopt video diffusion models for iterative view extrapolation, followed by 3DGS optimization to build a static scene. To generate dynamic scenes, GenX D [119] and CAT4D [326] adopt distinct multiview-temporal strate- gies to construct multi-view video models capable of gen- erating all views across all timestamps. StarGen [327] and Streetscapes [328] use past frames as guidance for video generation, enhancing long-range scene synthesis through an autoregressive approach. By utilizing the natural multi- view 3D prior of panoramic images, 4K4DGen [43] samples perspective images from a static panorama, animates them, and aligns them into a dynamic panorama. 360DVD [329], Imagine360 [330], Genex [331], and DynamicScaler [332] integrate panoramic constraints into video diffusion models to generate spherical-consistent panoramic videos. In scene generation for video games and autonomous driving, these methods enhance both control and realism by integrating various control signals as conditions. In open-world gaming environments, vast datasets compris- ing user inputs and rendered videos enable models like DIAMOND [333], GameNGen [334], Oasis [335], GameGen- X [120], and WORLDMEM [336] to predict future frames based on user interactions, creating responsive virtual envi- ronments as neural game engines. In autonomous driving, models such as DriveDreamer [337], MagicDrive [39], Drive- WM [338], and GAIA-1 [339] utilize inputs like text, bound- ing boxes, Bird’s Eye View (BEV) maps, and driver actions to control video generation for complex driving scenarios. Recent works further enhance view consistency [340], [341], [342], [343], [344], [345], [346], [347], [348], [349], [350], ex- pand control capabilities [118], [351], [352], [353], [354], en- able 3D-level control via occupancy [355], [356], [357], [358], [359], support multimodal output [360], [361], [362], and improve generation speed [363] and sequence length [364], [365], [366], [367]. 11 4 DATASETS AND EVALUATION 4.1 Datasets We summarize the commonly used datasets for 3D scene generation in Table 3, grouping them by scene type into three categories: indoor, natural, and urban. 4.1.1 Indoor Datasets Existing indoor datasets are either captured from real-world scenes using RGB or RGB-D sensors or professionally de- signed with curated 3D CAD furniture models. Real-world datasets are captured from physical scenes using sensors like depth, DSLR, or panoramic cameras. Early datasets provide RGB-D or panoramic images with semantic labels (e.g., NYUv2 [369], 2D-3D-S [372]), while recent ones like ScanNet [375] and Matterport3D [374] offer 3D reconstructions with dense meshes and instance-level annotations. • SUN360 [368] contains 67,583 high-res 360°× 180° panoramic images in equirectangular format, manually categorized into 80 scene types. • NYUv2 [369] provides 1,449 densely annotated RGB- D images from 464 indoor scenes, covering per-pixel semantics and instance-level objects. • SUN-RGBD [370] offers 10,335 RGB-D images and reconstructed point cloud, with rich annotations in- cluding room types, 2D polygons, 3D bounding boxes, camera poses, and room layouts. • SceneNN [371] offers 502K RGB-D frames from 100 indoor scenes with reconstructed meshes, textured models, camera poses, and both object-oriented and axis-aligned bounding boxes. • 2D-3D-S [372] includes over 70,000 panoramic im- ages from six indoor areas, with aligned depth, sur- face normals, semantic labels, point clouds, meshes, global XYZ maps, and full camera metadata. • Laval Indoor [373] offers 2.2K high-res indoor panoramas (7768×3884) with HDR lighting from var- ious settings such as homes, offices, and factories. • Matterport3D [374] contains 10,800 panoramic im- ages from 194,400 RGB-D views in 90 buildings, with dense camera trajectories, aligned depth maps, and semantic labels. • ScanNet [375] offers 2.5M RGB-D frames in 1,513 scans from 707 distinct spaces with camera poses, surface reconstructions, dense 3D semantic labels, and aligned CAD models. • Replica [377] provides high-quality 3D reconstruc- tions of 35 rooms across 18 scenes, featuring PBR textures, HDR lighting, and semantic annotations. • RealEstate10K [376] contains 10 million frames from 10K YouTube videos, featuring both indoor and out- door scenes with per-frame camera parameters. • 3DSSG [378] provides scene graphs for 478 indoor rooms from 3RScan [398], with 93 object attributes, 40 relationship types, and 534 semantic classes. • HM3D [379] offers 1,000 high-res 3D reconstructions of indoor spaces across residential, commercial, and civic buildings. • ScanNet++ [380] includes 1,000+ scenes captured with laser scanner, DSLR, and iPhone RGB-D, featur- ing fine-grained semantics and long-tail categories. • DL3DV-10K [381] contains 51.2M frames from 10,510 video sequences across 65 indoor and semi-outdoor locations, featuring varied visual conditions such as reflections and different lighting. Synthetic indoor datasets overcome real-world limita- tions like limited diversity, occlusion, and costly annotation. Using designed layouts and textured 3D assets, datasets like SUNCG [382] and 3D-FRONT [385] offer large-scale, diverse scenes. Some [383], [384] leverage advanced rendering for photorealistic images with accurate 2D labels. • SceneSynth [152] includes 130 indoor scenes (e.g., studies, kitchens, living rooms) with 1,723 unique models from Google 3D Warehouse. • SUNCG [382] offers 45,622 manually designed scenes, featuring 404K rooms and 5.7M object in- stances from 2,644 meshes across 84 categories. • Structured3D [383] includes 196.5K images from 3.5K professionally designed houses with detailed 3D annotations (e.g., lines, planes). • Hypersim [384] provides 77.4K photorealistic ren- ders with PBR materials and lighting for realistic view synthesis. • 3D-FRONT [385] offers 6,813 professionally de- signed houses and 18,797 diversely furnished rooms, populated with high-quality textured 3D objects from 3D-FUTURE [399]. • SG-FRONT [92] augments 3D-FRONT with scene graph annotations. 4.1.2 Natural Datasets Datasets for natural scenes are still limited, mainly due to the difficulties of large-scale collection and annotation in open outdoor environments. However, several notable efforts have been made to advance research in this area. • Laval Outdoor [386] provides 205 high-res HDR panoramas of diverse natural and urban scenes. • LHQ [387] offers 91,693 curated landscape images from Unsplash and Flickr, designed for high-quality image generation tasks. • ACID [33] features 2.1M drone-captured frames from 891 YouTube videos of coastal regions, with 3D cam- era trajectories obtained via structure-from-motion. 4.1.3 Urban Datasets Urban datasets are built from real-world imagery or syn- thesized using game engines, providing images and anno- tations in 2D or 3D. Real-world datasets mainly focus on driving scenes, rep- resented by KITTI [388], Waymo [391], and nuScenes [392], due to the significant attention autonomous driving has received over the past decade. Another major source is Google’s street views and aerial views, exemplified by HoliCity [393] and GoogleEarth [98]. These datasets pro- vide rich annotations, such as semantic segmentation and instance segmentation. 12 TABLE 3 Summary and comparison of popular datasets for 3D scene generation. The scene types I , N , and U represent “Indoor”, “Nature”, and “Urban”. The annotations G , P , and M denote scene graph, camera pose, and motion annotations (e.g., optical flow), respectively. S2 / I2 / B2 and S3 / I3 / B3 represent 2D/3D semantic maps, instance maps, and bounding boxes. P , N , I , and V indicate procedural, neural 3D-based, image-based, and video-based generation, respectively. MeshR and PCDR are reconstructed mesh and point clouds, respectively. Note that “-” in #Images, 3D Model, and Annotations indicates datasets do not provide these; “-” in #Scenes and Area means the information cannot be inferred. Dataset Year Type Source #Images #Scenes Area 3D Model Annotations Used by URL SUN360 [368] 2012 I N U Real 67.5K - - - - I NYUv2 [369] 2012 I Real 1.4K 464 - - S2 I2 N W Sun-RGBD [370] 2015 I Real 10.3K - - PCDR S2 B3 N W SceneNN [371] 2016 I Real 502K 100 2,260 m2 MeshR S3 I3 B3 P N W 2D-3D-S [372] 2017 I Real 70.5K 270 6,000 m2 MeshR, PCDR S2 I2 S3 I3 P I W Laval Indoor [373] 2017 I Real 2.2K - - - - I W Matterport3D [374] 2017 I Real 10.8K 90 0.102 km2 MeshR S2 I2 S3 I3 P N I W ScanNet [375] 2017 I Real 2.5M 707 39,980 m2 MeshR, CAD S2 I2 S3 I3 P N W RealEstate10K [376] 2018 I U Real 10M - - - P N I V W Replica [377] 2019 I Real - 18 2,190 m2 MeshR S3 I3 N I W 3DSSG [378] 2020 I Real 363K 478 - MeshR S3 I3 G P N W HM3D [379] 2021 I Real - 1,000 0.365 km2 MeshR - I W ScanNet++ [380] 2023 I Real 11.1M 1006 - MeshR S3 I3 P N W DL3DV-10K [381] 2024 I N U Real 51.2M - - - P N V W SceneSynth [152] 2012 I Synthetic - 130 - CAD - N W SUNCG [382] 2017 I Synthetic - 45,622 24 km2 CAD S3 I3 P N Structured3D [383] 2020 I Synthetic 196.5K 3500 - CAD S2 I2 S3 I3 N I W Hypersim [384] 2021 I Synthetic 77.4K 461 - CAD S2 I2 P N W 3D-FRONT [385] 2021 I Synthetic - 6,813 0.51 km2 CAD - P N W SG-FRONT [92] 2023 I Synthetic - 6,813 0.51 km2 CAD G N W Laval Outdoor [386] 2017 N U Real 0.2K - - - - I W LHQ [387] 2021 N Real 91.7K - - - - N I W ACID [33] 2021 N Real 2.1M - - - P N I V W KITTI [388] 2012 U Real 15K 1 - LiDAR S2 I2 B3 P M N I W Cityscapes [389] 2016 U Real 25K 50 - - S2 I2 I W SemanticKITTI [390] 2019 U Real - 1 - LiDAR S3 P N W Waymo [391] 2020 U Real 1M 3 76 km2 LiDAR B2 B3 P N V W nuScenes [392] 2020 U Real 1.4M 2 5 km2 LiDAR S3 B3 P N V W HoliCity [393] 2020 U Real 6.3K 1 20 km2 CAD S2 P N W OmniCity [394] 2023 U Real 108.6K 1 - - S2 I2 N W KITTI-360 [395] 2023 U Real 150K 1 - LiDAR S2 I2 S3 I3 B3 P N W GoogleEarth [98] 2024 U Real 24K 1 25 km2 Voxel Grid S2 I2 S3 I3 P N W OSM [98] 2024 U Real - 80 6,000 km2 - S2 I2 P N W CARLA [13] 2017 U Synthetic ∞ 13 - LiDAR S2 I2 S3 I3 P M N I V W Virtual-KITTI-2 [396] 2020 U Synthetic 42.5K 1 - - S2 I2 B2 B3 P M I W CarlaSC [397] 2022 U Synthetic 43.2K 8 - Voxel Grid S3 P M N W CityTopia [218] 2025 U Synthetic 37.5K 11 36 km2 Voxel Grid S2 I2 S3 I3 P N W • KITTI [388], captured in Karlsruhe, includes stereo and optical flow pairs, 39.2 km of visual odometry, and 200K+ 3D object annotations, using a Velodyne LiDAR, GPS/IMU, and a stereo camera rig with grayscale and color cameras. • SemanticKITTI [390] extends KITTI with dense point-wise semantics for full 360°LiDAR scans. • KITTI-360 [395] extends KITTI with 73.7 km of driv- ing, 150K+ images, 1B 3D points, and dense 2D/3D labels, using a setup of two 180°fisheye side cameras, a front stereo camera, and two LiDARs. • Cityscapes [389] provides street-view videos from 50 cities, with 5K pixel-level and 20K coarse annotations for strong and weak supervision. • Waymo [391] offers 1M frames from 1,150 20s scenes (6.4 hour total) with 12M 3D and 9.9M 2D boxes, col- lected in San Francisco, Mountain View, and Phoenix using 5 LiDARs and 5 high-res pinhole cameras. • nuScenes [392] provides 1.4M images and 390K Li- DAR sweeps from 1,000 20s scenes in Boston and Singapore, using 6 cameras, 1 LiDAR, 5 RADARs, GPS, and IMU, with 3D box tracking for 23 classes. • HoliCity [393] aligns 6,300 high-res panoramas (13312×6656) with CAD models of downtown Lon- 13 don for image-CAD fusion. • OmniCity [394] provides 100K+ pixel-annotated street, satellite, and panorama images from 25K lo- cations in New York City. • GoogleEarth [98] offers 24K New York images from 400 Google Earth2 trajectories with 2D/3D semantic and instance masks plus camera parameters. • OSM dataset [98], sourced from Open Street Map3, provides bird’s eye view semantic maps, height fields, and vector data of roads, buildings, and land use across 80+ cities worldwide. Real-world annotations are costly and viewpoint- limited. Synthetic datasets like CARLA [13] and City- Topia [218], built in game engines, provide diverse street and drone views with rich 2D/3D annotations. • CARLA [13] is an open-source simulator based on Unreal Engine, offering diverse urban environ- ments, sensor simulations (camera, LiDAR, radar), and customizable driving scenarios with control over weather, lighting, traffic, and pedestrian behaviors, enabling unlimited rendering of RGB images and corresponding 2D/3D annotations. • CarlaSC [397] offers 43.2K frames of semantic scenes from 24 sequences across 8 maps, captured by vir- tual LiDAR sensors in the CARLA simulator under varying traffic conditions. • Virtual-KITTI-2 [396] replicates 5 KITTI sequences using Unity, offering photorealistic video under varying conditions with dense annotations for depth, segmentation, optical flow, and object tracking. • CityTopia [218] provides 37.5K photorealistic frames with fine-grained 2D/3D annotations from 11 proce- dural cities in Unreal Engine, featuring varied light- ing and aerial/street-view perspectives. 4.2 Evaluation Evaluating 3D scene generation methods is essential for comparing different methods across different domains. Var- ious metrics have been proposed to assess key aspects of generated scenes, including geometric accuracy, structural consistency, visual realism, diversity, and physical plausibil- ity. This section summarizes and discusses commonly used evaluation metrics in 3D scene generation, highlighting their relevance to different generation paradigms and focuses. 4.2.1 Metrics-based Evaluation Fidelity is evaluated by using metrics from image and video generation to assess the visual quality and realism of gener- ated scenes, particularly for renderable outputs like NeRFs, 3D Gaussians, or image sequences. Fr´echet Inception Dis- tance (FID) [400], Kernel Inception Distance (KID) [401], and Inception Score (IS) [402] are widely used to evaluate the distributional similarity between rendered images and real samples. FID and KID compute statistical distances between feature distributions extracted from a pre-trained Inception network, while IS measures both image quality 2. https://earth.google.com/studio 3. https://openstreetmap.org and diversity based on classification confidence. SwAV- FID [403], FDD [404], and FIDCLIP [405] explore alternative feature spaces for better correlation with human evalua- tions. No-reference image quality metrics such as Natural Image Quality Evaluator (NIQE) [406], Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) [407] are used to estimate perceptual quality directly from the image statis- tics. CLIP-IQA [408] combines CLIP features with learned IQA models to better align with human perception under textual or semantic conditioning. For assessing photorealism specifically in 3D space, F3D [234] is a 3D adaptation of FID, which is based on a pre-trained autoencoder with a 3D CNN architecture. In addition to perceptual scores, some metrics evaluate distributional alignment between generated and real samples. Minimum Matching Distance (MMD) [409] quantifies the average pairwise distance between closest points across distributions, Coverage (COV) [409] measures how well generated samples cover the target distribution, and 1-Nearest Neighbor Accuracy (1-NNA) [410] estimates mode collapse or overfitting by classifying samples using nearest-neighbor retrieval. Spatial Consistency metrics assess the 3D geometry and multi-view alignment of the generated scenes. For depth error, pseudo ground-truth depth maps can be obtained using state-of-the-art monocular depth estimation models, while the depth map of the scene can be obtained us- ing reliable Structure-from-Motion (SfM) pipelines such as COLMAP [66]. For camera pose error, COLMAP is also used to estimate camera trajectories from the rendered sequences. The distances between these predicted values and ground- truth are computed using distance functions, such as L2 dis- tance, RMSE, and Scale-Invariant Root Mean Square Error (SI-RMSE) [411]. Temporal Coherence is a critical metric for evaluating the generated 3D scenes across time, particularly in dy- namic scenes or video-based outputs. Flow warping error (FE) [412] measures the temporal stability of a video by computing the warping error of optical flow between two frames. Fr´echet Video Distance (FVD) [413] builds on the principles underlying FID and introduces a different feature representation that captures the temporal coherence of a video, in addition to the quality of each frame. Focusing on the complex motion patterns in generated videos, Fr´echet Video Motion Distance (FVMD) [414] designs explicit mo- tion features based on keypoint tracking, measuring the similarity between these features via the Fr´echet distance for evaluating the motion coherence of the generated videos. Controllability evaluates the ability to respond to user inputs. CLIP Score [415] leverages a pre-trained CLIP model to measure the alignment between generated images and conditioning text, reflecting how faithfully the generation follows user-specified prompts. Diversity means the ability to produce varied outputs. Category distribution KL divergence (CKL) [177] compares the object category distribution in the synthesized scenes to that of the training set, with lower divergence indicating better diversity. Scene Classification Accuracy (SCA) [177] uses a trained classifier to distinguish between real and generated scenes, measuring how well the distribution of synthetic scenes matches that of real scenes. Plausibility measures how well generated scenes obey 14 physical and semantic constraints. Collision rate measures the proportion of collision objects among all generated objects within a scene. Out-of-bounds object area (OBA) assesses the cumulated out-of-bounds object area in a scene. 4.2.2 Benchmark-based Evaluation To promote fair, reproducible, and comprehensive evalu- ation of diverse 3D scene generation methods, recent re- search has increasingly embraced standardized benchmark suites that integrate multiple metrics, task configurations, and quality dimensions. This trend marks a shift from relying only on isolated quantitative metrics to adopting more holistic, task-aligned evaluations that better reflect the complexity of real-world applications. Q-Align [416] adopts large multi-modal models (LMMs) to predict visual quality scores that align with human judgment. It covers three core dimensions: Image Quality Assessment (IQA), Image Aesthetic Assessment (IAA), and Video Quality Assessment (VQA). During inference, the mean opinion scores are collected and re-weighted to obtain the LMM-predicted score. VideoScore [417] enables video quality evaluation by train- ing on a large-scale human-feedback dataset. It provides assessment across five aspects: Visual Quality (VQ), Tempo- ral Consistency (TC), Dynamic Degree (DD), Text-to-Video Alignment (TVA), and Factual Consistency (FC). VBench [418] and VBench++ [419] are comprehensive and versatile benchmark suites for video generation. They com- prise 16 dimensions in video generation (e.g., subject iden- tity inconsistency, motion smoothness, temporal flickering, and spatial relationship, etc). VBench-2.0 [420] further ad- dresses more complex challenges associated with intrinsic faithfulness, including commonsense reasoning, physics- based realism, human motion, and creative composition. WorldScore [421] unifies the evaluation of 3D, 4D, and video models on their ability to generate a world following instructions. It formulates the evaluation of 3D scene gener- ation to a sequence of next-scene generation tasks guided by camera trajectories, jointly measures controllability, quality, and dynamics in various fine-grained features. 4.2.3 Human Evaluation User studies remain an essential component for capturing subjective qualities of 3D scene generation that are difficult to quantify through automated metrics, such as visual ap- peal, realism, and perceptual coherence. Participants are typically asked to rank or rate generated scenes based on multiple aspects, including photorealism, aesthetics, input alignment (e.g., text or layout), 3D consis- tency across views, and physical or semantic plausibility. Ideally, participants should include both domain experts (e.g., 3D artists, designers, researchers) and normal users. Their feedback offers complementary perspectives: experts may provide more critical and structured insights, while non-experts better reflect general user impressions. Although human evaluations are resource-intensive and inherently subjective, they provide essential qualitative in- sights that complement other evaluation methods by cap- turing human preferences in real-world contexts. Platforms like Prolific4 and Amazon Mechanical Turk (AMT)5 facilitate the recruitment of diverse participants and enable efficient scaling of user studies. 5 APPLICATIONS AND TASKS The rapid progress in 3D scene generation has enabled diverse applications across various related domains. This section highlights key areas of 3D scene generation applica- tions, including 3D scene editing, human-scene interaction, embodied AI, robotics, and autonomous driving. 5.1 3D Scene Editing 3D scene editing involves altering a scene’s appearance and structure, from individual object modifications to complete environment customization. It broadly includes texture edit- ing, which focuses on generating stylized or realistic surface appearances, and layout editing, which involves arranging objects in a physically and semantically plausible manner. Texturing and stylization aim to create aesthetic and stylish appearances based on user specifications. While recent advances achieve impressive results on scanned meshes [422], [423], [424] or synthetic indoor datasets [425], [426], [427], they are constrained by incomplete geometry from reconstructions or extensive manual modeling. To ad- dress these limitations, recent methods leverage 3D scene generation to synthesize complete and semantically con- sistent scenes, directly supporting texture generation tasks. Methods such as Ctrl-Room [190], ControlRoom3D [261], RoomTex [428], and DreamSpace [429] employ holistic generation techniques to create panoramic room textures, followed by detailed refinement. Beyond direct genera- tion, 3D scene generation also benefits the evaluation of texturing methods. InstanceTex [430] generates texture across both existing datasets and new scenes generated by EchoScene [208], improving the diversity and robustness of benchmark evaluations. 3D scene layout editing focuses on arranging objects within a scene to produce semantically meaningful and physically plausible configurations. Several methods, such as LEGO-Net [431], CabiNet [432], and DeBaRA [185], address the rearrangement of existing scenes. These ap- proaches use object-level attributes, such as class labels, positions, and orientations, to produce more organized and regular arrangements. Some methods support more inter- active and dynamic layout editing. For example, SceneEx- pander [433] and SceneDirector [434] enable real-time edit- ing through intuitive user interactions, such as modifying room shapes or moving objects, and automatically update surrounding objects to maintain spatial coherence. Recent advances in compositional generative NeRF further push the boundary of layout control to enable editing of implicit representation. DisCoScene [220], Neural Assets [435], and Lift3D [436] enable object-level editing by adjusting control signals such as spatial locations or latent features, allowing for flexible and controllable scene manipulation. 4. https://www.prolific.com 5. https://www.mturk.com 15 5.2 Human-Scene Interaction Human-Scene Interaction (HSI) focuses on modeling how humans interact with and influence their environment. Re- alistic character animation and behavior modeling require synthesizing believable interactions between virtual charac- ters and their environments. Recent advances in HSI have made notable progress in generating realistic and physically plausible human motions within 3D environments [437], [438], [439], as well as creating scenes that align with specific motion sequences [87], [197], [198]. To generate human motion conditioned on scene envi- ronments, some approaches [437], [440], [441], [442] directly learn from datasets containing scanned indoor scenes and captured human motion [443], [444], [445]. However, these datasets are often limited in scalability and restricted to static scenes, prohibiting the modeling of dynamic human- object interactions. Some other works [438], [439], [446], [447], [448] employ simulated environments with reinforce- ment learning to generate physically plausible motion. Yet, due to high setup costs, these simulations often rely on simplified scenes, introducing a sim-to-real gap between synthetic training and real-world applications where envi- ronments are more complex and diverse. Recent efforts like GenZI [449] have initially addressed this issue by lifting the generated human in 2D images into 3D, enabling zero-shot generalization to novel scenes. Although GenZI still depends on pre-designed synthetic scenes for evaluation, it highlights the potential of com- bining scene generation with motion generation to scale HSI data more effectively. Integrating high-quality 3D scene generation is essential for advancing scalable and realistic HSI research, particularly by jointly considering human affordances, motion feasibility, and scene semantics. 5.3 Embodied AI In embodied AI, agents interact with environments to de- velop high-level semantic understanding and goal-directed behaviors. 3D scene generation supports this by providing visually and functionally rich environments that enable tasks like navigation, exploration, and instruction follow- ing, with an emphasis on cognitive reasoning over precise physical control. Simulated environments are typically built from recon- structed real-world data [379], [450] or manually designed scenes [451], [452], but both approaches have limitations: real-world datasets suffer from quality and annotation is- sues, while manual creation is labor-intensive and diffi- cult to scale. In this context, 3D scene generation offers a scalable, diverse, and physically plausible alternative for creating simulated environments for embodied AI research. For indoor environments, ProcTHOR [15] uses procedural generation to produce scenes that follow realistic layouts and physical constraints. Holodeck [175] leverages LLM to generate 3D environments that match user-supplied prompts automatically. InfiniteWorld [453] further expands assets with different textures for more diverse and stylish scenes. PhyScene [186] integrates physics and interactivity constraints into a conditional diffusion model to synthesize plausibly interactive environments. Architect [454] employs iterative image-based inpainting to populate scenes with large furniture and small objects, enriching scene com- plexity. Beyond indoor settings, procedural methods have also enabled city-scale simulation. MetaUrban [17], GRU- topia [16], and URBAN-SIM [455] construct diverse, large- scale urban environments for embodied agents. Embod- iedCity [456] provides a high-quality 3D real environment based on a real city, supporting various agents, continuous decision-making, and systematic benchmark tasks for em- bodied intelligence. 5.4 Robotics In robotics, 3D scene generation enables learning of low- level skills like manipulation and control within physically realistic environments. These scenes are typically embedded in simulators, where accurate modeling of dynamics and contact is crucial for training robots to perceive, plan, and act effectively in the real world. Simulated environments have become a central tool for developing robotic capabilities across various tasks, includ- ing complex manipulation and locomotion. However, re- cent approaches in robot learning [457], [458], [459], [460], [461], [462] require tremendous human effort to construct these environments and the corresponding demonstrations, restricting the scalability of robot learning even in simu- lated worlds. RoboGen [463] and RoboVerse [464] automate task, scene, and supervision generation through a propose- generate-learn cycle, where agents propose skills, generate environments with plausible object layouts, and learn with minimal human input. Eurekaverse [465] further scales skill learning by using LLMs to progressively generate diverse and increasingly challenging terrains, forming an adaptive curriculum for parkour training. Beyond explicitly constructing simulated environments, 3D scene generation also serves as world models for predict- ing future frames that visually represent intended actions, enabling robots to simulate and predict complex manipu- lation tasks in virtual settings. Robotics-focused video gen- eration models [466], [467], [468], [469], [470], [471], [472], [473], [474], [475], [476] aim to synthesize videos condi- tioned on inputs like text or images, specifically to help robots visualize and plan complex manipulation tasks by predicting future action sequences in a physically plausible way. Instead of directly generating video frames, some methods [477], [478], [479] leverage NeRFs and dynamic 3D Gaussians to capture the spatial and semantic complexity of real-world environments, enabling more accurate motion estimation and planning. 5.5 Autonomous Driving 3D scene generation is increasingly important in au- tonomous driving, offering controllable, scalable, and di- verse simulations of real-world environments. These capa- bilities help overcome limitations of real-world datasets and environments. It supports key components of self-driving systems, such as predictive modeling and data generation. Several 3D scene generation methods serve as world models for autonomous driving, enabling future scene pre- diction, risk anticipation, and the planning of safer, more efficient actions. Some [39], [118], [337], [338], [339], [355], [364], [366] focus on predicting future video frames, while 16 others [480], [481], [482], [483], [484] generate 3D occupan- cies to model the environment explicitly. With high-fidelity generation, DriveArena [351] and DrivingSphere [359] in- troduce closed-loop simulators for training and evaluating autonomous driving agents, enabling agents to learn and evolve in a closed-loop manner continuously. Autonomous driving demands large, diverse datasets, but real-world collections like nuScenes [392], KITTI [388], and Waymo [391] are costly and rarely capture critical corner cases. Controllable video-based generation meth- ods [341], [343], [344], [345], [353] address this by synthe- sizing diverse driving scenarios with flexible control over weather, lighting, and traffic conditions, especially for rare and safety-critical events. 6 CHALLENGES AND FUTURE DIRECTIONS 6.1 Challenges Despite recent advancements, 3D scene generation still has significant potential for improvement. Generative Capacity. Existing generative models exhibit a trade-off in jointly satisfying photorealism, 3D consistency, and controllability. Procedural and neural 3D-based ap- proaches excel at generating geometrically coherent scenes with controllable spatial layouts, but often fall short in producing photorealistic textures and lighting. In contrast, image- and video-based generation models achieve high visual realism, yet struggle to maintain 3D consistency, resulting in artifacts such as distorted geometry, unrealistic object interactions, or implausible physical dynamics. As a result, current models still find it challenging to synthesize complex, multi-object scenes that are both visually plausible and physically grounded. 3D Representation. The evolution of 3D scene representa- tions has progressed from geometry-centric formats such as voxel grids and point clouds, both of which struggle to capture photorealistic appearance, to NeRFs, which im- prove visual quality but remain inefficient and lack explicit geometry. Recent advances like 3D Gaussians offer better efficiency but still lack geometric grounding, limiting their applicability to tasks like relighting or physical interaction. Mesh- and B´ezier-triangle-based methods [485], [486], [487] partially address these limitations by introducing explicit surface representations, yet they are largely confined to object-level generation. Scene-level representations that are compact, physically meaningful, and visually realistic re- main an open challenge, hindering progress in controllable and generalizable 3D scene generation. Data and Annotations. The progress of 3D scene generation is tightly bound to dataset quality. Synthetic datasets offer precise annotations but suffer from limited content diversity and suboptimal photorealism due to rendering constraints in current game engines. In contrast, real-world scans pro- vide visually realistic imagery but often lack sufficient anno- tations. While image- and video-based generative methods alleviate annotation needs, they still struggle to capture accurate 3D geometry, often resulting in spatial distortions. Additionally, existing datasets rarely include rich meta- data, such as physical affordances, material attributes, or interaction cues, hindering broader applications in robotics, embodied AI, and physical simulation. Evaluation. A persistent challenge in 3D scene generation is the lack of unified evaluation protocols. Methods often rely on disparate metrics, hindering consistent compari- son. Benchmark-based efforts [420], [421] have partially addressed this by introducing standardized and human- aligned evaluation frameworks. However, current bench- marks are largely conditioned on text or images, with lim- ited support for other inputs such as layouts, actions, or trajectories. Moreover, evaluations still primarily focus on image and video fidelity, offering an insufficient assessment of underlying 3D geometry and physical plausibility. Recent work like Eval3D [488] introduces a benchmark that begins to address 3D structural, semantic, and geometric consis- tency, though it remains limited to object-level generation and lacks scene-level complexity. 6.2 Future Directions Given the substantial progress made and the key challenges outlined above, we believe that future research in 3D scene generation can advance in the following directions. Better Fidelity. High-fidelity 3D scene generation demands coherence in geometry, texture, lighting, and multi-view consistency. While current methods often trade off between geometric accuracy or visual richness, future models should focus on bridging this gap that jointly reason about structure and appearance. Key goals include improved material and lighting modeling, consistent object identity across views, and capturing subtle cues like shadows and occlusions. Achieving scene-level fidelity also means aligning local details with global spatial and semantic coherence, enabling more realistic and useful 3D environments. Physical-aware Generation. Despite impressive visual progress, current methods often overlook the physical plau- sibility of generated scenes. To ensure that object placements and articulations conform to physical laws, future work should incorporate physics priors, constraints, or simula- tions into the generation process. Emerging approaches that integrate physics-based feedback, such as differentiable simulators [489], offer a promising path toward jointly op- timizing structure, semantics, and physical behavior. These capabilities are especially important for embodied AI and robotics, where agents depend on physically consistent en- vironments for effective planning and control. Interactive Scene Generation. Recent advances in 4D scene generation have enabled the creation of dynamic environ- ments with movable objects. However, these scenes remain largely non-interactive, where objects do not respond to user inputs or environmental changes. As a result, current gen- erative models produce passive rather than reactive experi- ences. A key future direction is interactive scene generation, where scenes contain interactive objects that can respond meaningfully to physical interactions, user commands, or contextual variations. Achieving this will require models to go beyond geometry and motion, incorporating reasoning about object affordances, causal relationships, and multi- agent dynamics. Unified Perception-Generation. A promising frontier lies in unifying perception and generation under a shared model. Tasks such as segmentation, reconstruction, and scene syn- thesis benefit from common spatial and semantic priors. 17 Moreover, generation tasks inherently require an under- standing of the input modalities. A unified architecture could leverage bidirectional capabilities: enhancing gener- ative performance via perceptual grounding and improving scene understanding through generative modeling. Such models could serve as general-purpose backbones for em- bodied agents, supporting joint reasoning across vision, language, and 3D spatial representations. REFERENCES [1] B. Mendiburu, 3D movie making: stereoscopic digital cinema from script to screen. Routledge, 2012. [2] N. Anantrasirichai and D. Bull, “Artificial intelligence in the creative industries: a review,” Artificial Intelligence Review, vol. 55, no. 1, pp. 589–656, 2022. [3] Y. I. H. Parish and P. M¨uller, “Procedural modeling of cities,” in SIGGRAPH, 2001. [4] G. N. Yannakakis and J. Togelius, Artificial intelligence and games. Springer, 2018, vol. 2. [5] T. Short and T. Adams, Procedural generation in game design. CRC Press, 2017. [6] P. M¨uller, P. Wonka, S. Haegler, A. Ulmer, and L. V. Gool, “Procedural modeling of buildings,” ACM TOG, vol. 25, no. 3, pp. 614–623, 2006. [7] K. Chang, C. Cheng, J. Luo, S. Murata, M. Nourbakhsh, and Y. Tsuji, “Building-GAN: Graph-conditioned architectural volu- metric design generation,” in ICCV, 2021. [8] S. M. LaValle, Virtual reality. Cambridge university press, 2023. [9] L. Lee, T. Braud, P. Y. Zhou, L. Wang, D. Xu, Z. Lin, A. Kumar, C. Bermejo, and P. Hui, “All one needs to know about metaverse: A complete survey on technological singularity, virtual ecosys- tem, and research agenda,” Foundations and Trends in Human- Computer Interaction, vol. 18, no. 2-3, pp. 100–337, 2024. [10] M. M. Soliman, E. Ahmed, A. Darwish, and A. E. Hassanien, “Artificial intelligence powered metaverse: analysis, challenges and future perspectives,” Artificial Intelligence Review, vol. 57, no. 2, p. 36, 2024. [11] C. Li, R. Zhang, J. Wong, C. Gokmen, S. Srivastava, R. Mart´ın- Mart´ın, C. Wang, G. Levine, W. Ai, B. J. Martinez, H. Yin, M. Lingelbach, M. Hwang, A. Hiranaka, S. Garlanka, A. Aydin, S. Lee, J. Sun, M. Anvari, M. Sharma, D. Bansal, S. Hunter, K. Kim, A. Lou, C. R. Matthews, I. Villa-Renteria, J. H. Tang, C. Tang, F. Xia, Y. Li, S. Savarese, H. Gweon, C. K. Liu, J. Wu, and L. Fei- Fei, “BEHAVIOR-1K: A human-centered, embodied AI bench- mark with 1, 000 everyday activities and realistic simulation,” arXiv 2403.09227, 2024. [12] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fu- sai, L. Groom, K. Hausman, B. Ichter, S. Jakubczak, T. Jones, L. Ke, S. Levine, A. Li-Bell, M. Mothukuri, S. Nair, K. Pertsch, L. X. Shi, J. Tanner, Q. Vuong, A. Walling, H. Wang, and U. Zhilinsky, “π0: A vision-language-action flow model for general robot control,” arXiv 2410.24164, 2024. [13] A. Dosovitskiy, G. Ros, F. Codevilla, A. M. L´opez, and V. Koltun, “CARLA: an open urban driving simulator,” in CoRL, vol. 78, 2017, pp. 1–16. [14] L. Chen, P. Wu, K. Chitta, B. Jaeger, A. Geiger, and H. Li, “End- to-end autonomous driving: Challenges and frontiers,” IEEE TPAMI, vol. 46, no. 12, pp. 10 164–10 183, 2024. [15] M. Deitke, E. VanderBilt, A. Herrasti, L. Weihs, K. Ehsani, J. Sal- vador, W. Han, E. Kolve, A. Kembhavi, and R. Mottaghi, “Proc- THOR: Large-scale embodied AI using procedural generation,” in NeurIPS, 2022. [16] H. Wang, J. Chen, W. Huang, Q. Ben, T. Wang, B. Mi, T. Huang, S. Zhao, Y. Chen, S. Yang, P. Cao, W. Yu, Z. Ye, J. Li, J. Long, Z. Wang, H. Wang, Y. Zhao, Z. Tu, Y. Qiao, D. Lin, and J. Pang, “GRUtopia: Dream general robots in a city at scale,” arXiv 2407.10943, 2024. [17] W. Wu, H. He, Y. Wang, C. Duan, J. He, Z. Liu, Q. Li, and B. Zhou, “MetaUrban: A simulation platform for embodied AI in urban spaces,” in ICLR, 2025. [18] Z. Zhu, X. Wang, W. Zhao, C. Min, N. Deng, M. Dou, Y. Wang, B. Shi, K. Wang, C. Zhang, Y. You, Z. Zhang, D. Zhao, L. Xiao, J. Zhao, J. Lu, and G. Huang, “Is sora a world simulator? A comprehensive survey on general world models and beyond,” arXiv 2405.03520, 2024. [19] N. Agarwal, A. Ali, M. Bala, Y. Balaji, E. Barker, T. Cai, P. Chat- topadhyay, Y. Chen, Y. Cui, Y. Ding et al., “Cosmos world foun- dation model platform for physical AI,” arXiv 2501.03575, 2025. [20] D. Liu, J. Zhang, A.-D. Dinh, E. Park, S. Zhang, and C. Xu, “Generative physical AI in vision: A survey,” arXiv 2501.10928, 2025. [21] H. Jiang, D. Yan, X. Zhang, and P. Wonka, “Selection expressions for procedural modeling,” IEEE TVCG, vol. 26, no. 4, pp. 1775– 1788, 2020. [22] W. Zhao, Y. Cao, J. Xu, Y. Dong, and Y. Shan, “DI-PCG: diffusion- based efficient inverse procedural content generation for high- quality 3D asset creation,” arXiv 2412.15200, 2024. [23] M. Hendrikx, S. A. Meijer, J. V. D. Velden, and A. Iosup, “Proce- dural content generation for games: A survey,” ACM Transactions on Multimedia Computing, Communications, and Applications, vol. 9, no. 1, pp. 1:1–1:22, 2013. [24] S. Zhang, M. Zhou, Y. Wang, C. Luo, R. Wang, Y. Li, X. Yin, Z. Zhang, and J. Peng, “CityX: Controllable procedural content generation for unbounded 3D cities,” arXiv 2407.17572, 2024. [25] Y. Yang, J. Wang, E. Vouga, and P. Wonka, “Urban pattern: layout design by hierarchical domain splitting,” ACM TOG, vol. 32, no. 6, pp. 181:1–181:12, 2013. [26] J. O. Talton, Y. Lou, S. Lesser, J. Duke, R. Mech, and V. Koltun, “Metropolis procedural modeling,” ACM TOG, vol. 30, no. 2, pp. 11:1–11:14, 2011. [27] W. Wu, L. Fan, L. Liu, and P. Wonka, “Miqp-based layout design for building interiors,” Computer Graphics Forum, 2018. [28] L. Yu, S. K. Yeung, C. Tang, D. Terzopoulos, T. F. Chan, and S. J. Osher, “Make it home: automatic optimization of furniture arrangement,” ACM TOG, vol. 30, no. 4, p. 86, 2011. [29] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde- Farley, S. Ozair, A. C. Courville, and Y. Bengio, “Generative adversarial networks,” in NIPS, 2014. [30] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” in NeurIPS, 2020. [31] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ra- mamoorthi, and R. Ng, “NeRF: Representing scenes as neural radiance fields for view synthesis,” in ECCV, 2020. [32] B. Kerbl, G. Kopanas, T. Leimk¨uhler, and G. Drettakis, “3D Gaussian splatting for real-time radiance field rendering,” ACM TOG, vol. 42, no. 4, pp. 139:1–139:14, 2023. [33] A. Liu, A. Makadia, R. Tucker, N. Snavely, V. Jampani, and A. Kanazawa, “Infinite Nature: Perpetual view generation of natural scenes from a single image,” in ICCV, 2021. [34] Z. Li, Q. Wang, N. Snavely, and A. Kanazawa, “InfiniteNature- Zero: Learning perpetual view generation of natural scenes from single images,” in ECCV, 2022. [35] Z. Chen, G. Wang, and Z. Liu, “Text2Light: Zero-shot text-driven HDR panorama generation,” ACM TOG, vol. 41, no. 6, pp. 195:1– 195:16, 2022. [36] S. Tang, F. Zhang, J. Chen, P. Wang, and Y. Furukawa, “MVD- iffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion,” in NeurIPS, 2023. [37] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, V. Jampani, and R. Rombach, “Stable video diffusion: Scaling latent video diffusion models to large datasets,” arXiv 2311.15127, 2023. [38] Y. Liu, K. Zhang, Y. Li, Z. Yan, C. Gao, R. Chen, Z. Yuan, Y. Huang, H. Sun, J. Gao, L. He, and L. Sun, “Sora: A review on background, technology, limitations, and opportunities of large vision models,” arXiv 2402.17177, 2024. [39] R. Gao, K. Chen, E. Xie, L. Hong, Z. Li, D. Yeung, and Q. Xu, “MagicDrive: Street view generation with diverse 3D geometry control,” in ICLR, 2024. [40] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, and X. Wang, “4D Gaussian splatting for real-time dynamic scene rendering,” in CVPR, 2024. [41] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, “De- formable 3D Gaussians for high-fidelity monocular dynamic scene reconstruction,” in CVPR, 2024. 18 [42] W. Sun, S. Chen, F. Liu, Z. Chen, Y. Duan, J. Zhang, and Y. Wang, “DimensionX: Create any 3D and 4D scenes from a single image with controllable video diffusion,” arXiv 2411.04928, 2024. [43] R. Li, P. Pan, B. Yang, D. Xu, S. Zhou, X. Zhang, Z. Li, A. Kadambi, Z. Wang, and Z. Fan, “4K4DGen: Panoramic 4D generation at 4K resolution,” in ICLR, 2025. [44] R. M. Smelik, T. Tutenel, R. Bidarra, and B. Benes, “A survey on procedural modelling for virtual worlds,” Computer Graphics Forum, vol. 33, no. 6, pp. 31–50, 2014. [45] E. Cogo, E. Krupalija, I. Prazina, S. Becirovic, V. Okanovic, S. Rizvic, and R. T. Mulahasanovic, “A survey of procedural modelling methods for layout generation of virtual scenes,” Computer Graphics Forum, vol. 43, no. 1, 2024. [46] S. Zhang, S. Zhang, Y. Liang, and P. Hall, “A survey of 3D in- door scene synthesis,” Journal of Computer Science and Technology, vol. 34, no. 3, pp. 594–608, 2019. [47] A. G. Patil, S. G. Patil, M. Li, M. Fisher, M. Savva, and H. Zhang, “Advances in data-driven analysis and synthesis of 3D indoor scenes,” Computer Graphics Forum, vol. 43, no. 1, 2024. [48] D. V. Ayyildiz, A. J. Alnaser, S. Taj, M. Zakaria, and L. G. Jaimes, “A survey of learning techniques for virtual scene generation,” SAE International Journal of Connected and Automated Vehicles, 2024. [49] H. Wang, X. Xiang, W. Xia, and J. Xue, “A survey on text-driven 360-degree panorama generation,” arXiv 2502.14799, 2025. [50] M. A. Ghorab and A. Lakhfif, “Text to 3D, 2D scene generation systems, frameworks and approaches: a survey,” in Pattern Anal- ysis and Intelligent Systems, 2022. [51] Z. Shi, S. Peng, Y. Xu, Y. Liao, and Y. Shen, “Deep generative models on 3D representations: A survey,” arXiv 2210.15663, 2022. [52] C. Li, C. Zhang, A. Waghwase, L. Lee, F. Rameau, Y. Yang, S. Bae, and C. S. Hong, “Generative AI meets 3D: A survey on text-to-3D in AIGC era,” arXiv 2305.06131, 2023. [53] X. Li, Q. Zhang, D. Kang, W. Cheng, Y. Gao, J. Zhang, Z. Liang, J. Liao, Y. Cao, and Y. Shan, “Advances in 3D generation: A survey,” arXiv 2401.17807, 2024. [54] J. Liu, X. Huang, T. Huang, L. Chen, Y. Hou, S. Tang, Z. Liu, W. Ouyang, W. Zuo, J. Jiang, and X. Liu, “A comprehensive survey on 3D content generation,” arXiv 2402.01166, 2024. [55] Z. Wang, D. Li, and R. Jiang, “Diffusion models in 3D vision: A survey,” arXiv 2410.04738, 2024. [56] Q. Miao, K. Li, J. Quan, Z. Min, S. Ma, Y. Xu, Y. Yang, and Y. Luo, “Advances in 4D generation: A survey,” arXiv 2503.14501, 2025. [57] J. Ding, Y. Zhang, Y. Shang, Y. Zhang, Z. Zong, J. Feng, Y. Yuan, H. Su, N. Li, N. Sukiennik, F. Xu, and Y. Li, “Understanding world or predicting future? A comprehensive survey of world models,” arXiv 2411.14499, 2024. [58] T. Feng, W. Wang, and Y. Yang, “A survey of world models for autonomous driving,” arXiv 2501.11260, 2025. [59] X. Han, H. Laga, and M. Bennamoun, “Image-based 3D object reconstruction: State-of-the-art and trends in the deep learning era,” IEEE TPAMI, vol. 43, no. 5, pp. 1578–1604, 2021. [60] Z. Huang, Y. Wen, Z. Wang, J. Ren, and K. Jia, “Surface recon- struction from point clouds: A survey and a benchmark,” IEEE TPAMI, vol. 46, no. 12, pp. 9727–9748, 2024. [61] Z. Xing, Q. Feng, H. Chen, Q. Dai, H. Hu, H. Xu, Z. Wu, and Y. Jiang, “A survey on video diffusion models,” ACM Computing Surveys, vol. 57, no. 2, pp. 41:1–41:42, 2025. [62] U. Singer, S. Sheynin, A. Polyak, O. Ashual, I. Makarov, F. Kokki- nos, N. Goyal, A. Vedaldi, D. Parikh, J. Johnson, and Y. Taigman, “Text-to-4D dynamic scene generation,” in ICML, 2023. [63] D. Xu, H. Liang, N. P. Bhatt, H. Hu, H. Liang, K. N. Plataniotis, and Z. Wang, “Comp4D: Llm-guided compositional 4D scene generation,” arXiv 2403.16993, 2024. [64] Y. Zheng, X. Li, K. Nagano, S. Liu, O. Hilliges, and S. D. Mello, “A unified approach for text-and image-guided 4D scene gener- ation,” in CVPR, 2024. [65] B. Curless and M. Levoy, “A volumetric method for building complex models from range images,” in SIGGRAPH, 1996. [66] J. L. Sch¨onberger and J. Frahm, “Structure-from-motion revis- ited,” in CVPR, 2016. [67] J. J. Park, P. R. Florence, J. Straub, R. A. Newcombe, and S. Love- grove, “DeepSDF: Learning continuous signed distance functions for shape representation,” in CVPR, 2019. [68] J. C. Hart, “Sphere tracing: a geometric method for the antialiased ray tracing of implicit surfaces,” The Visual Computer, vol. 12, no. 10, pp. 527–545, 1996. [69] J. T. Kajiya and B. V. Herzen, “Ray tracing volume densities,” in SIGGRAPH, 1984. [70] N. L. Max, “Optical models for direct volume rendering,” IEEE TVCG, vol. 1, no. 2, pp. 99–108, 1995. [71] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” in ICLR, 2014. [72] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in NeurIPS, 2017. [73] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. W. Battaglia, “Learning deep generative models of graphs,” arXiv 1803.03324, 2018. [74] D. J. Rezende and F. Viola, “Taming VAEs,” arXiv 1810.00597, 2018. [75] J. Lucas, G. Tucker, R. B. Grosse, and M. Norouzi, “Don’t Blame the ELBO! A linear VAE perspective on posterior collapse,” in NeurIPS, 2019. [76] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, “Improved training of wasserstein gans,” in NIPS, 2017. [77] P. Dhariwal and A. Q. Nichol, “Diffusion models beat gans on image synthesis,” in NeurIPS, 2021. [78] F. K. Musgrave, C. E. Kolb, and R. S. Mace, “The synthesis and rendering of eroded fractal terrains,” in SIGGRAPH, 1989. [79] G. Cordonnier, E. Galin, J. Gain, B. Benes, E. Gu´erin, A. Peytavie, and M. Cani, “Authoring landscapes by combining ecosystem and terrain erosion simulation,” ACM TOG, vol. 36, no. 4, pp. 134:1–134:12, 2017. [80] A. Raistrick, L. Lipson, Z. Ma, L. Mei, M. Wang, Y. Zuo, K. Kayan, H. Wen, B. Han, Y. Wang, A. Newell, H. Law, A. Goyal, K. Yang, and J. Deng, “Infinite photorealistic worlds using procedural generation,” in CVPR, 2023. [81] A. Raistrick, L. Mei, K. Kayan, D. Yan, Y. Zuo, B. Han, H. Wen, M. Parakh, S. Alexandropoulos, L. Lipson, Z. Ma, and J. Deng, “Infinigen Indoors: Photorealistic indoor scenes using procedural generation,” in CVPR, 2024. [82] W. Feng, W. Zhu, T. Fu, V. Jampani, A. R. Akula, X. He, S. Basu, X. E. Wang, and W. Y. Wang, “LayoutGPT: Compositional vi- sual planning and generation with large language models,” in NeurIPS, 2023. [83] C. Sun, J. Han, W. Deng, X. Wang, Z. Qin, and S. Gould, “3D- GPT: Procedural 3D modeling with large language models,” in 3DV, 2025. [84] M. Zhou, J. Hou, C. Luo, Y. Wang, Z. Zhang, and J. Peng, “SceneX: procedural controllable large-scale scene generation via large- language models,” in AAAI, 2025. [85] K. Wang, M. Savva, A. X. Chang, and D. Ritchie, “Deep convo- lutional priors for indoor scene synthesis,” ACM TOG, vol. 37, no. 4, p. 70, 2018. [86] D. Paschalidou, A. Kar, M. Shugrina, K. Kreis, A. Geiger, and S. Fidler, “ATISS: autoregressive transformers for indoor scene synthesis,” in NeurIPS, 2021. [87] H. Yi, C. P. Huang, S. Tripathi, L. Hering, J. Thies, and M. J. Black, “MIME: human-aware 3D scene generation,” in CVPR, 2023. [88] J. Tang, Y. Nie, L. Markhasin, A. Dai, J. Thies, and M. Nießner, “DiffuScene: Denoising diffusion models for generative indoor scene synthesis,” in CVPR, 2024. [89] K. Wang, Y. Lin, B. Weissmann, M. Savva, A. X. Chang, and D. Ritchie, “PlanIT: planning and instantiating indoor scenes with relation graph and spatial prior networks,” ACM TOG, vol. 38, no. 4, pp. 132:1–132:15, 2019. [90] M. Li, A. G. Patil, K. Xu, S. Chaudhuri, O. Khan, A. Shamir, C. Tu, B. Chen, D. Cohen-Or, and H. R. Zhang, “GRAINS: generative recursive autoencoders for indoor scenes,” ACM TOG, vol. 38, no. 2, pp. 12:1–12:16, 2019. [91] H. Dhamo, F. Manhardt, N. Navab, and F. Tombari, “Graph-to- 3D: End-to-end generation and manipulation of 3D scenes using scene graphs,” in ICCV, 2021. [92] G. Zhai, E. P. ¨Ornek, S. Wu, Y. Di, F. Tombari, N. Navab, and B. Busam, “CommonScenes: Generating commonsense 3D indoor scenes with scene graph diffusion,” in NeurIPS, 2023. [93] C. Lin and Y. Mu, “InstructScene: Instruction-driven 3D indoor scene synthesis with semantic graph prior,” in ICLR, 2024. [94] Z. Hao, A. Mallya, S. J. Belongie, and M. Liu, “GANcraft: Un- supervised 3D neural rendering of minecraft worlds,” in ICCV, 2021. 19 [95] S. Bahmani, J. J. Park, D. Paschalidou, X. Yan, G. Wetzstein, L. J. Guibas, and A. Tagliasacchi, “CC3D: layout-conditioned generation of compositional 3D scenes,” in ICCV, 2023. [96] C. H. Lin, H. Lee, W. Menapace, M. Chai, A. Siarohin, M. Yang, and S. Tulyakov, “InfiniCity: Infinite-scale city synthesis,” in ICCV, 2023. [97] Z. Chen, G. Wang, and Z. Liu, “SceneDreamer: Unbounded 3D scene generation from 2D image collections,” IEEE TPAMI, vol. 45, no. 12, pp. 15 562–15 576, 2023. [98] H. Xie, Z. Chen, F. Hong, and Z. Liu, “CityDreamer: Composi- tional generative model of unbounded 3D cities,” in CVPR, 2024. [99] R. Po and G. Wetzstein, “Compositional 3D scene generation using locally conditioned diffusion,” in 3DV, 2024. [100] Z. Wu, Y. Li, H. Yan, T. Shang, W. Sun, S. Wang, R. Cui, W. Liu, H. Sato, H. Li, and P. Ji, “BlockFusion: Expandable 3D scene gen- eration using latent tri-plane extrapolation,” ACM TOG, vol. 43, no. 4, pp. 43:1–43:17, 2024. [101] T. DeVries, M. ´A. Bautista, N. Srivastava, G. W. Taylor, and J. M. Susskind, “Unconstrained scene generation with locally conditioned radiance fields,” in ICCV, 2021. [102] M. ´A. Bautista, P. Guo, S. Abnar, W. Talbott, A. Toshev, Z. Chen, L. Dinh, S. Zhai, H. Goh, D. Ulbricht, A. Dehghan, and J. M. Susskind, “GAUDI: A neural architect for immersive 3D scene generation,” in NeurIPS, 2022. [103] S. W. Kim, B. Brown, K. Yin, K. Kreis, K. Schwarz, D. Li, R. Rombach, A. Torralba, and S. Fidler, “NeuralField-LDM: Scene generation with hierarchical latent diffusion models,” in CVPR, 2023. [104] X. Ren, J. Huang, X. Zeng, K. Museth, S. Fidler, and F. Williams, “XCube: Large-scale 3D generative modeling using sparse voxel hierarchies,” in CVPR, 2024. [105] X. Li, Z. Lai, L. Xu, Y. Qu, L. Cao, S. Zhang, B. Dai, and R. Ji, “Director3D: Real-world camera trajectory and 3D scene generation from text,” in NeurIPS, 2024. [106] M. R. K. Dastjerdi, Y. Hold-Geoffroy, J. Eisenmann, S. Kho- dadadeh, and J. Lalonde, “Guided co-modulated GAN for 360° field of view extrapolation,” in 3DV, 2022. [107] C. Zhang, Q. Wu, C. C. Gambardella, X. Huang, D. Phung, W. Ouyang, and J. Cai, “Taming stable diffusion for text to 360° panorama image generation,” in CVPR, 2024. [108] G. Wang, P. Wang, Z. Chen, W. Wang, C. C. Loy, and Z. Liu, “PERF: panoramic neural radiance field from a single panorama,” IEEE TPAMI, vol. 46, no. 10, pp. 6905–6918, 2024. [109] S. Yang, J. Tan, M. Zhang, T. Wu, Y. Li, G. Wetzstein, Z. Liu, and D. Lin, “LayerPano3D: Layered 3D panorama for hyper- immersive scene generation,” in SIGGRAPH, 2025. [110] C. Rockwell, D. F. Fouhey, and J. Johnson, “PixelSynth: Gener- ating a 3D-consistent experience from a single image,” in ICCV, 2021. [111] R. Rombach, P. Esser, and B. Ommer, “Geometry-free view syn- thesis: Transformers and no 3D priors,” in ICCV, 2021. [112] X. Li, Z. Cao, H. Sun, J. Zhang, K. Xian, and G. Lin, “3D cinemagraphy from a single image,” in CVPR, 2023. [113] L. H¨ollein, A. Cao, A. Owens, J. Johnson, and M. Nießner, “Text2Room: Extracting textured 3D meshes from 2D text-to- image models,” in ICCV, 2023. [114] J. Zhang, X. Li, Z. Wan, C. Y. Wang, and J. Liao, “Text2NeRF: Text- driven 3D scene generation with neural radiance fields,” IEEE TVCG, 2024. [115] H. Yu, H. Duan, J. Hur, K. Sargent, M. Rubinstein, W. T. Freeman, F. Cole, D. Sun, N. Snavely, J. Wu, and C. Herrmann, “Wonder- Journey: Going from anywhere to everywhere,” in CVPR, 2024. [116] J. Chung, S. Lee, H. Nam, J. Lee, and K. M. Lee, “LucidDreamer: Domain-free generation of 3D Gaussian splatting scenes,” arXiv 2311.13384, 2023. [117] H. Yu, C. Wang, P. Zhuang, W. Menapace, A. Siarohin, J. Cao, L. A. Jeni, S. Tulyakov, and H. Lee, “4Real: Towards photorealistic 4D scene generation via video diffusion models,” in NeurIPS, 2024. [118] S. Gao, J. Yang, L. Chen, K. Chitta, Y. Qiu, A. Geiger, J. Zhang, and H. Li, “Vista: A generalizable driving world model with high fidelity and versatile controllability,” in NeurIPS, 2024. [119] Y. Zhao, C. Lin, K. Lin, Z. Yan, L. Li, Z. Yang, J. Wang, G. H. Lee, and L. Wang, “GenXD: generating any 3D and 4D scenes,” ICLR, 2025. [120] H. Che, X. He, Q. Liu, C. Jin, and H. Chen, “GameGen-X: Interactive open-world game video generation,” in ICLR, 2025. [121] B. Mandelbrot, “How long is the coast of britain? statistical self- similarity and fractional dimension,” Science, vol. 156, no. 3775, pp. 636–638, 1967. [122] M. B. B, “The fractal geometry of nature,” New York, 1983. [123] A. Fournier, D. S. Fussell, and L. C. Carpenter, “Computer rendering of stochastic models,” Commun. ACM, vol. 25, no. 6, pp. 371–384, 1982. [124] P. Przemyslaw and H. Mark, “A fractal model of mountains and rivers,” in Graphics Interface, 1993. [125] F. Belhadj and P. Audibert, “Modeling landscapes with ridges and rivers: bottom up approach,” in GRAPHITE, 2005. [126] M. B. B and V. N. J. W, “Fractional brownian motions, fractional noises and applications,” SIAM review, vol. 10, no. 4, pp. 422–437, 1968. [127] L. Aristid, “Mathematical models for cellular interactions in development i. filaments with one-sided inputs,” Journal of the- oretical biology, vol. 18, no. 3, pp. 280–299, 1968. [128] G. Stiny and J. Gips, “Shape grammars and the generative specification of painting and sculpture,” in Information Processing, 1971. [129] A. D. Kelley, M. C. Malin, and G. M. Nielson, “Terrain simulation using a model of stream erosion,” in SIGGRAPH, 1988. [130] G. Cordonnier, G. Jouvet, A. Peytavie, J. Braun, M. Cani, B. Benes, E. Galin, E. Gu´erin, and J. Gain, “Forming terrains by glacial erosion,” ACM TOG, vol. 42, no. 4, pp. 61:1–61:14, 2023. [131] J. G´enevaux, E. Galin, E. Gu´erin, A. Peytavie, and B. Benes, “Ter- rain generation using procedural models based on hydrology,” ACM TOG, vol. 32, no. 4, pp. 143:1–143:13, 2013. [132] H. Schott, A. Paris, L. Fournier, E. Gu´erin, and E. Galin, “Large- scale terrain authoring through interactive erosion simulation,” ACM TOG, vol. 42, no. 5, pp. 162:1–162:15, 2023. [133] A. Paris, E. Gu´erin, P. Collon, and E. Galin, “Authoring and simulating meandering rivers,” ACM TOG, vol. 42, no. 6, pp. 239:1–239:14, 2023. [134] O. Deussen, P. Hanrahan, B. Lintermann, R. Mech, M. Pharr, and P. Prusinkiewicz, “Realistic modeling and rendering of plant ecosystems,” in SIGGRAPH, 1998. [135] M. Makowski, T. H¨adrich, J. Scheffczyk, D. L. Michels, S. Pirk, and W. Palubicki, “Synthetic silviculture: multi-scale modeling of plant ecosystems,” ACM TOG, vol. 38, no. 4, pp. 131:1–131:14, 2019. [136] W. Palubicki, M. Makowski, W. Gajda, T. H¨adrich, D. L. Michels, and S. Pirk, “Ecoclimates: climate-response modeling of vegeta- tion,” ACM TOG, vol. 41, no. 4, pp. 155:1–155:19, 2022. [137] B. Benes, M. Abdul-Massih, P. Jarvis, D. G. Aliaga, and C. A. Vanegas, “Urban ecosystem design,” in Symposium on Interactive 3D Graphics and Games, I3D, 2011. [138] C. A. Vanegas, D. G. Aliaga, B. Benes, and P. Waddell, “Inter- active design of urban spaces using geometrical and behavioral modeling,” ACM TOG, vol. 28, no. 5, p. 111, 2009. [139] B. Weber, P. M¨uller, P. Wonka, and M. H. Gross, “Interactive geometric simulation of 4D cities,” Computer Graphics Forum, vol. 28, no. 2, pp. 481–492, 2009. [140] P. Merrell, “Example-based model synthesis,” in SI3D, 2007. [141] P. Merrell and D. Manocha, “Continuous model synthesis,” ACM TOG, vol. 27, no. 5, p. 158, 2008. [142] H. Zhou, J. Sun, G. Turk, and J. M. Rehg, “Terrain synthesis from digital elevation models,” IEEE TVCG, vol. 13, no. 4, pp. 834–848, 2007. [143] G. Nishida, I. Garcia-Dorado, and D. G. Aliaga, “Example-driven procedural urban roads,” Comput. Graph. Forum, vol. 35, no. 6, pp. 5–17, 2016. [144] C. A. Vanegas, I. Garcia-Dorado, D. G. Aliaga, B. Benes, and P. Waddell, “Inverse design of urban procedural models,” ACM TOG, vol. 31, no. 6, pp. 168:1–168:11, 2012. [145] A. Emilien, U. Vimont, M. Cani, P. Poulin, and B. Benes, “World- brush: interactive example-based synthesis of procedural virtual worlds,” ACM TOG, vol. 34, no. 4, pp. 106:1–106:11, 2015. [146] G. Kelly and H. McCabe, “Citygen: An interactive system for procedural city generation,” in Fifth International Conference on Game Design and Technology, 2007. [147] K. Xu, J. Stewart, and E. Fiume, “Constraint-based automatic placement for scene composition,” in Graphics Interface, 2002. [148] P. Merrell, E. Schkufza, Z. Li, M. Agrawala, and V. Koltun, “Interactive furniture layout using interior design guidelines,” ACM TOG, vol. 30, no. 4, p. 87, 2011. 20 [149] P. K´an and H. Kaufmann, “Automatic furniture arrangement using greedy cost minimization,” in VR, 2018. [150] Y. Zhao, K. Lin, Z. Jia, Q. Gao, G. Thattai, J. Thomason, and G. S. Sukhatme, “LUMINOUS: indoor scene generation for embodied AI challenges,” arXiv 2111.05527, 2021. [151] P. Merrell, E. Schkufza, and V. Koltun, “Computer-generated residential building layouts,” ACM TOG, vol. 29, no. 6, p. 181, 2010. [152] M. Fisher, D. Ritchie, M. Savva, T. A. Funkhouser, and P. Hanra- han, “Example-based synthesis of 3D object arrangements,” ACM TOG, vol. 31, no. 6, pp. 135:1–135:11, 2012. [153] L. Yu, S. K. Yeung, and D. Terzopoulos, “The Clutterpalette: An interactive tool for detailing indoor scenes,” IEEE TVCG, vol. 22, no. 2, pp. 1138–1148, 2016. [154] S. Qi, Y. Zhu, S. Huang, C. Jiang, and S. Zhu, “Human-Centric indoor scene synthesis using stochastic grammar,” in CVPR, 2018. [155] S. Zhang, S. Zhang, W. Xie, C. Luo, Y. Yang, and H. Fu, “Fast 3D indoor scene synthesis by learning spatial relation priors of objects,” IEEE TVCG, vol. 28, no. 9, pp. 3082–3092, 2022. [156] S. Zhang, Y. Li, Y. He, Y. Yang, and S. Zhang, “MageAdd: Real- time interaction simulation for scene synthesis,” in ACM MM, 2021. [157] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe, “Training language models to follow instructions with human feedback,” in NeurIPS, 2022. [158] OpenAI, “GPT-4 technical report,” arXiv 2303.08774, 2023. [159] Y. Yang, J. Lu, Z. Zhao, Z. Luo, J. J. Q. Yu, V. Sanchez, and F. Zheng, “LLplace: The 3D indoor scene layout generation and editing via large language model,” arXiv 2406.03866, 2024. [160] R. Aguina-Kang, M. Gumin, D. H. Han, S. Morris, S. J. Yoo, A. Ganeshan, R. K. Jones, Q. A. Wei, K. Fu, and D. Ritchie, “Open- universe indoor scene generation using LLM program synthesis and uncurated object databases,” arXiv 2403.09675, 2024. [161] J. Deng, W. Chai, J. Huang, Z. Zhao, Q. Huang, M. Gao, J. Guo, S. Hao, W. Hu, J. Hwang, X. Li, and G. Wang, “CityCraft: A real crafter for 3D city generation,” arXiv /2406.04983, 2024. [162] F. Sun, W. Liu, S. Gu, D. Lim, G. Bhat, F. Tombari, M. Li, N. Haber, and J. Wu, “LayoutVLM: Differentiable optimization of 3D layout via vision-language models,” in CVPR, 2025. [163] R. Fu, Z. Wen, Z. Liu, and S. Sridhar, “AnyHome: Open- vocabulary generation of structured and textured 3D homes,” in ECCV, 2024. [164] B. M. ¨Ocal, M. Tatarchenko, S. Karaoglu, and T. Gevers, “SceneTeller: Language-to-3D scene generation,” in ECCV, 2024. [165] Y. Zhang, Z. Li, M. Zhou, S. Wu, and J. Wu, “The Scene Language: Representing scenes with programs, words, and embeddings,” in CVPR, 2025. [166] X. Zhou, X. Ran, Y. Xiong, J. He, Z. Lin, Y. Wang, D. Sun, and M. Yang, “GALA3D: towards text-to-3D complex scene genera- tion via layout-guided generative Gaussian splatting,” in ICML, 2024. [167] A. C¸ elen, G. Han, K. Schindler, L. V. Gool, I. Armeni, A. Obukhov, and X. Wang, “I-Design: Personalized LLM interior designer,” arXiv 2404.02838, 2024. [168] W. Deng, M. Qi, and H. Ma, “Global-local tree search in vlms for 3D indoor scene generation,” in CVPR, 2025. [169] L. Liu, S. Chen, S. Jia, J. Shi, Z. Jiang, C. Jin, W. Zongkai, J. Hwang, and L. Li, “Graph canvas for controllable 3D scene generation,” arXiv 2412.00091, 2024. [170] G. Gao, W. Liu, A. Chen, A. Geiger, and B. Sch¨olkopf, “Graph- Dreamer: Compositional 3D scene synthesis from scene graphs,” in CVPR, 2024. [171] X. Li, H. Li, H. Chen, T. Mu, and S. Hu, “DIScene: Object decou- pling and interaction modeling for complex scene generation,” in SIGGRAPH Asia, 2024. [172] K. Bhat, N. Khanna, K. Channa, T. Zhou, Y. Zhu, X. Sun, C. Shang, A. Sudarshan, M. Chu, D. Li, K. Deng, J. Fauconnier, T. Verhuls- donck, M. Agrawala, K. Fatahalian, A. Weiss, C. Reiser, R. K. Chirravuri, R. Kandur, A. Pelaez, A. Garg, M. Palleschi, J. Wang, S. Litz, L. Liu, A. Li, D. Harmon, D. Liu, L. Feng, D. Goupil, L. Kuczynski, J. Yoon, N. Marri, P. Zhuang, Y. Zhang, B. Yin, H. Jiang, M. van Workum, T. Lane, B. Erickson, S. Pathare, K. Price, A. Singh, and D. Baszucki, “Cube: A roblox view of 3D intelligence,” arXiv 2503.15475, 2025. [173] J. Liu, S. Zhang, C. Zhang, and S. Zhang, “Controllable procedu- ral generation of landscapes,” in ACM MM, 2024. [174] Z. Hu, A. Iscen, A. Jain, T. Kipf, Y. Yue, D. A. Ross, C. Schmid, and A. Fathi, “SceneCraft: An LLM agent for synthesizing 3D scenes as blender code,” in ICML, 2024. [175] Y. Yang, F. Sun, L. Weihs, E. VanderBilt, A. Herrasti, W. Han, J. Wu, N. Haber, R. Krishna, L. Liu, C. Callison-Burch, M. Yatskar, A. Kembhavi, and C. Clark, “Holodeck: Language guided gener- ation of 3D embodied AI environments,” in CVPR, 2024. [176] X. Liu, C. Tang, and Y. Tai, “WorldCraft: Photo-realistic 3D world creation and customization via LLM agents,” arXiv 2502.15601, 2025. [177] D. Ritchie, K. Wang, and Y. Lin, “Fast and flexible indoor scene synthesis via deep convolutional generative models,” in CVPR, 2019. [178] Z. Zhang, Z. Yang, C. Ma, L. Luo, A. Huth, E. Vouga, and Q. Huang, “Deep generative modeling for scene synthesis via hybrid representations,” ACM TOG, vol. 39, no. 2, pp. 17:1–17:21, 2020. [179] H. Yang, Z. Zhang, S. Yan, H. Huang, C. Ma, Y. Zheng, C. Bajaj, and Q. Huang, “Scene synthesis via uncertainty-driven attribute synchronization,” in ICCV, 2021. [180] X. Wang, C. Yeshwanth, and M. Nießner, “SceneFormer: Indoor scene generation with transformers,” in 3DV, 2021. [181] W. R. Para, P. Guerrero, N. J. Mitra, and P. Wonka, “COFS: controllable furniture layout synthesis,” in SIGGRAPH, 2023. [182] Y. Nie, A. Dai, X. Han, and M. Nießner, “Learning 3D scene priors with 2D supervision,” in CVPR, 2023. [183] Y. Zhao, Z. Zhao, J. Li, S. Dong, and S. Gao, “RoomDesigner: En- coding anchor-latents for style-consistent and shape-compatible indoor scene generation,” in 3DV, 2024. [184] W. Feng, H. Zhou, J. Liao, L. Cheng, and W. Zhou, “CasaGPT: cuboid arrangement and scene assembly for interior design,” in CVPR, 2025. [185] L. Maillard, N. Sereyjol-Garros, T. Durand, and M. Ovsjanikov, “DeBaRA: Denoising-based 3D room arrangement generation,” in NeurIPS, 2024. [186] Y. Yang, B. Jia, P. Zhi, and S. Huang, “PhyScene: Physically interactable 3D scene synthesis for embodied AI,” in CVPR, 2024. [187] Z. Ye, X. Zheng, Y. Liu, and Y. Peng, “RelScene: A benchmark and baseline for spatial relations in text-driven 3D scene generation,” in ACM MM, 2024. [188] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre- training of deep bidirectional transformers for language under- standing,” in NAACL-HLT, 2019. [189] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” in CVPR, 2022. [190] C. Fang, X. Hu, K. Luo, and P. Tan, “Ctrl-Room: Controllable text- to-3D room meshes generation with layout constraints,” in 3DV, 2025. [191] A. Bokhovkin, Q. Meng, S. Tulsiani, and A. Dai, “SceneFactor: Factored latent 3D diffusion for controllable 3D scene genera- tion,” arXiv 2412.01801, 2024. [192] D. Epstein, B. Poole, B. Mildenhall, A. A. Efros, and A. Holynski, “Disentangled 3D scene generation with layout learning,” in ICML, 2024. [193] Q. Zhang, C. Wang, A. Siarohin, P. Zhuang, Y. Xu, C. Yang, D. Lin, B. Zhou, S. Tulyakov, and H. Lee, “SceneWiz3D: Towards text- guided 3D scene composition,” arXiv 2312.08885, 2023. [194] H. Li, H. Shi, W. Zhang, W. Wu, Y. Liao, L. Wang, L. Lee, and P. Y. Zhou, “DreamScene: 3D Gaussian-based text-to-3D scene generation via formation pattern sampling,” in ECCV, 2024. [195] B. Poole, A. Jain, J. T. Barron, and B. Mildenhall, “DreamFusion: Text-to-3D using 2D diffusion,” in ICLR, 2023. [196] Y. Nie, A. Dai, X. Han, and M. Nießner, “Pose2Room: Under- standing 3D scenes from human activities,” in ECCV, 2022. [197] S. Ye, Y. Wang, J. Li, D. Park, C. K. Liu, H. Xu, and J. Wu, “Scene synthesis from human motion,” in SIGGRAPH Asia, 2022. [198] V. D. An, M. N. Vu, T. Nguyen, B. Huang, D. Nguyen, T. Vo, and A. Nguyen, “Language-driven scene synthesis using multi- conditional diffusion model,” in NeurIPS, 2023. [199] J. Li, T. Huang, Q. Zhu, and T. Wong, “Physics-based scene layout generation from human motion,” in SIGGRAPH, 2024. [200] A. X. Chang, M. Savva, and C. D. Manning, “Learning spatial knowledge for text to 3D scene generation,” in EMNLP, 2014. 21 [201] Z. S. Kermani, Z. Liao, P. Tan, and H. Zhang, “Learning 3D scene synthesis from annotated RGB-D images,” Computer Graphics Forum, vol. 35, no. 5, pp. 197–206, 2016. [202] Q. Fu, X. Chen, X. Wang, S. Wen, B. Zhou, and H. Fu, “Adaptive synthesis of indoor scenes via activity-associated object relation graphs,” ACM TOG, vol. 36, no. 6, pp. 201:1–201:13, 2017. [203] R. Ma, A. G. Patil, M. Fisher, M. Li, S. Pirk, B. Hua, S. Yeung, X. Tong, L. J. Guibas, and H. Zhang, “Language-driven synthesis of 3D scenes from scene databases,” ACM TOG, vol. 37, no. 6, p. 212, 2018. [204] A. Luo, Z. Zhang, J. Wu, and J. B. Tenenbaum, “End-to-end optimization of scene layout,” in CVPR, 2020. [205] A. Kar, A. Prakash, M. Liu, E. Cameracci, J. Yuan, M. Rusiniak, D. Acuna, A. Torralba, and S. Fidler, “Meta-sim: Learning to generate synthetic datasets,” in ICCV, 2019. [206] J. Devaranjan, A. Kar, and S. Fidler, “Meta-sim2: Unsupervised learning of scene structure for synthetic data generation,” in ECCV, 2020. [207] L. Gao, J. Sun, K. Mo, Y. Lai, L. J. Guibas, and J. Yang, “Scene- HGN: Hierarchical graph networks for 3D indoor scene genera- tion with fine-grained geometry,” IEEE TPAMI, vol. 45, no. 7, pp. 8902–8919, 2023. [208] G. Zhai, E. P. ¨Ornek, D. Z. Chen, R. Liao, Y. Di, N. Navab, F. Tombari, and B. Busam, “EchoScene: Indoor scene generation via information echo over scene graph diffusion,” in ECCV, 2024. [209] Z. Yang, K. Lu, C. Zhang, J. Qi, H. Jiang, R. Ma, S. Yin, Y. Xu, M. Xing, Z. Xiao, J. Long, X. Liu, and G. Zhai, “MMGDreamer: mixed-modality graph for geometry-controllable 3D indoor scene generation,” in AAAI, 2025. [210] Z. Wu, M. Feng, Y. Wang, H. Xie, W. Dong, B. Miao, and A. Mian, “External knowledge enhanced 3D scene generation from sketch,” in ECCV, 2024. [211] Y. Liu, X. Li, Y. Zhang, L. Qi, X. Li, W. Wang, C. Li, X. Li, and M.- H. Yang, “Controllable 3D outdoor scene generation via scene graphs,” in CVPR, 2025. [212] W. Dong, B. Yang, Z. Yang, Y. Li, T. Hu, H. Bao, Y. Ma, and Z. Cui, “HiScene: creating hierarchical 3D scenes with isometric view generation,” arXiv 2504.13072, 2025. [213] Q. Zhang, Y. Xu, Y. Shen, B. Dai, B. Zhou, and C. Yang, “Berf- Scene: Bev-conditioned equivariant radiance fields for infinite 3D scene generation,” in CVPR, 2024. [214] H. Yan, Y. Li, Z. Wu, S. Chen, W. Sun, T. Shang, W. Liu, T. Chen, X. Dai, C. Ma, H. Li, and P. Ji, “Frankenstein: Gen- erating semantic-compositional 3D scenes in one tri-plane,” in SIGGRAPH Asia, 2024. [215] E. R. Chan, C. Z. Lin, M. A. Chan, K. Nagano, B. Pan, S. D. Mello, O. Gallo, L. J. Guibas, J. Tremblay, S. Khamis, T. Karras, and G. Wetzstein, “Efficient geometry-aware 3D generative ad- versarial networks,” in CVPR, 2022. [216] C. H. Lin, H. Lee, Y. Cheng, S. Tulyakov, and M. Yang, “Infinity- GAN: Towards infinite-pixel image synthesis,” in ICLR, 2022. [217] H. Xie, Z. Chen, F. Hong, and Z. Liu, “Generative Gaussian splatting for unbounded 3D city generation,” in CVPR, 2025. [218] H. Xie, Z. Chen, F. Hong, and Z. Liu, “Compositional generative model of unbounded 4D cities,” arXiv 2501.08983, 2025. [219] Y. Yang, Y. Yang, H. Guo, R. Xiong, Y. Wang, and Y. Liao, “UrbanGIRAFFE: Representing urban scenes as compositional generative neural feature fields,” in ICCV, 2023. [220] Y. Xu, M. Chai, Z. Shi, S. Peng, I. Skorokhodov, A. Siarohin, C. Yang, Y. Shen, H. Lee, B. Zhou, and S. Tulyakov, “DisCoScene: Spatially disentangled generative radiance fields for controllable 3D-aware scene synthesis,” in CVPR, 2023. [221] Y. Lin, H. Bai, S. Li, H. Lu, X. Lin, H. Xiong, and L. Wang, “CompoNeRF: Text-guided multi-object compositional nerf with editable 3D scene layout,” arXiv 2303.13843, 2023. [222] D. Cohen-Bar, E. Richardson, G. Metzer, R. Giryes, and D. Cohen- Or, “Set-the-Scene: Global-local training for generating control- lable nerf scenes,” in ICCV, 2023. [223] J. Zhou, X. Li, L. Qi, and M. Yang, “Layout-your-3D: Controllable and precise 3D generation with 2D blueprint,” arXiv 2410.15391, 2024. [224] X. Yang, Y. Man, J. Chen, and Y. Wang, “SceneCraft: Layout- guided 3D scene generation,” in NeurIPS, 2024. [225] M. Chen, L. Wang, S. Ao, Y. Zhang, K. Xu, and Y. Guo, “Lay- out2Scene: 3D semantic layout guided scene generation via ge- ometry and appearance diffusion priors,” arXiv 2501.02519, 2025. [226] F. Lu, K. Lin, Y. Xu, H. Li, G. Chen, and C. Jiang, “Urban Ar- chitect: Steerable 3D urban scene generation with layout prior,” arXiv 2404.06780, 2024. [227] A. R. Kosiorek, H. Strathmann, D. Zoran, P. Moreno, R. Schneider, S. Mokr´a, and D. J. Rezende, “NeRF-VAE: A geometry aware 3D scene generative model,” in ICML, 2021. [228] M. Niemeyer and A. Geiger, “GIRAFFE: representing scenes as compositional generative neural feature fields,” in CVPR, 2021. [229] L. Chai, R. Tucker, Z. Li, P. Isola, and N. Snavely, “Persistent Nature: A generative model of unbounded 3D worlds,” in CVPR, 2023. [230] Y. Yang, J. Shao, X. Li, Y. Shen, A. Geiger, and Y. Liao, “Prometheus: 3D-aware latent diffusion models for feed-forward text-to-3D scene generation,” arXiv 2412.21117, 2024. [231] H. Go, B. Park, J. Jang, J. Kim, S. Kwon, and C. Kim, “Splat- Flow: Multi-view rectified flow model for 3D gaussian splatting synthesis,” in CVPR, 2025. [232] J. Lee, W. Im, S. Lee, and S. Yoon, “Diffusion probabilistic models for scene-scale 3D categorical data,” arXiv 2301.00527, 2023. [233] X. Ju, Z. Huang, Y. Li, G. Zhang, Y. Qiao, and H. Li, “DiffInD- Scene: Diffusion-based high-quality 3D indoor scene generation,” in CVPR, 2024. [234] Y. Liu, X. Li, X. Li, L. Qi, C. Li, and M. Yang, “Pyramid diffusion for fine 3D large scene generation,” in ECCV, 2024. [235] Q. Meng, L. Li, M. Nießner, and A. Dai, “LT3SD: latent trees for 3D scene diffusion,” in CVPR, 2025. [236] J. Lee, S. Lee, C. Jo, W. Im, J. Seon, and S. Yoon, “SemCity: Semantic scene generation with triplane diffusion,” in CVPR, 2024. [237] H. Lee, Q. Han, and A. X. Chang, “NuiScene: Exploring efficient generation of unbounded outdoor scenes,” arXiv 2503.16375, 2025. [238] H. Bian, L. Kong, H. Xie, L. Pan, Y. Qiao, and Z. Liu, “DynamicC- ity: Large-scale 4D occupancy generation from dynamic scenes,” in ICLR, 2025. [239] A. Cao and J. Johnson, “HexPlane: A fast representation for dynamic scenes,” in CVPR, 2023. [240] N. Akimoto, S. Kasai, M. Hayashi, and Y. Aoki, “360-degree image completion by two-stage conditional gans,” in ICIP, 2019. [241] J. S. Sumantri and I. K. Park, “360 panorama synthesis from a sparse set of images with unknown field of view,” in WACV, 2020. [242] G. Somanath and D. Kurz, “HDR environment map estimation for real-time augmented reality,” in CVPR, 2021. [243] T. Hara, Y. Mukuta, and T. Harada, “Spherical image generation from a single image by considering scene symmetry,” in AAAI, 2021. [244] T. Hara, Y. Mukuta, and T. Harada, “Spherical image generation from a few normal-field-of-view images by considering scene symmetry,” IEEE TPAMI, vol. 45, no. 5, pp. 6339–6353, 2023. [245] C. Oh, W. Cho, Y. Chae, D. Park, L. Wang, and K. Yoon, “BIPS: bi-modal indoor panorama synthesis via residual depth-aided adversarial learning,” in ECCV, 2022. [246] S. Zhao, J. Cui, Y. Sheng, Y. Dong, X. Liang, E. I. Chang, and Y. Xu, “Large scale image completion via co-modulated generative ad- versarial networks,” in ICLR, 2021. [247] P. Esser, R. Rombach, and B. Ommer, “Taming transformers for high-resolution image synthesis,” in CVPR, 2021. [248] N. Akimoto, Y. Matsuo, and Y. Aoki, “Diverse plausible 360- degree image outpainting for efficient 3DCG background cre- ation,” in CVPR, 2022. [249] H. Ai, Z. Cao, H. Lu, C. Chen, J. Ma, P. Zhou, T. Kim, P. Hui, and L. Wang, “Dream360: Diverse and immersive outdoor virtual scene creation via transformer-based 360° image outpainting,” IEEE TVCG, vol. 30, no. 5, pp. 2734–2744, 2024. [250] T. Wu, C. Zheng, and T. Cham, “PanoDiffusion: 360-degree panorama outpainting via diffusion,” in ICLR, 2024. [251] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agar- wal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, “Learning transferable visual models from natural language supervision,” in ICML, 2021. [252] Y. Lee, K. Kim, H. Kim, and M. Sung, “SyncDiffusion: Coherent montage via synchronized joint diffusions,” in NeurIPS, 2023. [253] O. Bar-Tal, L. Yariv, Y. Lipman, and T. Dekel, “MultiDiffusion: Fusing diffusion paths for controlled image generation,” in ICML, 2023. 22 [254] Q. Zhang, J. Song, X. Huang, Y. Chen, and M. Liu, “DiffCollage: Parallel generation of large content with diffusion models,” in CVPR, 2023. [255] N. Kalischek, M. Oechsle, F. Manhardt, P. Henzler, K. Schindler, and F. Tombari, “CubeDiff: Repurposing diffusion-based image models for panorama generation,” in ICLR, 2025. [256] H. Wang, X. Xiang, Y. Fan, and J. Xue, “Customizing 360-degree panoramas through text-to-image diffusion models,” in WACV, 2024. [257] M. Feng, J. Liu, M. Cui, and X. Xie, “Diffusion360: Seamless 360 degree panoramic image generation based on diffusion models,” arXiv 2311.13141, 2023. [258] J. Wang, Z. Chen, J. Ling, R. Xie, and L. Song, “360-degree panorama generation from few unregistered nfov images,” in ACM MM, 2023. [259] W. Ye, C. Ji, Z. Chen, J. Gao, X. Huang, S. Zhang, W. Ouyang, T. He, C. Zhao, and G. Zhang, “DiffPano: Scalable and consis- tent text to panorama generation with spherical epipolar-aware diffusion,” in NeurIPS, 2024. [260] G. B. M. Stan, D. Wofk, S. Fox, A. Redden, W. Saxton, J. Yu, E. Aflalo, S. Tseng, F. Nonato, M. M¨uller, and V. Lal, “LDM3D: latent diffusion model for 3D,” arXiv 2305.10853, 2023. [261] J. Schult, S. S. Tsai, L. H¨ollein, B. Wu, J. Wang, C. Ma, K. Li, X. Wang, F. Wimbauer, Z. He, P. Zhang, B. Leibe, P. Vajda, and J. Hou, “ControlRoom3D: Room generation using semantic proxy rooms,” in CVPR, 2024. [262] S. Zhou, Z. Fan, D. Xu, H. Chang, P. Chari, T. Bharadwaj, S. You, Z. Wang, and A. Kadambi, “DreamScene360: Unconstrained text- to-3D scene generation with panoramic Gaussian splatting,” in ECCV, 2024. [263] Y. Ma, D. Zhan, and Z. Jin, “FastScene: Text-driven fast indoor 3D scene generation via panoramic Gaussian splatting,” in IJCAI, 2024. [264] H. Zhou, X. Cheng, W. Yu, Y. Tian, and L. Yuan, “HoloDreamer: Holistic 3D panoramic world generation from text descriptions,” arXiv 2407.15187, 2024. [265] W. Li, Y. Mi, F. Cai, Z. Yang, W. Zuo, X. Wang, and X. Fan, “SceneDreamer360: Text-driven 3D-consistent scene generation with panoramic Gaussian splatting,” arXiv 2408.13711, 2024. [266] X. Lu, Z. Li, Z. Cui, M. R. Oswald, M. Pollefeys, and R. Qin, “Geometry-aware satellite-to-ground image synthesis for urban areas,” in CVPR, 2020. [267] Y. Shi, D. Campbell, X. Yu, and H. Li, “Geometry-guided street- view panorama synthesis from satellite imagery,” IEEE TPAMI, vol. 44, no. 12, pp. 10 009–10 022, 2022. [268] S. Wu, H. Tang, X. Jing, H. Zhao, J. Qian, N. Sebe, and Y. Yan, “Cross-view panorama image synthesis,” IEEE TMM, vol. 25, pp. 3546–3559, 2023. [269] Z. Li, Z. Li, Z. Cui, R. Qin, M. Pollefeys, and M. R. Oswald, “Sat2vid: Street-view panoramic video synthesis from a single satellite image,” in ICCV, 2021. [270] M. Qian, J. Xiong, G. Xia, and N. Xue, “Sat2Density: Faithful density learning from satellite-ground image pairs,” in ICCV, 2023. [271] N. Xu and R. Qin, “Geospecific view generation geometry- context aware high-resolution ground view inference from satel- lite views,” in ECCV, 2024. [272] S. Niklaus, L. Mai, J. Yang, and F. Liu, “3D ken burns effect from a single image,” ACM TOG, vol. 38, no. 6, pp. 184:1–184:15, 2019. [273] O. Wiles, G. Gkioxari, R. Szeliski, and J. Johnson, “SynSin: End- to-end view synthesis from a single image,” in CVPR, 2020. [274] J. Y. Koh, H. Agrawal, D. Batra, R. Tucker, A. Waters, H. Lee, Y. Yang, J. Baldridge, and P. Anderson, “Simple and effective synthesis of indoor 3D scenes,” in AAAI, 2023. [275] R. Tucker and N. Snavely, “Single-view view synthesis with multiplane images,” in CVPR, 2020. [276] T. A. Habtegebrial, V. Jampani, O. Gallo, and D. Stricker, “Gener- ative view synthesis: From single-view semantics to novel-view images,” in NeurIPS, 2020. [277] M. Shih, S. Su, J. Kopf, and J. Huang, “3D photography using context-aware layered depth inpainting,” in CVPR, 2020. [278] R. Hu, N. Ravi, A. C. Berg, and D. Pathak, “Worldsheet: Wrap- ping the world in a 3D sheet for view synthesis from a single image,” in ICCV, 2021. [279] J. Y. Koh, H. Lee, Y. Yang, J. Baldridge, and P. Anderson, “Path- dreamer: A world model for indoor navigation,” in ICCV, 2021. [280] Y. Shen, W. Ma, and S. Wang, “SGAM: building a virtual 3D world through simultaneous generation and mapping,” in NeurIPS, 2022. [281] S. Cai, E. R. Chan, S. Peng, M. Shahbazi, A. Obukhov, L. V. Gool, and G. Wetzstein, “DiffDreamer: Towards consistent unsuper- vised single-view scene extrapolation with conditional diffusion models,” in ICCV, 2023. [282] X. Ren and X. Wang, “Look outside the room: Synthesizing A consistent long-term 3D scene video from A single image,” in CVPR, 2022. [283] H. Tseng, Q. Li, C. Kim, S. Alsisan, J. Huang, and J. Kopf, “Consistent view synthesis with pose-guided diffusion models,” in CVPR, 2023. [284] J. J. Yu, F. Forghani, K. G. Derpanis, and M. A. Brubaker, “Long- term photometric consistent novel view synthesis with diffusion models,” in ICCV, 2023. [285] M. Wallingford, A. Bhattad, A. Kusupati, V. Ramanujan, M. Deitke, A. Kembhavi, R. Mottaghi, W. Ma, and A. Farhadi, “From an Image to a Scene: Learning to imagine the world from a million 360° videos,” in NeurIPS, 2024. [286] R. Gao, A. Holynski, P. Henzler, A. Brussee, R. Martin-Brualla, P. P. Srinivasan, J. T. Barron, and B. Poole, “CAT3D: create anything in 3D with multi-view diffusion models,” in NeurIPS, 2024. [287] S. Szymanowicz, J. Y. Zhang, P. P. Srinivasan, R. Gao, A. Brussee, A. Holynski, R. Martin-Brualla, J. T. Barron, and P. Henzler, “Bolt3D: Generating 3D scenes in seconds,” arXiv 2503.14445, 2025. [288] L. Zhang, A. Rao, and M. Agrawala, “Adding conditional control to text-to-image diffusion models,” in ICCV, 2023. [289] J. Li and M. Bansal, “PanoGen: Text-conditioned panoramic environment generation for vision-and-language navigation,” in NeurIPS, 2023. [290] Z. Lu, K. Hu, C. Wang, L. Bai, and Z. Wang, “Autoregressive omni-aware outpainting for open-vocabulary 360-degree image generation,” in AAAI, 2024. [291] A. Liu, Z. Li, Z. Chen, N. Li, Y. Xu, and B. A. Plummer, “PanoFree: tuning-free holistic multi-view image generation with cross-view self-guidance,” in ECCV, 2024. [292] P. Gao, K. Yao, T. Ye, S. Wang, Y. Yao, and X. Wang, “Opa-ma: Text guided mamba for 360-degree image out-painting,” arXiv 2407.10923, 2024. [293] P. Engstler, A. Vedaldi, I. Laina, and C. Rupprecht, “Invisible Stitch: Generating smooth 3D scenes with depth inpainting,” in 3DV, 2025. [294] R. Ranftl, K. Lasinger, D. Hafner, K. Schindler, and V. Koltun, “Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer,” IEEE TPAMI, vol. 44, no. 3, pp. 1623–1637, 2022. [295] S. F. Bhat, R. Birkl, D. Wofk, P. Wonka, and M. M¨uller, “Zoedepth: Zero-shot transfer by combining relative and metric depth,” arXiv 2302.12288, 2023. [296] S. M. H. Miangoleh, S. Dille, L. Mai, S. Paris, and Y. Aksoy, “Boosting monocular depth estimation models to high-resolution via content-adaptive multi-resolution merging,” in CVPR, 2021. [297] R. Fridman, A. Abecasis, Y. Kasten, and T. Dekel, “SceneScape: Text-driven consistent scene generation,” in NeurIPS, 2023. [298] X. Li, Y. Wu, J. Cen, J. Peng, K. Wang, K. Xian, Z. Wang, Z. Cao, and G. Lin, “iControl3D: An interactive system for controllable 3D scene generation,” in ACM MM, 2024. [299] S. Zhang, Y. Zhang, Q. Zheng, R. Ma, W. Hua, H. Bao, W. Xu, and C. Zou, “3D-SceneDreamer: Text-driven 3D-consistent scene generation,” in CVPR, 2024. [300] Y. Yang, F. Yin, J. Fan, X. Chen, W. Li, and G. Yu, “Scene123: One prompt to 3D scene generation via video-assisted and consistency-enhanced MAE,” arXiv 2408.05477, 2024. [301] H. Ouyang, K. Heal, S. Lombardi, and T. Sun, “Text2Immersion: generative immersive scene with 3D Gaussians,” arXiv 2312.09242, 2023. [302] H. Yu, H. Duan, C. Herrmann, W. T. Freeman, and J. Wu, “WonderWorld: Interactive 3D scene generation from a single image,” in CVPR, 2025. [303] J. Shriram, A. Trevithick, L. Liu, and R. Ramamoorthi, “Realm- Dreamer: text-driven 3D scene generation with inpainting and depth diffusion,” in 3DV, 2025. [304] X. Hou, M. Li, D. Yang, J. Chen, Z. Qian, X. Zhao, Y. Jiang, J. Wei, Q. Xu, and L. Zhang, “BloomScene: Lightweight structured 3D 23 gaussian splatting for crossmodal scene generation,” in AAAI, 2025. [305] C. Ni, X. Wang, Z. Zhu, W. Wang, H. Li, G. Zhao, J. Li, W. Qin, G. Huang, and W. Mei, “WonderTurbo: Generating interactive 3D world in 0.72 seconds,” arXiv 2504.02261, 2025. [306] Z. Chen, J. Tang, Y. Dong, Z. Cao, F. Hong, Y. Lan, T. Wang, H. Xie, T. Wu, S. Saito, L. Pan, D. Lin, and Z. Liu, “3DTopia-XL: scaling high-quality 3D asset generation via primitive diffusion,” in CVPR, 2024. [307] J. Xiang, Z. Lv, S. Xu, Y. Deng, R. Wang, B. Zhang, D. Chen, X. Tong, and J. Yang, “Structured 3D latents for scalable and versatile 3D generation,” in CVPR, 2025. [308] Y. Lan, F. Hong, S. Yang, S. Zhou, X. Meng, B. Dai, X. Pan, and C. C. Loy, “LN3Diff: scalable latent neural fields diffusion for speedy 3D generation,” in ECCV, 2024. [309] F. Hong, J. Tang, Z. Cao, M. Shi, T. Wu, Z. Chen, T. Wang, L. Pan, D. Lin, and Z. Liu, “3DTopia: large text-to-3D generation model with hybrid diffusion priors,” arXiv 2403.02234, 2024. [310] Y. Lan, S. Zhou, Z. Lyu, F. Hong, S. Yang, B. Dai, X. Pan, and C. C. Loy, “GaussianAnything: interactive point cloud latent diffusion for 3D generation,” in ICLR, 2025. [311] P. Engstler, A. Shtedritski, I. Laina, C. Rupprecht, and A. Vedaldi, “SynCity: Training-free generation of 3D worlds,” arXiv 2503.16420, 2025. [312] L. Shen, X. Li, H. Sun, J. Peng, K. Xian, Z. Cao, and G. Lin, “Make- it-4D: Synthesizing a consistent long-term dynamic scene video from a single image,” in ACM MM, 2023. [313] I.-H. Jin, H. Choo, S.-H. Jeong, H. Park, J. Kim, O. joon Kwon, and K. Kong, “Optimizing 4D Gaussians for dynamic scene video from single landscape images,” in ICLR, 2025. [314] Y. Lee, Y. Chen, A. Wang, T. Liao, B. Y. Feng, and J. Huang, “VividDream: Generating 3D scene with ambient dynamics,” ICLR, 2025. [315] H. Feng, Z. Ding, Z. Xia, S. Niklaus, V. F. Abrevaya, M. J. Black, and X. Zhang, “Explorative inbetweening of time and space,” in ECCV, 2024. [316] V. Gupta, Y. Man, and Y. Wang, “PaintScene4D: Consistent 4D scene generation from text prompts,” arXiv 2412.04471, 2024. [317] T. Liu, Z. Huang, Z. Chen, G. Wang, S. Hu, l. Shen, H. Sun, Z. Cao, W. Li, and Z. Liu, “Free4D: Tuning-free 4D scene generation with spatial-temporal consistency,” arXiv 2503.20785, 2025. [318] K. Liu, L. Shao, and S. Lu, “Novel view extrapolation with video diffusion priors,” arXiv 2411.14208, 2024. [319] W. Yu, J. Xing, L. Yuan, W. Hu, X. Li, Z. Huang, X. Gao, T. Wong, Y. Shan, and Y. Tian, “ViewCrafter: Taming video diffusion mod- els for high-fidelity novel view synthesis,” arXiv 2409.02048, 2024. [320] L. Chen, Z. Zhou, M. Zhao, Y. Wang, G. Zhang, W. Huang, H. Sun, J.-R. Wen, and C. Li, “FlexWorld: Progressively expand- ing 3D scenes for flexiable-view synthesis,” arXiv 2503.13265, 2025. [321] H. Feng, Z. Zuo, J.-H. Pan, K.-H. Hui, Y. Shao, Q. Dou, W. Xie, and Z. Liu, “WonderVerse: Extendable 3D scene generation with video generative models,” arXiv 2503.09160, 2025. [322] S. Zhang, J. Li, X. Fei, H. Liu, and Y. Duan, “Scene Splatter: Momentum 3D scene generation from single image with video diffusion model,” in CVPR, 2025. [323] J. Hao, P. Wang, H. Wang, X. Zhang, and Z. Guo, “GaussVideo- Dreamer: 3D scene generation with video diffusion and inconsistency-aware gaussian splatting,” arXiv 2504.10001, 2025. [324] H. Liang, J. Cao, V. Goel, G. Qian, S. Korolev, D. Terzopoulos, K. N. Plataniotis, S. Tulyakov, and J. Ren, “Wonderland: Navigat- ing 3D scenes from a single image,” in CVPR, 2025. [325] H. Wang, F. Liu, J. Chi, and Y. Duan, “VideoScene: Distilling video diffusion model to generate 3D scenes in one step,” in CVPR, 2025. [326] R. Wu, R. Gao, B. Poole, A. Trevithick, C. Zheng, J. T. Barron, and A. Holynski, “CAT4D: create anything in 4D with multi-view video diffusion models,” in CVPR, 2025. [327] S. Zhai, Z. Ye, J. Liu, W. Xie, J. Hu, Z. Peng, H. Xue, D. Chen, X. Wang, L. Yang, N. Wang, H. Liu, and G. Zhang, “StarGen: A spatiotemporal autoregression framework with video diffusion model for scalable and controllable scene generation,” in CVPR, 2025. [328] B. Deng, R. Tucker, Z. Li, L. J. Guibas, N. Snavely, and G. Wet- zstein, “Streetscapes: Large-scale consistent street view genera- tion using autoregressive video diffusion,” in SIGGRAPH, 2024. [329] Q. Wang, W. Li, C. Mou, X. Cheng, and J. Zhang, “360DVD: Controllable panorama video generation with 360-degree video diffusion model,” in CVPR, 2024. [330] J. Tan, S. Yang, T. Wu, J. He, Y. Guo, Z. Liu, and D. Lin, “Imagine360: Immersive 360 video generation from perspective anchor,” arXiv 2412.03552, 2024. [331] T. Lu, T. Shu, A. L. Yuille, D. Khashabi, and J. Chen, “Generative world explorer,” ICLR, 2025. [332] J. Liu, S. Lin, Y. Li, and M. Yang, “DynamicScaler: Seamless and scalable video generation for panoramic scenes,” in CVPR, 2025. [333] E. Alonso, A. Jelley, V. Micheli, A. Kanervisto, A. J. Storkey, T. Pearce, and F. Fleuret, “Diffusion for world modeling: Visual details matter in atari,” in NeurIPS, 2024. [334] D. Valevski, Y. Leviathan, M. Arar, and S. Fruchter, “Diffusion models are real-time game engines,” in ICLR, 2025. [335] Decart, J. Quevedo, Q. McIntyre, S. Campbell, X. Chen, and R. Wachen, “Oasis: A universe in a transformer,” 2024. [Online]. Available: https://oasis-model.github.io/ [336] Z. Xiao, Y. Lan, Y. Zhou, W. Ouyang, S. Yang, Y. Zeng, and X. Pan, “WORLDMEM: Long-term consistent world simulation with memory,” arXiv 2504.12369, 2025. [337] X. Wang, Z. Zhu, G. Huang, X. Chen, and J. Lu, “DriveDreamer: Towards real-world-driven world models for autonomous driv- ing,” in ECCV, 2024. [338] Y. Wang, J. He, L. Fan, H. Li, Y. Chen, and Z. Zhang, “Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving,” in CVPR, 2024. [339] A. Hu, L. Russell, H. Yeo, Z. Murez, G. Fedoseev, A. Kendall, J. Shotton, and G. Corrado, “GAIA-1: A generative world model for autonomous driving,” arXiv 2309.17080, 2023. [340] X. Li, Y. Zhang, and X. Ye, “DrivingDiffusion: layout-guided multi-view driving scenarios video generation with latent dif- fusion model,” in ECCV, 2024. [341] B. Xie, Y. Liu, T. Wang, J. Cao, and X. Zhang, “Glad: A streaming scene generator for autonomous driving,” in ICLR, 2025. [342] R. Gao, K. Chen, Z. Li, L. Hong, Z. Li, and Q. Xu, “MagicDrive3D: Controllable 3D generation for any-view rendering in street scenes,” arXiv 2405.14475, 2024. [343] J. Mao, B. Li, B. Ivanovic, Y. Chen, Y. Wang, Y. You, C. Xiao, D. Xu, M. Pavone, and Y. Wang, “DreamDrive: Generative 4D scene modeling from street view images,” arXiv 2501.00601, 2025. [344] Y. Yan, Z. Xu, H. Lin, H. Jin, H. Guo, Y. Wang, K. Zhan, X. Lang, H. Bao, X. Zhou, and S. Peng, “StreetCrafter: Street view synthe- sis with controllable video diffusion models,” in CVPR, 2025. [345] J. Mei, Y. Ma, X. Yang, L. Wen, T. Wei, M. Dou, B. Shi, and Y. Liu, “DreamForge: Motion-aware autoregressive video generation for multi-view driving scenes,” arXiv 2409.04003, 2024. [346] Y. Wen, Y. Zhao, Y. Liu, F. Jia, Y. Wang, C. Luo, C. Zhang, T. Wang, X. Sun, and X. Zhang, “Panacea: Panoramic and controllable video generation for autonomous driving,” in CVPR, 2024. [347] H. Lu, X. Wu, S. Wang, X. Qin, X. Zhang, J. Han, W. Zuo, and J. Tao, “Seeing Beyond Views: Multi-view driving scene video generation with holistic attention,” arXiv 2412.03520, 2024. [348] D. Liang, D. Zhang, X. Zhou, S. Tu, T. Feng, X. Li, Y. Zhang, M. Du, X. Tan, and X. Bai, “Seeing the Future, Perceiving the Future: A unified driving world model for future generation and perception,” arXiv 2503.13587, 2025. [349] J. Guo, Y. Ding, X. Chen, S. Chen, B. Li, Y. Zou, X. Lyu, F. Tan, X. Qi, Z. Li, and H. Zhao, “DiST-4D: Disentangled spatiotemporal diffusion with metric depth for 4D driving scene generation,” arXiv 2503.15208, 2025. [350] L. Russell, A. Hu, L. Bertoni, G. Fedoseev, J. Shotton, E. Arani, and G. Corrado, “GAIA-2: A controllable multi-view generative world model for autonomous driving,” arXiv 2503.20523, 2025. [351] X. Yang, L. Wen, Y. Ma, J. Mei, X. Li, T. Wei, W. Lei, D. Fu, P. Cai, M. Dou, B. Shi, L. He, Y. Liu, and Y. Qiao, “DriveArena: A closed- loop generative simulation platform for autonomous driving,” arXiv 2408.00415, 2024. [352] G. Zhao, X. Wang, Z. Zhu, X. Chen, G. Huang, X. Bao, and X. Wang, “DriveDreamer-2: LLM-enhanced world models for diverse driving video generation,” in AAAI, 2025. [353] J. Jiang, G. Hong, L. Zhou, E. Ma, H. Hu, X. Zhou, J. Xiang, F. Liu, K. Yu, H. Sun, K. Zhan, P. Jia, and M. Zhang, “DiVE: Dit-based video generation with enhanced control,” arXiv 2409.01595, 2024. [354] M. Hassan, S. Stapf, A. Rahimi, P. M. B. Rezende, Y. Haghighi, D. Br¨uggemann, I. Katircioglu, L. Zhang, X. Chen, S. Saha, M. Cannici, E. Aljalbout, B. Ye, X. Wang, A. Davtyan, M. Salz- 24 mann, D. Scaramuzza, M. Pollefeys, P. Favaro, and A. Alahi, “GEM: A generalizable ego-vision multimodal world model for fine-grained ego-motion, object dynamics, and scene composition control,” in CVPR, 2025. [355] J. Lu, Z. Huang, Z. Yang, J. Zhang, and L. Zhang, “WoVoGen: world volume-aware diffusion for controllable multi-camera driving scene generation,” in ECCV, 2024. [356] L. Li, W. Qiu, Y. Cai, X. Yan, Q. Lian, B. Liu, and Y. Chen, “SyntheOcc: Synthesize geometric-controlled street view images through 3D semantic mpis,” arXiv 2410.00337, 2024. [357] Y. Lu, X. Ren, J. Yang, T. Shen, Z. Wu, J. Gao, Y. Wang, S. Chen, M. Chen, S. Fidler, and J. Huang, “InfiniCube: Unbounded and controllable dynamic 3D driving scene generation with world- guided video models,” arXiv 2412.03934, 2024. [358] B. Li, J. Guo, H. Liu, Y. Zou, Y. Ding, X. Chen, H. Zhu, F. Tan, C. Zhang, T. Wang, S. Zhou, L. Zhang, X. Qi, H. Zhao, M. Yang, W. Zeng, and X. Jin, “UniScene: Unified occupancy-centric driv- ing scene generation,” in CVPR, 2025. [359] T. Yan, D. Wu, W. Han, J. Jiang, X. Zhou, K. Zhan, C. Xu, and J. Shen, “DrivingSphere: Building a high-fidelity 4D world for closed-loop simulation,” in CVPR, 2025. [360] Y. Zhang, S. Gong, K. Xiong, X. Ye, X. Tan, F. Wang, J. Huang, H. Wu, and H. Wang, “BEVWorld: A multimodal world model for autonomous driving via unified BEV latent space,” arXiv 2407.05679, 2024. [361] Z. Wu, J. Ni, X. Wang, Y. Guo, R. Chen, L. Lu, J. Dai, and Y. Xiong, “HoloDrive: Holistic 2D-3D multi-modal street scene generation for autonomous driving,” arXiv 2412.01407, 2024. [362] Y. Wu, H. Zhang, T. Lin, L. Huang, S. Luo, R. Wu, C. Qiu, W. Ke, and T. Zhang, “Generating multimodal driving scenes via next- scene prediction,” in CVPR, 2025. [363] W. Wu, X. Guo, W. Tang, T. Huang, C. Wang, D. Chen, and C. Ding, “DriveScape: Towards high-resolution controllable multi-view driving video generation,” in CVPR, 2025. [364] E. Ma, L. Zhou, T. Tang, Z. Zhang, D. Han, J. Jiang, K. Zhan, P. Jia, X. Lang, H. Sun, D. Lin, and K. Yu, “Unleashing generalization of end-to-end autonomous driving with controllable long video generation,” arXiv 2406.01349, 2024. [365] R. Gao, K. Chen, B. Xiao, L. Hong, Z. Li, and Q. Xu, “MagicDrive- V2: High-resolution long video generation for autonomous driv- ing with adaptive control,” arXiv 2411.13807, 2024. [366] X. Hu, W. Yin, M. Jia, J. Deng, X. Guo, Q. Zhang, X. Long, and P. Tan, “DrivingWorld: Constructing world model for au- tonomous driving via video GPT,” arXiv 2412.19505, 2024. [367] J. Ni, Y. Guo, Y. Liu, R. Chen, L. Lu, and Z. Wu, “MaskGWM: A generalizable driving world model with video mask reconstruc- tion,” in CVPR, 2025. [368] J. Xiao, K. A. Ehinger, A. Oliva, and A. Torralba, “Recogniz- ing scene viewpoint using panoramic place representation,” in CVPR, 2012. [369] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor seg- mentation and support inference from RGBD images,” in ECCV, 2012. [370] S. Song, S. P. Lichtenberg, and J. Xiao, “SUN RGB-D: A RGB-D scene understanding benchmark suite,” in CVPR, 2015. [371] B. Hua, Q. Pham, D. T. Nguyen, M. Tran, L. Yu, and S. Yeung, “SceneNN: A scene meshes dataset with annotations,” in 3DV, 2016. [372] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, “Joint 2D-3D- Semantic data for indoor scene understanding,” arXiv 1702.01105, 2017. [373] M. Gardner, K. Sunkavalli, E. Yumer, X. Shen, E. Gambaretto, C. Gagn´e, and J. Lalonde, “Learning to predict indoor illumina- tion from a single image,” ACM TOG, vol. 36, no. 6, pp. 176:1– 176:14, 2017. [374] A. X. Chang, A. Dai, T. A. Funkhouser, M. Halber, M. Nießner, M. Savva, S. Song, A. Zeng, and Y. Zhang, “Matterport3D: Learning from RGB-D data in indoor environments,” in 3DV, 2017. [375] A. Dai, A. X. Chang, M. Savva, M. Halber, T. A. Funkhouser, and M. Nießner, “ScanNet: Richly-annotated 3D reconstructions of indoor scenes,” in CVPR, 2017. [376] T. Zhou, R. Tucker, J. Flynn, G. Fyffe, and N. Snavely, “Stereo magnification: learning view synthesis using multiplane images,” ACM TOG, vol. 37, no. 4, p. 65, 2018. [377] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Y. Ren, S. Verma, A. Clarkson, M. Yan, B. Budge, Y. Yan, X. Pan, J. Yon, Y. Zou, K. Leon, N. Carter, J. Briales, T. Gillingham, E. Mueggler, L. Pesqueira, M. Savva, D. Batra, H. M. Strasdat, R. De Nardi, M. Goesele, S. Lovegrove, and R. A. Newcombe, “The replica dataset: A digital replica of indoor spaces,” arXiv 1906.05797, 2019. [378] J. Wald, H. Dhamo, N. Navab, and F. Tombari, “Learning 3D se- mantic scene graphs from 3D indoor reconstructions,” in CVPR, 2020. [379] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. M. Turner, E. Undersander, W. Galuba, A. West- bury, A. X. Chang, M. Savva, Y. Zhao, and D. Batra, “Habitat- Matterport 3D dataset (HM3D): 1000 large-scale 3D environ- ments for embodied AI,” in NeurIPS, 2021. [380] C. Yeshwanth, Y. Liu, M. Nießner, and A. Dai, “ScanNet++: A high-fidelity dataset of 3D indoor scenes,” in ICCV, 2023. [381] L. Ling, Y. Sheng, Z. Tu, W. Zhao, C. Xin, K. Wan, L. Yu, Q. Guo, Z. Yu, Y. Lu, X. Li, X. Sun, R. Ashok, A. Mukherjee, H. Kang, X. Kong, G. Hua, T. Zhang, B. Benes, and A. Bera, “DL3DV-10K: A large-scale scene dataset for deep learning-based 3D vision,” in CVPR, 2024. [382] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. A. Funkhouser, “Semantic scene completion from a single depth image,” in CVPR, 2017. [383] J. Zheng, J. Zhang, J. Li, R. Tang, S. Gao, and Z. Zhou, “Struc- tured3D: A large photo-realistic dataset for structured 3D model- ing,” in ECCV, 2020. [384] M. Roberts, J. Ramapuram, A. Ranjan, A. Kumar, M. ´A. Bautista, N. Paczan, R. Webb, and J. M. Susskind, “Hypersim: A photore- alistic synthetic dataset for holistic indoor scene understanding,” in ICCV, 2021. [385] H. Fu, B. Cai, L. Gao, L. Zhang, J. Wang, C. Li, Q. Zeng, C. Sun, R. Jia, B. Zhao, and H. Zhang, “3D-FRONT: 3D furnished rooms with layouts and semantics,” in ICCV, 2021. [386] Y. Hold-Geoffroy, A. Athawale, and J. Lalonde, “Deep sky model- ing for single image outdoor lighting estimation,” in CVPR, 2019. [387] I. Skorokhodov, G. Sotnikov, and M. Elhoseiny, “Aligning latent and image spaces to connect the unconnectable,” in ICCV, 2021. [388] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for au- tonomous driving? the KITTI vision benchmark suite,” in CVPR, 2012. [389] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The Cityscapes dataset for semantic urban scene understanding,” in CVPR, 2016. [390] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stach- niss, and J. Gall, “SemanticKITTI: A dataset for semantic scene understanding of LiDAR sequences,” in ICCV, 2019. [391] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam, H. Zhao, A. Timofeev, S. Ettinger, M. Krivokon, A. Gao, A. Joshi, Y. Zhang, J. Shlens, Z. Chen, and D. Anguelov, “Scalabil- ity in perception for autonomous driving: Waymo open dataset,” in CVPR, 2020. [392] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuScenes: A multimodal dataset for autonomous driving,” in CVPR, 2020. [393] Y. Zhou, J. Huang, X. Dai, L. Luo, Z. Chen, and Y. Ma, “Holicity: A city-scale data platform for learning holistic 3D structures,” arXiv 2008.03286, 2020. [394] W. Li, Y. Lai, L. Xu, Y. Xiangli, J. Yu, C. He, G. Xia, and D. Lin, “OmniCity: Omnipotent city understanding with multi-level and multi-view images,” in CVPR, 2023. [395] Y. Liao, J. Xie, and A. Geiger, “KITTI-360: A novel dataset and benchmarks for urban scene understanding in 2D and 3D,” IEEE TPAMI, vol. 45, no. 3, pp. 3292–3310, 2023. [396] Y. Cabon, N. Murray, and M. Humenberger, “Virtual KITTI 2,” arXiv 2001.10773, 2020. [397] J. Wilson, J. Song, Y. Fu, A. Zhang, A. Capodieci, P. Jayakumar, K. Barton, and M. Ghaffari, “MotionSC: Data set and network for real-time semantic mapping in dynamic environments,” IEEE Robotics Autom. Lett., vol. 7, no. 3, pp. 8439–8446, 2022. [398] J. Wald, A. Avetisyan, N. Navab, F. Tombari, and M. Nießner, “RIO: 3D object instance re-localization in changing indoor envi- ronments,” in ICCV, 2019. [399] H. Fu, R. Jia, L. Gao, M. Gong, B. Zhao, S. J. Maybank, and D. Tao, “3D-FUTURE: 3D furniture shape with texture,” IJCV, 2021. 25 [400] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, “Gans trained by a two time-scale update rule converge to a local nash equilibrium,” in NIPS, 2017. [401] M. Binkowski, D. J. Sutherland, M. Arbel, and A. Gretton, “De- mystifying MMD gans,” in ICLR, 2018. [402] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, “Improved techniques for training gans,” in NIPS, 2016. [403] S. Morozov, A. Voynov, and A. Babenko, “On self-supervised image representations for GAN evaluation,” in ICLR, 2021. [404] G. Stein, J. C. Cresswell, R. Hosseinzadeh, Y. Sui, B. L. Ross, V. Villecroze, Z. Liu, A. L. Caterini, J. E. T. Taylor, and G. Loaiza- Ganem, “Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models,” in NeurIPS, 2023. [405] T. Kynk¨a¨anniemi, T. Karras, M. Aittala, T. Aila, and J. Lehtinen, “The role of imagenet classes in fr´echet inception distance,” in ICLR, 2023. [406] A. Mittal, A. K. Moorthy, and A. C. Bovik, “No-reference image quality assessment in the spatial domain,” IEEE TIP, vol. 21, no. 12, pp. 4695–4708, 2012. [407] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a ”com- pletely blind” image quality analyzer,” IEEE Signal Process. Lett., vol. 20, no. 3, pp. 209–212, 2013. [408] J. Wang, K. C. K. Chan, and C. C. Loy, “Exploring CLIP for assessing the look and feel of images,” in AAAI, 2023. [409] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. J. Guibas, “Learn- ing representations and generative models for 3D point clouds,” in ICML, 2018. [410] D. Lopez-Paz and M. Oquab, “Revisiting classifier two-sample tests,” in ICLR, 2017. [411] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a single image using a multi-scale deep network,” in NIPS, 2014. [412] W. Lai, J. Huang, O. Wang, E. Shechtman, E. Yumer, and M. Yang, “Learning blind video temporal consistency,” in ECCV, 2018. [413] T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly, “FVD: A new metric for video generation,” in ICLR, 2019. [414] J. Liu, Y. Qu, Q. Yan, X. Zeng, L. Wang, and R. Liao, “Fr´echet video motion distance: A metric for evaluating motion consis- tency in videos,” arXiv 2407.16124, 2024. [415] J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y. Choi, “CLIP- Score: A reference-free evaluation metric for image captioning,” in EMNLP, 2021. [416] H. Wu, Z. Zhang, W. Zhang, C. Chen, L. Liao, C. Li, Y. Gao, A. Wang, E. Zhang, W. Sun, Q. Yan, X. Min, G. Zhai, and W. Lin, “Q-Align: Teaching LMMs for visual scoring via discrete text- defined levels,” in ICML, 2024. [417] X. He, D. Jiang, G. Zhang, M. Ku, A. Soni, S. Siu, H. Chen, A. Chandra, Z. Jiang, A. Arulraj, K. Wang, Q. D. Do, Y. Ni, B. Lyu, Y. Narsupalli, R. Fan, Z. Lyu, Y. Lin, and W. Chen, “VideoScore: Building automatic metrics to simulate fine-grained human feedback for video generation,” arXiv 2406.15252, 2024. [418] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit, Y. Wang, X. Chen, L. Wang, D. Lin, Y. Qiao, and Z. Liu, “VBench: Comprehensive benchmark suite for video generative models,” in CVPR, 2024. [419] Z. Huang, F. Zhang, X. Xu, Y. He, J. Yu, Z. Dong, Q. Ma, N. Chanpaisit, C. Si, Y. Jiang, Y. Wang, X. Chen, Y. Chen, L. Wang, D. Lin, Y. Qiao, and Z. Liu, “VBench++: Comprehensive and versatile benchmark suite for video generative models,” arXiv 2411.13503, 2024. [420] D. Zheng, Z. Huang, H. Liu, K. Zou, Y. He, F. Zhang, Y. Zhang, J. He, W.-S. Zheng, Y. Qiao, and Z. Liu, “VBench-2.0: Advanc- ing video generation benchmark suite for intrinsic faithfulness,” arXiv 2503.21755, 2025. [421] H. Duan, H.-X. Yu, S. Chen, L. Fei-Fei, and J. Wu, “WorldScore: A unified evaluation benchmark for world generation,” arXiv 2504.00983, 2025. [422] L. H¨ollein, J. Johnson, and M. Nießner, “StyleMesh: Style transfer for indoor 3D scene reconstructions,” in CVPR, 2022. [423] L. Song, L. Cao, H. Xu, K. Kang, F. Tang, J. Yuan, and Z. Yang, “RoomDreamer: Text-driven 3D indoor scene synthesis with coherent geometry and texture,” in ACM MM, 2023. [424] Y. Chen, H. Huang, T. Vu, K. Shum, and S. Yeung, “StyleCity: Large-scale 3D urban scenes stylization,” in ECCV, 2024. [425] D. Z. Chen, H. Li, H. Lee, S. Tulyakov, and M. Nießner, “SceneTex: High-quality texture synthesis for indoor scenes via diffusion priors,” in CVPR, 2024. [426] Z. Huang, W. Yu, X. Cheng, C. Zhao, Y. Ge, M. Guo, L. Yuan, and Y. Tian, “Roompainter: View-integrated diffusion for consistent indoor scene texturing,” in CVPR, 2025. [427] I. Hwang, H. Kim, and Y. M. Kim, “Text2Scene: Text-driven indoor scene stylization with part-aware details,” in CVPR, 2023. [428] Q. Wang, R. Lu, X. Xu, J. Wang, M. Y. Wang, B. Dai, G. Zeng, and D. Xu, “RoomTex: Texturing compositional indoor scenes via iterative inpainting,” in ECCV, 2024. [429] B. Yang, W. Dong, L. Ma, W. Hu, X. Liu, Z. Cui, and Y. Ma, “DreamSpace: Dreaming your room space with text-driven panoramic texture propagation,” in VR, 2024. [430] M. Yang, J. Guo, Y. Chen, L. Chen, P. Li, Z. Cheng, X. Zhang, and H. Huang, “InstanceTex: Instance-level controllable texture synthesis for 3D scenes via diffusion priors,” in SIGGRAPH Asia, 2024. [431] Q. A. Wei, S. Ding, J. J. Park, R. Sajnani, A. Poulenard, S. Sridhar, and L. J. Guibas, “LEGO-Net: Learning regular rearrangements of objects in rooms,” in CVPR, 2023. [432] A. Murali, A. Mousavian, C. Eppner, A. Fishman, and D. Fox, “CabiNet: Scaling neural collision detection for object rearrange- ment with procedural scene generation,” in ICRA, 2023. [433] S. Zhang, J. Huang, L. Yue, J. Zhang, J. Liu, Y. Lai, and S. Zhang, “SceneExpander: Real-time scene synthesis for interactive floor plan editing,” in ACM MM, 2024. [434] S. Zhang, H. Tam, Y. Li, K. Ren, H. Fu, and S. Zhang, “SceneDirec- tor: Interactive scene synthesis by simultaneously editing multi- ple objects in real-time,” IEEE TVCG, vol. 30, no. 8, pp. 4558–4569, 2024. [435] Z. Wu, Y. Rubanova, R. Kabra, D. A. Hudson, I. Gilitschenski, Y. Aytar, S. van Steenkiste, K. R. Allen, and T. Kipf, “Neural As- sets: 3D-aware multi-object scene synthesis with image diffusion models,” in NeurIPS, 2024. [436] L. Li, Q. Lian, L. Wang, N. Ma, and Y. Chen, “Lift3D: Synthesize 3D training data by lifting 2D GAN to 3D generative radiance field,” in CVPR, 2023. [437] S. Huang, Z. Wang, P. Li, B. Jia, T. Liu, Y. Zhu, W. Liang, and S. Zhu, “Diffusion-based generation, optimization, and planning in 3D scenes,” in CVPR, 2023. [438] M. Hassan, Y. Guo, T. Wang, M. J. Black, S. Fidler, and X. B. Peng, “Synthesizing physical character-scene interactions,” in SIGGRAPH, 2023. [439] L. Pan, Z. Yang, Z. Dou, W. Wang, B. Huang, B. Dai, T. Komura, and J. Wang, “Tokenhsi: Unified synthesis of physical human- scene interactions through task tokenization,” in CVPR, 2025. [440] J. Wang, Y. Rong, J. Liu, S. Yan, D. Lin, and B. Dai, “Towards diverse and natural scene-aware 3D human motion synthesis,” in CVPR, 2022. [441] K. Zhao, S. Wang, Y. Zhang, T. Beeler, and S. Tang, “Composi- tional human-scene interaction synthesis with semantic control,” in ECCV, 2022. [442] F. Hong, V. Guzov, H. J. Kim, Y. Ye, R. A. Newcombe, Z. Liu, and L. Ma, “EgoLM: multi-modal language model of egocentric motions,” arXiv 2409.18127, 2024. [443] M. Hassan, V. Choutas, D. Tzionas, and M. J. Black, “Resolving 3D human pose ambiguities with 3D scene constraints,” in ICCV, 2019. [444] Z. Wang, Y. Chen, T. Liu, Y. Zhu, W. Liang, and S. Huang, “HUMANISE: language-conditioned human motion generation in 3D scenes,” in NeurIPS, 2022. [445] L. Ma, Y. Ye, F. Hong, V. Guzov, Y. Jiang, R. Postyeni, L. Pesqueira, A. Gamino, V. Baiyya, H. J. Kim, K. Bailey, D. S. Fosas, C. K. Liu, Z. Liu, J. Engel, R. De Nardi, and R. A. Newcombe, “Nymeria: A massive collection of multimodal egocentric daily motion in the wild,” in ECCV, 2024. [446] K. Zhao, Y. Zhang, S. Wang, T. Beeler, and S. Tang, “Synthesizing diverse human motions in 3D indoor scenes,” in ICCV, 2023. [447] L. Pan, J. Wang, B. Huang, J. Zhang, H. Wang, X. Tang, and Y. Wang, “Synthesizing physically plausible human motions in 3D scenes,” in 3DV, 2024. [448] W. Wang, L. Pan, Z. Dou, J. Mei, Z. Liao, Y. Lou, Y. Wu, L. Yang, J. Wang, and T. Komura, “SIMS: Simulating stylized human- scene interactions with retrieval-augmented script generation,” arXiv 2411.19921, 2025. 26 [449] L. Li and A. Dai, “GenZI: Zero-shot 3D human-scene interaction generation,” in CVPR, 2024. [450] A. Szot, A. Clegg, E. Undersander, E. Wijmans, Y. Zhao, J. M. Turner, N. Maestre, M. Mukadam, D. S. Chaplot, O. Maksymets, A. Gokaslan, V. Vondrus, S. Dharur, F. Meier, W. Galuba, A. X. Chang, Z. Kira, V. Koltun, J. Malik, M. Savva, and D. Batra, “Habitat 2.0: Training home assistants to rearrange their habitat,” in NeurIPS, 2021. [451] E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi, “AI2-THOR: an interactive 3D environment for visual AI,” arXiv 1712.05474, 2017. [452] C. Li, R. Zhang, J. Wong, C. Gokmen, S. Srivastava, R. Mart´ın- Mart´ın, C. Wang, G. Levine, M. Lingelbach, J. Sun, M. Anvari, M. Hwang, M. Sharma, A. Aydin, D. Bansal, S. Hunter, K. Kim, A. Lou, C. R. Matthews, I. Villa-Renteria, J. H. Tang, C. Tang, F. Xia, S. Savarese, H. Gweon, C. K. Liu, J. Wu, and L. Fei- Fei, “BEHAVIOR-1K: A benchmark for embodied AI with 1, 000 everyday activities and realistic simulation,” in CoRL, 2022. [453] P. Ren, M. Li, Z. Luo, X. Song, Z. Chen, W. Liufu, Y. Yang, H. Zheng, R. Xu, Z. Huang, T. Ding, L. Xie, K. Zhang, C. Fu, Y. Liu, L. Lin, F. Zheng, and X. Liang, “InfiniteWorld: A unified scalable simulation framework for general visual-language robot interaction,” arXiv 2412.05789, 2024. [454] Y. Wang, X. Qiu, J. Liu, Z. Chen, J. Cai, Y. Wang, T. J. Wang, Z. Xian, and C. Gan, “Architect: Generating vivid and interactive 3D scenes with hierarchical 2D inpainting,” in NeurIPS, 2024. [455] W. Wu, H. He, C. Zhang, J. He, S. Z. Zhao, R. Gong, Q. Li, and B. Zhou, “Towards autonomous micromobility through scalable urban simulation,” in CVPR, 2025. [456] C. Gao, B. Zhao, W. Zhang, J. Mao, J. Zhang, Z. Zheng, F. Man, J. Fang, Z. Zhou, J. Cui, X. Chen, and Y. Li, “EmbodiedCity: A benchmark platform for embodied agent in real-world city environment,” arXiv 2410.09604, 2024. [457] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mot- taghi, L. Zettlemoyer, and D. Fox, “ALFRED: A benchmark for interpreting grounded instructions for everyday tasks,” in CVPR, 2020. [458] O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, “CALVIN: A benchmark for language-conditioned policy learning for long- horizon robot manipulation tasks,” RA-L, vol. 7, no. 3, pp. 7327– 7334, 2022. [459] B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone, “LIBERO: benchmarking knowledge transfer for lifelong robot learning,” in NeurIPS, 2023. [460] R. Gong, J. Huang, Y. Zhao, H. Geng, X. Gao, Q. Wu, W. Ai, Z. Zhou, D. Terzopoulos, S. Zhu, B. Jia, and S. Huang, “ARNOLD: A benchmark for language-grounded task learning with contin- uous states in realistic 3D scenes,” in ICCV, 2023. [461] X. Li, K. Hsu, J. Gu, O. Mees, K. Pertsch, H. R. Walke, C. Fu, I. Lunawat, I. Sieh, S. Kirmani, S. Levine, J. Wu, C. Finn, H. Su, Q. Vuong, and T. Xiao, “Evaluating real-world robot manipula- tion policies in simulation,” in CoRL, 2024. [462] S. Nasiriany, A. Maddukuri, L. Zhang, A. Parikh, A. Lo, A. Joshi, A. Mandlekar, and Y. Zhu, “RoboCasa: Large-scale simulation of everyday tasks for generalist robots,” arXiv 2406.02523, 2024. [463] Y. Wang, Z. Xian, F. Chen, T. Wang, Y. Wang, K. Fragkiadaki, Z. Erickson, D. Held, and C. Gan, “RoboGen: Towards unleashing infinite data for automated robot learning via generative simula- tion,” in ICML, 2024. [464] H. Geng, F. Wang, S. Wei, Y. Li, B. Wang, B. An, C. T. Cheng, H. Lou, P. Li, Y.-J. Wang, Y. Liang, D. Goetting, C. Xu, H. Chen, Y. Qian, Y. Geng, J. Mao, W. Wan, M. Zhang, J. Lyu, S. Zhao, J. Zhang, J. Zhang, C. Zhao, H. Lu, Y. Ding, R. Gong, Y. Wang, Y. Kuang, R. Wu, B. Jia, C. Sferrazza, H. Dong, S. Huang, Y. Wang, J. Malik, and P. Abbeel, “RoboVerse: Towards a unified platform, dataset and benchmark for scalable and generalizable robot learning,” arXiv 2504.18904, 2025. [465] W. Liang, S. Wang, H. Wang, O. Bastani, D. Jayaraman, and Y. J. Ma, “Eurekaverse: Environment curriculum generation via large language models,” arXiv 2411.01775, 2024. [466] Y. Du, S. Yang, B. Dai, H. Dai, O. Nachum, J. Tenenbaum, D. Schuurmans, and P. Abbeel, “Learning universal policies via text-guided video generation,” in NeurIPS, 2023. [467] A. Ajay, S. Han, Y. Du, S. Li, A. Gupta, T. S. Jaakkola, J. B. Tenenbaum, L. P. Kaelbling, A. Srivastava, and P. Agrawal, “Compositional foundation models for hierarchical planning,” in NeurIPS, 2023. [468] J. Cen, C. Wu, X. Liu, S. Yin, Y. Pei, J. Yang, Q. Chen, N. Duan, and J. Zhang, “Using left and right brains together: Towards vision and language planning,” in ICML, 2024. [469] Q. Bu, J. Zeng, L. Chen, Y. Yang, G. Zhou, J. Yan, P. Luo, H. Cui, Y. Ma, and H. Li, “Closed-loop visuomotor control with generative expectation for robotic manipulation,” in NeurIPS, 2024. [470] H. Wu, Y. Jing, C. Cheang, G. Chen, J. Xu, X. Li, M. Liu, H. Li, and T. Kong, “Unleashing large-scale video generative pre-training for visual robot manipulation,” in ICLR, 2024. [471] C. Cheang, G. Chen, Y. Jing, T. Kong, H. Li, Y. Li, Y. Liu, H. Wu, J. Xu, Y. Yang, H. Zhang, and M. Zhu, “GR-2: A generative video- language-action model with web-scale knowledge for robot ma- nipulation,” arXiv 2410.06158, 2024. [472] Y. Hong, B. Liu, M. Wu, Y. Zhai, K. Chang, L. Li, K. Lin, C. Lin, J. Wang, Z. Yang, Y. Wu, and L. Wang, “SlowFast-VGen: Slow-fast learning for action-driven long video generation,” in ICLR, 2025. [473] Y. Hu, Y. Guo, P. Wang, X. Chen, Y. Wang, J. Zhang, K. Sreenath, C. Lu, and J. Chen, “Video Prediction Policy: A generalist robot policy with predictive visual representations,” in ICML, 2025. [474] Z. Ren, Y. Wei, X. Guo, Y. Zhao, B. Kang, J. Feng, and X. Jin, “Vide- oWorld: Exploring knowledge learning from unlabeled videos,” arXiv 2501.09781, 2025. [475] H. A. Alhaija, J. Alvarez, M. Bala, T. Cai, T. Cao, L. Cha, J. Chen, M. Chen, F. Ferroni, S. Fidler, D. Fox, Y. Ge, J. Gu, A. Hassani, M. Isaev, P. Jannaty, S. Lan, T. Lasser, H. Ling, M.-Y. Liu, X. Liu, Y. Lu, A. Luo, Q. Ma, H. Mao, F. Ramos, X. Ren, T. Shen, S. Tang, T.-C. Wang, J. Wu, J. Xu, S. Xu, K. Xie, Y. Ye, X. Yang, X. Zeng, and Y. Zeng, “Cosmos-Transfer1: Conditional world generation with adaptive multimodal control,” arXiv 2503.14492, 2025. [476] H. Zhen, Q. Sun, H. Zhang, J. Li, S. Zhou, Y. Du, and C. Gan, “TesserAct: learning 4D embodied world models,” arXiv 2504.20995, 2025. [477] Y. Ze, G. Yan, Y. Wu, A. Macaluso, Y. Ge, J. Ye, N. Hansen, L. E. Li, and X. Wang, “GNFactor: Multi-task real robot learning with generalizable neural feature fields,” in CoRL, 2023. [478] S. Dasgupta, A. Gupta, S. Tuli, and R. Paul, “ActNeRF: Uncertainty-aware active learning of nerf-based object models for robot manipulators using visual and re-orientation actions,” in IROS, 2024. [479] G. Lu, S. Zhang, Z. Wang, C. Liu, J. Lu, and Y. Tang, “ManiGaus- sian: Dynamic gaussian splatting for multi-task robotic manipu- lation,” in ECCV, 2024. [480] W. Zheng, W. Chen, Y. Huang, B. Zhang, Y. Duan, and J. Lu, “Oc- cWorld: Learning a 3D occupancy world model for autonomous driving,” in ECCV, 2024. [481] Y. Yang, J. Mei, Y. Ma, S. Du, W. Chen, Y. Qian, Y. Feng, and Y. Liu, “Driving in the occupancy world: Vision-centric 4D occupancy forecasting and planning via world models for autonomous driving,” in AAAI, 2025. [482] J. Ma, X. Chen, J. Huang, J. Xu, Z. Luo, J. Xu, W. Gu, R. Ai, and H. Wang, “Cam4DOcc: Benchmark for camera-only 4D occu- pancy forecasting in autonomous driving applications,” in CVPR, 2024. [483] J. Wei, S. Yuan, P. Li, Q. Hu, Z. Gan, and W. Ding, “OccLLaMa: An occupancy-language-action generative world model for au- tonomous driving,” arXiv 2409.03272, 2024. [484] L. Wang, W. Zheng, Y. Ren, H. Jiang, Z. Cui, H. Yu, and J. Lu, “OccSora: 4D occupancy generation models as world simulators for autonomous driving,” arXiv 2405.20337, 2024. [485] J. Tang, Z. Li, Z. Hao, X. Liu, G. Zeng, M. Liu, and Q. Zhang, “EdgeRunner: Auto-regressive auto-encoder for artistic mesh generation,” in CVPR, 2024. [486] M. Wu, H. Dai, K. Yao, T. Tuytelaars, and J. Yu, “BG-Triangle: B´ezier gaussian triangle for 3D vectorization and rendering,” in CVPR, 2025. [487] M. Guo, B. Wang, K. He, and W. Matusik, “TetSphere Splatting: Representing high-quality geometry with lagrangian volumetric meshes,” in ICLR, 2025. [488] S. Duggal, Y. Hu, O. Michel, A. Kembhavi, W. T. Freeman, N. A. Smith, R. Krishna, A. Torralba, A. Farhadi, and W.-C. Ma, “Eval3D: Interpretable and fine-grained evaluation for 3D generation,” in CVPR, 2025. [489] Y. Hu, L. Anderson, T. Li, Q. Sun, N. Carr, J. Ragan-Kelley, and F. Durand, “DiffTaichi: Differentiable programming for physical simulation,” in ICLR, 2020. 27 Beichen Wen received the B.Eng. degree in computer science and technology from Sun Yat- sen University, China, in 2024. He is currently a master student at Nanyang Technological Uni- versity, supervised by Prof. Ziwei Liu. His re- search interests include computer graphics and 3D vision. Haozhe Xie received his Ph.D. from the Harbin Institute of Technology, in 2021. He is currently a research fellow at MMLab@NTU, Nanyang Technological University, Singapore. Previously, he served as a senior research scientist at Ten- cent AI Lab from 2021 to 2023. His research interests include computer vision with a focus on 3D generation and reconstruction. He has published several papers in CVPR, ICCV, ECCV, ICLR, and IJCV, and serves as a reviewer for these journals and conferences. Zhaoxi Chen received the bachelor’s de- gree from Tsinghua University, in 2021. He is currently a Ph.D. student at MMLab@NTU, Nanyang Technological University, supervised by Prof. Ziwei Liu. He received the AISG PhD Fellowship in 2021. His research interests in- clude inverse rendering and 3D generative mod- els. He has published several papers in CVPR, ICCV, ECCV, ICLR, NeurIPS, TOG, and TPAMI. He also served as a reviewer for CVPR, ICCV, NeurIPS, TOG, and IJCV. Fangzhou Hong received Ph.D. degree from MMLab at Nanyang Technological University, su- pervised by Prof. Ziwei Liu. He received a B.Eng. degree in software engineering from Tsinghua University, China, in 2020. His research inter- ests include computer vision and deep learning. Particularly, he is interested in 3D representation learning. Ziwei Liu is currently an associate professor at Nanyang Technological University, Singapore. His research revolves around computer vision, machine learning, and computer graphics. He has published extensively on top-tier confer- ences and journals in relevant fields, includ- ing CVPR, ICCV, ECCV, NeurlPS, ICLR, ICML, TPAMI, TOG, and Nature Machine Intelligence. He is the recipient of the Microsoft Young Fellow- ship, Hong Kong PhD Fellowship, ICCV Young Researcher Award, HKSTP Best Paper Award and WAIC Yunfan Award. He serves as an Area Chair of CVPR, ICCV, NeurlPS, and ICLR, as well as an Associate Editor of IJCV. 