3D Gaussian Splatting Data Compression with Mixture of Priors Lei Liu The University of Hong Kong Hong Kong SAR, China liulei95@hku.hk Zhenghao Chen The University of Newcastle Newcastle, Australia zhenghao.chen@newcastle.edu.au Dong Xu The University of Hong Kong Hong Kong SAR, China dongxu@hku.hk Abstract 3D Gaussian Splatting (3DGS) data compression is crucial for en- abling efficient storage and transmission in 3D scene modeling. However, its development remains limited due to inadequate en- tropy models and suboptimal quantization strategies for both loss- less and lossy compression scenarios, where existing methods have yet to 1) fully leverage hyperprior information to construct robust conditional entropy models, and 2) apply fine-grained, element-wise quantization strategies for improved compression granularity. In this work, we propose a novel Mixture of Priors (MoP) strategy to simultaneously address these two challenges. Specifically, inspired by the Mixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior information through multiple lightweight MLPs to generate diverse prior features, which are subsequently integrated into the MoP feature via a gating mechanism. To en- hance lossless compression, the resulting MoP feature is utilized as a hyperprior to improve conditional entropy modeling. Meanwhile, for lossy compression, we employ the MoP feature as guidance information in an element-wise quantization procedure, leverag- ing a prior-guided Coarse-to-Fine Quantization (C2FQ) strategy with a predefined quantization step value. Specifically, we expand the quantization step value into a matrix and adaptively refine it from coarse to fine granularity, guided by the MoP feature, thereby obtaining a quantization step matrix that facilitates element-wise quantization. Extensive experiments demonstrate that our proposed 3DGS data compression framework achieves state-of-the-art per- formance across multiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and Tank&Temples. CCS Concepts • Computing methodologies →Computer vision; • Theory of computation →Data compression. Keywords 3D Gaussian Splatting, Data Compression, Mixture of Priors, Coarse- to-Fine Quantization 1 Introduction 3D Gaussian Splatting (3DGS) [24] employs an explicit 3D repre- sentation using learnable Gaussian Splatting. Due to its high train- ing efficiency and real-time rendering capabilities, it has rapidly emerged as a promising solution for high-quality novel view syn- thesis. Despite its efficiency, 3DGS relies on a large number of Gaussians and their associated attributes to preserve visual fidelity, leading to significant storage and deployment overhead. This has motivated the development of dedicated compression techniques tailored to the unique characteristics of 3DGS. Early approaches to compressing 3DGS primarily focus on reduc- ing the parameter count and quantization to achieve lossy compres- sion. These include clustering Gaussians into predefined codebooks via vector quantization [14, 28, 44, 45], or grouping them using anchor-based strategies [40]. However, these methods fall short in supporting lossless compression due to the absence of effective entropy coding techniques, and therefore cannot fully exploit the redundancy present in 3DGS representations. To overcome this lim- itation, recent studies [4, 51] have introduced entropy coding into 3DGS representations. Building upon anchor-based designs [40], Chen et al. proposed a Hash-grid Assisted Context (HAC) [4], while Wang et al. introduced an anchor-level context[51] to enhance entropy modeling and further improve compression performance. Despite recent progress in integrating such lossy-to-lossless com- pression strategies, current 3DGS compression methods still en- counter two fundamental limitations: 1) For lossy compression, most existing methods [4, 51] adopt a trivial quantization strategy that coarsely quantizes 3DGS data, while overlooking fine-grained (i.e., element-wise) quantization. This limits the ability to precisely control the bit-rate at the element level, thereby hindering optimal rate-distortion performance. 2) For lossless compression, most ex- isting methods [4, 51] adopt a conditional entropy model similar to those used in Neural Image Compression (NIC) [2, 10, 42], relying on a hyperprior to estimate the latent distribution. However, the design and expressiveness of the hyperprior remain limited. For instance, Chen et al. [4] directly adopted a shallow two-layer MLP to generate the hyperprior, which often fails to capture the full contextual dependencies needed for effective entropy modeling. In this work, we propose a novel strategy, termed Mixture of Priors (MoP), to address both of the aforementioned limitations. Inspired by the recent success of the Mixture of Experts (MoE) para- digm [13, 15, 30, 47, 56, 58] in foundation models, our proposed Mix- ture of Priors (MoP) strategy leverages multiple lightweight MLPs to extract diverse prior features and ensemble them to enhance both lossless and lossy compression performance. Specifically, each MLP is initialized with distinct parameters to promote the learn- ing of diverse and specialized prior features, thereby improving the generalizability of the overall MoP representation. To ensure efficiency, all MLPs are designed to be lightweight, minimizing storage overhead. A learnable gating network dynamically assigns aggregation weights to each MLP output, enabling adaptive fusion of prior features into the final MoP feature. The resulting MoP feature will serves as the hyperprior for conditional entropy cod- ing, enabling more accurate distribution estimation and enhancing lossless compression performance. Moreover, we leverage the produced MoP feature as guidance to implement a Coarse-to-Fine Quantization (C2FQ) mechanism, enabling element-wise quantization and improving lossy compres- sion with more optimal rate-distortion performance. Specifically, arXiv:2505.03310v1  [cs.CV]  6 May 2025 , , we start with a predefined quantization step size, which is first re- fined into a quantization value and then adaptively expanded into a quantization vector under the guidance of the MoP feature. This vector is further scaled by the element-wise gradients of the 3DGS attributes to construct a quantization matrix, which is then used to perform element-wise quantization across all 3DGS attributes. This design enables fine-grained rate-storage adjustment at the element level. Notably, by leveraging the gradient information for this expansion rather than relying on auxiliary networks, we ef- fectively avoid the memory overhead introduced by extra network parameters. Extensive experimental results demonstrate that our proposed compression framework, with proposed MoP and C2FQ strate- gies, achieves state-of-the-art performance on various benchmark datasets, including Mip-NeRF360 [3], BungeeNeRF [54], DeepBlend- ing [19], and Tank&Temples [26]. The main contributions of this work are summarized below: • We propose a novel Mixture of Priors strategy for 3DGS data compression, which employs multiple lightweight MLPs to generate diverse prior features and ensemble them into a unified MoP feature. This feature is used for both entropy coding and guiding quantization, thereby improving the performance of both lossy and lossless compression. • Guided by the MoP features and element-wise gradients, we further propose a Coarse-to-Fine Quantization strategy that adaptively expands a predefined quantization step into an element-wise quantization matrix, enabling precise rate– storage adjustment for each individual element within the 3DGS attributes. • We conduct comprehensive experiments on the Mip-NeRF360, BungeeNeRF, DeepBlending, and Tanks&Temples bench- marks to demonstrate the effectiveness of our 3DGS com- pression framework. Equipped with the proposed MoP and C2FQ strategies, our method achieves state-of-the-art per- formance across these datasets. 2 Related Work 2.1 3DGS Data Compression 3DGS encodes 3D scenes using learnable geometric and appearance attributes represented as 3D Gaussian distributions. This approach delivers high-fidelity scene representation while supporting fast training and real-time rendering, contributing to its widespread adoption. However, the substantial number of Gaussians and their associated parameters introduces significant storage overhead, un- derscoring the need for effective 3DGS compression strategies. Early methods primarily focused on reducing model complexity by refining Gaussian parameters. For example, vector quantization techniques grouped parameters into pre-defined codebooks [5, 14, 28, 44, 45], while other approaches employed direct pruning to discard redundant components [14, 28]. More recent efforts [4, 40, 43, 51] have explored structural relationships to improve compres- sion efficiency. Scaffold-GS [40], for instance, introduces anchor- centered features to represent scene content compactly. However, the above-mentioned methods do not use the entropy coding strat- egy to improve compression efficiency. Based on Scaffold-GS, HAC [4] first explores the entropy coding in 3DGS compression by utilizing a hash-grid structure to model spatial coherence, whereas Con- textGS [51] incorporates anchor-level contextual information as hy- perprior information to achieve efficient entropy coding on 3DGS. Despite their impressive performance, existing compression methods still exhibit several limitations. First, employing a single network with limited parameters inadequately extracts the prior features, leading to inefficient prediction of data distributions. Sec- ond, current methods lack fine-grained exploration of quantization steps, restricting their overall flexibility. Therefore, we propose the MoP strategy and the C2FQ strategy to address the aforementioned limitations. Specifically, our MoP strategy employs multiple lightweight MLPs to extract different prior features for diversity improvement, enabling efficient pre- diction of data distributions and providing guidance for further quantization. Moreover, our C2FQ module leverages MoP features and element-wise gradients to achieve both large-scale and fine- grained adjustment of quantization steps, significantly enhancing entropy coding performance. 2.2 Mixture of Experts The MoE framework [22, 47] has been widely adopted due to its capability to extract diverse and rich data features from multiple expert networks [13, 15, 30, 56, 58]. However, directly applying tra- ditional MoE models to 3DGS data compression still poses several limitations. Primarily, the large number of parameters typically used by MoE experts introduces prohibitive storage overhead for 3DGS compression tasks. To address this, we introduce multiple lightweight MLPs to extract rich features. This lightweight design significantly reduces the storage consumption associated with net- work parameters. Additionally, given the limited feature extrac- tion capability of each individual lightweight MLP, we employ a soft-gating mechanism to aggregate the outputs from all expert networks, thereby ensuring efficient utilization of each MLP. 2.3 Quantization in Compression In traditional image and video compression algorithms (e.g., H.266 [1], and H.265 [48]), quantization steps are typically adjusted based on the characteristics of the coding elements, enabling more effi- cient quantization and improved compression performance. Similar ideas have increasingly been adopted in many compression meth- ods [6–9, 11, 16, 18, 20, 27, 29, 34–37, 41, 49, 53], demonstrating the effectiveness of adaptive quantization in modern compression frameworks. Although some 3DGS compression methods, such as HAC [4], attempt to adjust quantization steps, their adjustment range re- mains limited, and they fail to support element-level quantization. This restricts the overall performance of the compression models. To address this limitation, we propose a Coarse-to-Fine Quanti- zation method. Leveraging MoP features, our approach enables a quantization vector. Furthermore, by accumulating the gradient of each element across multiple camera views and using it as weights, we refine the quantization vector to the quantization matrix. This improves the ability to precisely adjust the bit-rate at the element level, thereby enhancing the compression performance. 3D Gaussian Splatting Data Compression with Mixture of Priors , , Coarse-to-Fine  Quantization Arithmetic  Encoder  Quantization Lossless Encoder  Arithmetic Decoder  Compressed Location Compressed Attributes Location Attributes Lossless Decoder  Predefined Parameters MoP Network  Entropy Model Anchor Compressed Anchor Rendering Images bit-stream bit-stream Location Compression Attributes Compression MoP Features Distribution Interpolation MLPs 1  MLPs 2  MLPs n  Gating Weight 1 Weight 2 Weight n Prior  Feature 1 Prior  Feature 2 Prior  Feature n ... ... ... Compressed Location (a) Overview of 3DGS Data Compression Framework (b) Mixture of Priors Network ... MoP Features Figure 1: (a) The overview of our 3DGS compression framework, which integrates the proposed Mixture-of-Priors (MoP) network, the Coarse-to-Fine Quantization (C2FQ) module, and other standard 3DGS data compression components. (b) Details of the proposed MoP Network. It begins by applying a standard interpolation operation as in [4] to extract features from the compressed location. These features are then used by a gating network and several lightweight MLPs to generate gating weights and diverse prior features. The prior features are subsequently aggregated into a unified MoP feature through a weighted summation based on the corresponding gating weight. 3 Methodology 3.1 Preliminaries 3D Gaussian Splatting [24] represents a 3D scene as a collection of Gaussians. Each Gaussian is defined by a location 𝝁, a 3D covariance matrix Σ, an opacity term 𝛼and a view-dependent color 𝒄. Every Gaussian can be represented as:𝐺(𝒙) = 𝑒−1 2 (𝒙−𝝁)TΣ−1(𝒙−𝝁), where 𝒙denotes the coordinates of a 3D point. The covariance matrix Σ is further decomposed as Σ = 𝑹𝑺𝑺T𝑹T, with 𝑹and 𝑺corresponding to rotation and scaling components, respectively. When rendering 3DGS to images, these 3D Gaussians are splatted onto the given 2D plane, where pixel colors are determined by aggregating the splatted contributions using the opacity 𝛼and color 𝒄. Anchor is introduced by Scaffold-GS [40], which is a compact representation for 3DGS data. Specifically, each anchor is defined by a 3D location 𝒙𝑎∈R3 and a set of associated attributes A = 𝒇𝑎∈R𝐷𝑎, 𝒍∈R6, 𝒐∈R3𝐾, where 𝒇𝑎denotes the anchor’s local feature vector, 𝒍represents scaling factors, and 𝒐specifies positional offsets. During rendering, the feature 𝒇𝑎is passed through MLPs to predict the properties of the associated Gaussians. The positions of these Gaussians are computed by applying the offsets 𝒐to the anchor location 𝒙𝑎, while the scaling vector 𝒍modulates their spatial extent and shape. Quantization is a technique that maps continuous-valued sig- nals to a finite set of discrete levels, enabling lossy compression by reducing data precision. A predefined quantization step value determines the degree of data discretization, directly impacting storage cost. In this work, we adopt an element-wise quantization strategy that utilizes this step value to quantize each element. Entropy coding is a fundamental technique used to achieve lossless compression by efficiently encoding data based on its sta- tistical distribution. It relies on the probability distribution of the quantized item ˆ A for efficient encoding. Since the true distribution 𝑞( ˆ A) is generally inaccessible, it is commonly approximated by an estimated distribution 𝑝( ˆ A) [2, 21, 31–33, 39, 42]. According to Information Theory [12], the expected number of bits required to encode ˆ A using entropy coding is given by the cross-entropy, de- fined as 𝐻(𝑞, 𝑝) = E ˆ A∼𝑞[−log(𝑝( ˆ A))]. This cross-entropy serves as a lower bound on the achievable storage. Therefore, improving the accuracy of the approximation 𝑝( ˆ A) reduces 𝐻(𝑞, 𝑝) and leads to lower storage cost. In this work, we model the conditional distri- bution 𝑝( ˆ A|·) of the quantized attribute ˆ A and apply arithmetic coding as the cross-entropy coding algorithm. 3.2 The Overview of our 3DGS Data Compression Framework As a compact representation of 3DGS, the anchor has recently gained significant attention in 3DGS compression and has demon- strated impressive performance. Motivated by the advantages of anchors, we propose a novel anchor-based 3DGS compression framework, as illustrated in Figure 1 (a). An anchor primarily con- sists of two parts: location 𝒙𝑎and attributes A. Accordingly, our framework is designed with two separate branches—a location com- pression branch and an attributes compression branch—which are responsible for compressing the anchor’s location and attributes, , , Quantization  Matrix (n × k) MoP Features  FC  Gumbel Softmax/Max Scale List Adaptive Expansion Quantization  Step (1) Quantization Vector (n × 1) [0,0,1, ..., 0] Predefined  Parameters  Expansion  Element-wise Gradients in Multiple Camera Views (n × k × m) Quantized Attributes  (n × k) Element-wise Quantization Each Attribute (n × k) Averaging (n × k) Rescale Quantization Value (1) MLPs Value (1) Vector (n × 1) Matrix (n × k) Coarse-to-Fine Quantization Figure 2: Details of our Coarse-to-Fine Quantization mod- ule. This module first rescales the predefined quantization step into a quantization value using a scale, which is selected from the scale list utilizing a Gumbel-Softmax/Max strat- egy. Further leveraging the MoP features, the adaptive ex- pansion module expands this step value into a quantization vector. Subsequently, the quantization vector is extended into a quantization matrix by multiplying it with aggregated element-wise gradients from multiple camera views. Finally, each attribute is quantized in an element-wise manner using the quantization matrix. The notations (1), (𝑛× 1), (𝑛× 𝑘), and (𝑛×𝑘×𝑚) indicate the dimensions of the corresponding variables, where 𝑛is the number of anchors, 𝑘is the number of element in each anchor attribute, and 𝑚is the number of camera views. respectively. Specifically, the framework first compresses the an- chor locations to obtain compact location representations. These compressed locations are subsequently utilized to guide the com- pression of anchor attributes. Finally, the compressed location and attributes are combined to form the compressed anchors, which are passed to a rendering module to reconstruct the 3D Scene and ren- der the final images. The details of the entire network are described as follows: Location Compression. Following the standard practice in current 3DGS compression methods [4, 28, 46, 51], we construct a location compression pipeline accordingly. Specifically, we first quantize the anchor locations 𝒙𝑎from 32-bit precision to 16-bit. The quantized locations ˆ𝒙𝑎are then losslessly encoded into a bit- stream. Subsequently, the bit-stream is losslessly decoded to obtain the compressed locations ¯𝒙𝑎. MoP Network. The compressed location is further fed into our proposed MoP Network to generate MoP feature G, which serves both as guidance for quantization and as hyperprior information for enabling lossy and lossless compression. More details of the MoP network are provided in Section 3.3. Lossy Attribute Compression. We achieve lossy compression of the anchor attributes through quantization, guided by the MoP features generated by the MoP Network. Specifically, the Coarse- to-Fine Quantization procedure expands a predefined quantization step value into a quantization matrix, enabling element-wise quan- tization of each individual element within the anchor. More details are provided in Section 3.4. Lossless Attribute Compression. To perform lossless compres- sion of the quantized attribute ˆ A, we directly use the MoP feature G generated by the MoP Network as hyperprior information to predict the distribution 𝑝( ˆ A|G) with a conditional entropy model, following [4]. Based on such estimated distribution 𝑝( ˆ A|G), an arithmetic encoder losslessly encodes the quantized attributes ˆ A into a bit-stream. During the decoding phase, the bit-stream is loss- lessly decoded by an arithmetic decoder using the same distribution to reconstruct the quantized attributes ¯ A. Rendering. The compressed location ¯𝒙𝑎and attributes ¯ A to- gether form the compressed anchors. We follow the standard anchor- based rendering pipeline [40] to generate images from these an- chors. Specifically, the anchors are first reconstructed into 3DGS. Given the camera viewpoint, the 3DGS is then projected onto the 2D image plane, and pixel values are rendered using 𝛼-composited blending, as described in [4, 40, 51]. 3.3 Mixture of Priors The MoP Network is utilized to extract MoP features from the compressed locations for both lossy and lossless compression. The architecture of the MoP Network is illustrated in Figure 1 (b). The MoP network first interpolates the compressed location follow- ing the approach in [4], and the resulting interploated location information is then used to construct the MoP features. Different with previous 3DGS compression networks [4, 51] that employ a single MLP to exploit the hyperprior information Inspired by the MoE paradigms [13, 15, 30, 47, 56, 58], we design multiple lightweight MLPs to extract diverse prior features p1, p2, ..., pn from the interpolated location information. Meanwhile, a gating network produces a set of weights [𝑤1,𝑤2, ...,𝑤𝑛] corresponding to each prior feature. These weights are then applied to their respective prior features through element-wise multiplication, resulting in weighted prior features. Finally, all weighted prior features are summed to obtain the MoP feature as follows: G = 𝑛 ∑︁ 𝑖=1 𝑤𝑖× p𝑖 (1) where G represents the MoP features explored by our MoP Net- work, p𝑖denotes the output feature of the 𝑖-th MLPs, and 𝑤𝑖is the corresponding weight generated by the gating network. In this process, instead of using a single MLP to explore the priors as in previous 3DGS compression methods [4, 51], we design multiple MLPs to extract diverse prior features. To ensure feature diversity, we adopt different parameter initialization strategies for each MLP, encouraging them to learn distinct parameters during training and thus generate diverse prior features. While considering that network parameters must be stored with the compressed data and thus contribute to storage overhead, we adopt a lightweight design for the gating network and each MLP, avoiding the negative impact of excessive parameters on compression performance. 3D Gaussian Splatting Data Compression with Mixture of Priors , , Owing to the aforementioned advantages, the MoP features ex- tracted by our MoP network play a crucial role in both lossless and lossy compression. Specifically, for lossless compression, the diverse MoP features provide richer contextual information for the entropy model to predict more accurate data distributions, thereby improving compression efficiency. For lossy compression, the MoP features guide the quantization process, enabling precise adjust- ment of quantization steps, as described in Section 3.4. 3.4 Coarse-to-Fine Quantization Quantization is an essential component in lossy compression, as it directly affects the storage costs when encoding the data. To precisely adjust the storage costs of the data, element-wise quan- tization strategies have been explored in both traditional [1, 48] and deep learning-based [11, 16, 27, 29, 41, 49, 53] image and video compression. However, such techniques remain largely unexplored in 3DGS compression. To address this gap, we propose a Coarse-to- Fine Quantization strategy guided by our MoP features that adjusts the quantization step in an element-wise way. The details of our Coarse-to-Fine Quantization module are shown in Figure 2. In the initial adjustment stage, our goal is to obtain a coarse quantization step from a large scale, which serves as the basis for subsequent fine-gained quantization. To this end, a scale list is provided to the quantization module. Our strategy employs the MoP feature to compute the probability of each scale through one fully connected (FC) layer. The scale with the highest probabil- ity is then selected via a max operation. However, since the max operation is non-differentiable, it prevents end-to-end optimization of the entire network through backpropagation. To address this issue, we adopt the Gumbel-Softmax strategy [23] during train- ing to approximate the selection process. As the Gumbel-Softmax module is differentiable, it enables end-to-end optimization of the network. After the scale 𝑠selected by the Gubmel-Softmax/Max strategy, our C2FQ rescales the predefined quantization step 𝑄0 to the quantization value 𝑄1 as follows: 𝑄1 = 𝑄0 × 𝑠. Subsequently, the C2FQ strategy further leverages the MoP fea- ture G to adaptively expand the quantization value 𝑄1 into a quan- tization vector 𝑸2. This expansion is defined as 𝑸2 = 𝑄1 × (1 + Tanh(𝑓𝜙(G))), where 𝑓𝜙is the MLPs. To further expand the quantization vector 𝑸2 to element-wise, the most straightforward approach is to introduce a neural network that expands the 𝑸2 to a (𝑛×𝑘)-dimensional matrix, where 𝑛is the number of anchors and 𝑘is the number of elements in each anchor attribute. However, for large values of 𝑘, this introduces a signifi- cant number of network parameters, resulting in substantial storage overhead and negatively affecting compression performance. To avoid this issue, we exploit the relationship between an element’s gradient and its contribution to the loss: elements with larger gra- dients have a greater impact on the loss and, consequently, on the final compression performance, while those with smaller gradients contribute less. Based on this insight, we propose a network-free strategy to assign the element-wise quantization matrix 𝑸3 using gradient-based weighting. Specifically, we first expand the quanti- zation vector 𝑸2 of shape (𝑛× 1) to shape (𝑛× 𝑘) by duplicating it 𝑘times. Next, we compute the average gradient across all cam- era views for each element. These averaged gradients are then used as weights and multiplied element-wise with the expanded quantization steps to generate a quantization matrix 𝑸4 for each elements. Finally, the attribute values A are quantized using the computed fine-grained quantization steps 𝑸4, following ˆ A = Round(A × 𝑸4)/𝑸4. Through this process, we achieve element-level, fine-grained quantization of the attributes, leading to precisely adjusting the storage for each element in lossy compression. 3.5 Optimization The total loss function used to optimize the proposed 3DGS com- pression framework is formulated as follows: L = LRendering + 𝜆Lanchor, (2) where LRendering is the rendering loss defined in Scaffold-GS [40], and Lanchor denotes the estimated storage cost for anchor as in HAC [4] and Context-GS [51]. Specifically, Lanchor mainly reflects the estimated storage cost of anchor attributes derived from entropy coding. More details about Lanchor can be found in the supplemen- tary material. 𝜆is a hyperparameter that balances the different loss components. 4 Experiments 4.1 Datasets We performed extensive evaluations across multiple datasets, in- cluding the four large-scale real-world benchmarks: Mip-NeRF360 [3], BungeeNeRF [54], DeepBlending [19], and Tanks&Temples [26]. In our main paper, we present the average performance metrics aggregate over all scenes within each dataset, while the quantitative results of each scene are provided in the supplementary material for further reference. 4.2 Experiment Details Baseline methods. We compare our newly proposed 3DGS com- pression method against a range of existing 3DGS compression approaches. Several prior works [14, 28, 44, 45] focus on reduc- ing model size via parameter pruning or vector quantization with codebooks. Other approaches [4, 51] aim to enhance 3DGS data com- pression by incorporating entropy coding. Among them, HAC [4] leverages hash grids, while ContextGS [51] utilizes anchor-level context as a hyperprior to facilitate effective entropy coding of 3DGS data. Metrics. We evaluate compression performance in terms of stor- age size, measured in megabytes (MB). To assess the visual quality of rendered images generated from the compressed 3DGS data, we employ three standard metrics: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM)[52], and Learned Perceptual Im- age Patch Similarity (LPIPS)[57]. Implementation Details. Our method is implemented in Py- Torch with CUDA acceleration, and all experiments are conducted on the machine with Intel Xeon CPU and an NVIDIA RTX 3090 GPU equipped with 24GB of memory. The model is optimized us- ing the Adam optimizer [25] and trained for 60,000 iterations. To balance storage overhead and prior feature diversity, we empiri- cally set the number of MLP modules in our MoP strategy to five. When computing element-wise gradients for quantization, we first , , Table 1: A comparative analysis of the newly proposed method with other 3DGS data compression methods. Two distinct sets of results are reported for our method, reflecting varying trade-offs between size and fidelity. The best and second-best results are marked in red and yellow cells, respectively. Size measurements are provided in megabytes (MB). Datasets Mip-NeRF360 [3] BungeeNeRF [54] DeepBlending [19] Tank&Temples [26] Methods psnr↑ssim↑lpips↓size↓psnr↑ssim↑lpips↓size↓psnr↑ssim↑lpips↓size↓psnr↑ssim↑lpips↓size↓ 3DGS [24] 27.49 0.813 0.222 744.7 24.87 0.841 0.205 1616 29.42 0.899 0.247 663.9 23.69 0.844 0.178 431.0 Scaffold-GS [40] 27.50 0.806 0.252 253.9 26.62 0.865 0.241 183.0 30.21 0.906 0.254 66.00 23.96 0.853 0.177 86.50 EAGLES [17] 27.15 0.808 0.238 68.89 25.24 0.843 0.221 117.1 29.91 0.910 0.250 62.00 23.41 0.840 0.200 34.00 LightGaussian [14] 27.00 0.799 0.249 44.54 24.52 0.825 0.255 87.28 27.01 0.872 0.308 33.94 22.83 0.822 0.242 22.43 Compact3DGS [28] 27.08 0.798 0.247 48.80 23.36 0.788 0.251 82.60 29.79 0.901 0.258 43.21 23.32 0.831 0.201 39.43 Compressed3D [44] 26.98 0.801 0.238 28.80 24.13 0.802 0.245 55.79 29.38 0.898 0.253 25.30 23.32 0.832 0.194 17.28 Morgen. et al. [43] 26.01 0.772 0.259 23.90 22.43 0.708 0.339 48.25 28.92 0.891 0.276 8.40 22.78 0.817 0.211 13.05 Navaneet et al. [44] 27.16 0.808 0.228 50.30 24.63 0.823 0.239 104.3 29.75 0.903 0.247 42.77 23.47 0.840 0.188 27.97 Reduced3DGS [46] 27.19 0.807 0.230 29.54 24.57 0.812 0.228 65.39 29.63 0.902 0.249 18.00 23.57 0.840 0.188 14.00 RDOGaussian [50] 27.05 0.802 0.239 23.46 23.37 0.762 0.286 39.06 29.63 0.902 0.252 18.00 23.34 0.835 0.195 12.03 MesonGS [55] 26.99 0.796 0.247 27.16 23.06 0.771 0.235 63.11 29.51 0.901 0.251 24.76 23.32 0.837 0.193 16.99 CompGS [38] 27.26 0.803 0.239 16.50 - - - - 29.69 0.901 0.279 8.77 23.70 0.837 0.208 9.60 HAC [4] 27.77 0.811 0.230 21.87 27.08 0.872 0.209 29.72 30.34 0.906 0.258 6.35 24.40 0.853 0.177 11.24 Context-GS [51] 27.72 0.811 0.231 21.58 27.15 0.875 0.205 21.80 30.39 0.909 0.258 6.60 24.29 0.855 0.176 11.80 Ours (low-rate) 27.68 0.808 0.234 15.64 27.26 0.875 0.207 20.83 30.20 0.908 0.260 4.07 24.21 0.861 0.163 8.98 Ours (high-rate) 27.89 0.811 0.227 21.89 27.63 0.893 0.172 27.56 30.45 0.912 0.250 5.65 24.43 0.865 0.158 11.33 15 16 17 18 19 20 21 22 SIZE 27.6 27.7 27.8 27.9 PSNR Mip-NeRF360 Ours Context-GS HAC 22 24 26 28 30 SIZE 26.8 27.0 27.2 27.4 27.6 PSNR BungeeNeRF Ours Context-GS HAC 4.0 4.5 5.0 5.5 6.0 6.5 SIZE 30.0 30.1 30.2 30.3 30.4 PSNR DeepBlending Ours Context-GS HAC Figure 3: The Rate-Distortion (RD) curves on three benchmarks, including Mip-NeRF360, BungeeNeRF, and DeepBlending. compress the anchors under multiple camera views. At this stage, the element gradients fed into the Coarse-to-Fine Quantization module are initially set to 1, meaning that fine-grained quantiza- tion is not applied. Once the gradients are collected, we then feed the actual element-wise gradients into the quantization module to enable fine-grained quantization. 4.3 Experiment Results We compare our method with existing 3DGS compression approaches on four benchmark datasets: Mip-NeRF360 [3], BungeeNeRF [54], DeepBlending [19], and Tanks&Temples [26]. The experimental re- sults are presented in Table 1. Our method reduces storage by more than 97% compared to the original 3DGS [24], while achieving even higher rendering fidelity. Compared with Scaffold-GS [40], our ap- proach achieves over 88% storage savings and consistently delivers better reconstruction quality. These results demonstrate the effec- tiveness of our compression framework in significantly reducing the storage cost of 3DGS. Compared with other recent compres- sion approaches [4, 14, 17, 24, 28, 38, 40, 43, 44, 46, 50, 51, 55], our method consistently achieves the best performance. Furthermore, to more clearly compare with the recent state- of-the-art methods—HAC [4] and Context-GS [51]—we present performance comparisons across different storage sizes, as shown in Figure 3. To provide a more comprehensive evaluation, we also report the Bjøntegaard Delta Bit Rate (BDBR) results in Table 3. 3D Gaussian Splatting Data Compression with Mixture of Priors , , amsterdam pompidou room Figure 4: Visualization of “amsterdam”, “pompidou”, and “room” scenes from BungeeNeRF dataset, and the corresponding weights [𝑤1, 𝑤2, 𝑤3, 𝑤4, 𝑤5] for different prior features. Table 2: Ablation study on the DeepBlending dataset. (1) Ours: the full version of our proposed 3DGS compression frame- work. (2) Ours w/o C2FQ: our method without the Coarse-to- Fine Quantization strategy. (3) Ours w/o MoP: our method without the MoP strategy. (4) Ours w/o C2FQ & MoP: our method without both the Coarse-to-Fine Quantization strat- egy and the MoP strategy. PSNR↑ SSIM↑ Size (MB)↓ Ours 30.45 0.912 5.65 Ours w/o C2FQ 30.39 0.911 5.78 Ours w/o MoP 30.23 0.910 5.75 Ours w/o C2FQ & MoP 30.22 0.908 5.81 Table 3: BDBR (%) results for HAC, Context-GS, and our newly proposed 3DGS Compression method across different datasets. Positive BDBR values indicate additional storage costs compared to our method. Mip-NeRF360 DeepBlending Context-GS [51] 22.50 31.80 HAC [4] 16.55 41.82 These results show that, under the same PSNR setting, our method reduces the average storage cost by 31.80% and 41.82% compared to HAC and Context-GS, respectively, on the DeepBlending dataset. Overall, these experiments confirm that our method outperforms prior state-of-the-art approaches, demonstrating its effectiveness in 3DGS compression. 4.4 Ablation Study and Analysis Effectiveness of Different Components. As shown in Table 2, we take the DeepBlending [19] dataset as an example to evaluate the effectiveness of the key components in our proposed 3DGS compression framework. To assess the impact of the Coarse-to-Fine Quantization module, we remove it from our method, denoted as “Ours w/o C2FQ”. Compared to the full framework, “Ours w/o C2FQ” results in a drop of 0.06 PSNR and 0.001 SSIM, along with an increase of 0.13MB storage cost. To assess the impact of the MoP strategy, we remove it from our method, denoted as “Ours w/o MoP”. Compared to the full framework, “Ours w/o MoP” results in a drop of 0.22 PSNR and 0.002 SSIM, along with an increase of 0.10MB storage cost. Furthermore, we remove the MoP strategy from “Ours w/o C2FQ”, resulting in the variant “Ours w/o C2FQ & MoP”. This leads Table 4: Comparison of MLPs size and compression perfor- mance (i.e., PSNR, and total size) between HAC, Context- GS, and our “Ours w/o C2FQ” variant on the DeepBlending dataset. Here, “Ours w/o C2FQ” refers to our method without the Coarse-to-Fine Quantization module. Size (MB) of MLPs ↓ PSNR↑ Total Size (MB) ↓ Ours w/o C2FQ 0.378 30.39 5.78 HAC [4] 0.157 30.34 6.35 Context-GS [51] 0.316 30.39 6.60 to a further decrease of 0.05 PSNR and 0.005 SSIM, and increases the storage cost by an additional 0.57MB compared to “Ours w/o C2FQ”. These results clearly demonstrate the effectiveness of both the Coarse-to-Fine Quantization module and the MoP Network in improving compression performance. Analysis of our MoP strategy. Our MoP strategy employs lightweigth experts to extract diverse hyperprior features for distri- bution prediction, effectively balancing the trade-off between the storage cost of network parameters and compression performance. To isolate the contribution of the MoP strategy, we remove the Coarse-to-Fine quantization module from our complete framework and retain only the MoP network, denoted as “Ours w/o C2FQ”. We compare “Ours w/o C2FQ” with HAC [4] and Context-GS [51], both of which do not explicitly address the trade-off between model com- plexity and compression performance. The comparison is conducted on the DeepBlending [19] dataset, and the results are presented in Table 4. The results indicate that although the use of multiple lightweight MLPs introduces a slight increase in parameter size, the diverse hyperprior features extracted by them significantly improve the performance of the compression model (i.e., higher PSNR and lower overall storage consumption). Furthermore, we visualize the prior weights of different scenes from BungeeNeRF [54] dataset, as shown in Figure 4. The visual- ization results demonstrate that the gating network in our MoP network can adaptively adjust the weights for different prior fea- tures across scenes. This shows that the MoP strategy not only preserves the diversity of prior features but also emphasizes scene- relevant information, enabling the predicted distribution that is better suited to each specific scene. Consequently, the compression performance is further improved. Moreover, we visualize the out- puts of different MLPs to further demonstrate the diversity of the extracted prior features, as shown in the supplementary materials. , , Table 5: The storage cost of each component and the rendering qualities of HAC and our newly proposed compression framework on the “Garden” scene of the Mip-NeRF360 dataset. “Others” means additional storage costs, which are detailed in the supplementary materials. Methods Storage Costs (MB)↓ Fidelity Location Attributes MLPs Others Total PSNR↑ SSIM↑ LPIPS↓ Ours 3.10 20.25 0.38 0.73 24.45 27.51 0.851 0.135 HAC [4] 4.01 27.24 0.16 0.94 32.35 27.50 0.851 0.138 Table 6: Ablation study of different quantization stages on the Tank&Temples dataset. (1) Ours: the full version of our proposed 3DGS compression method. (2) Ours w/o QM: Our method removes the quantization matrix. (3) Ours w/o QM & QV: Our method removes both the quantization matrix and vector. PSNR ↑ Size (MB) ↓ Ours 24.37 10.35 Ours w/o QM 24.36 10.50 Ours w/o QM & QV 24.23 10.73 Analysis of our Coarse-to-Fine Quantization. We take the Tanks&Temples [26] dataset as an example to evaluate the effec- tiveness of different quantization stages in our C2FQ strategy, as shown in Table 6. To assess the impact of the quantization ma- trix, we remove it from the full framework, resulting in the variant “Ours w/o QM.” Compared to the full model, “Ours w/o QM” leads to a 0.01 dB drop in PSNR and a 0.15 MB increase in storage cost. Furthermore, we remove the quantization vector from “Ours w/o QM,” yielding the variant “Ours w/o QM & QV,” which causes an additional 0.13 dB PSNR drop and a further 0.23 MB increase in storage cost. These results validate the effectiveness of both the quantization matrix and vector in enhancing compression efficiency within the proposed C2FQ strategy. Analysis of the Storage Cost. To further demonstrate the effectiveness of our method, we present a detailed breakdown of the storage cost for each component, as shown in Table 5. The experimental results show that by incorporating both the MoP network and the Coarse-to-Fine Quantization module in the anchor attribute compression process, our method reduces the storage cost of anchor attributes by over 25% and the total 3DGS compression size by approximately 24% compared to HAC [4], while achieving comparable SSIM and even better PSNR and LPIPS scores. These results highlight the effectiveness of our method in compressing 3DGS data, particularly in optimizing attribute storage. Visualization. We visualize the “bilbao” scene from the BungeeN- eRF [54] dataset, as shown in Figure 5. Compared to HAC [4], the image rendered by our method exhibits more distinct structural de- tails and clearer textures. The quantitative metrics also demonstrate that our method achieves higher fidelity while consuming less stor- age. These visualization results further validate the effectiveness of our approach. bilbao from BungeeNeRF  dataset Ground Truth    Ours       Size:18.33  PSNR:28.09   SSIM:0.887    HAC        Size:18.82  PSNR:27.86   SSIM:0.876 Figure 5: Visualization comparison between HAC and our method on the “bilbao” scene from the BungeeNeRF dataset. PSNR, SSIM, and scene size (in MB) of the rendered images are reported. 5 Conclusion To enhance 3DGS data compression performance, we propose a MoP strategy, which leverages diverse priors generated by multi- ple lightweight MLPs and combines them using a learnable gating mechanism to produce a unified MoP feature. The resulting fea- ture improves both lossy and lossless compression by serving as enriched hyperprior information for conditional entropy coding and by guiding the Coarse-to-Fine Quantization (C2FQ) procedure to enable element-wise quantization. Comprehensive experiments demonstrate the effectiveness of our MoP module, achieving state- of-the-art performance in 3DGS compression. Beyond establishing a strong baseline for efficient 3DGS data compression, our work also inspires future research toward unified modules that jointly address both lossy and lossless compression for 3DGS data. Limitations and Future Works. Currently, the exploration of prior diversity is limited by the use of only a few lightweight MLPs. In future work, we plan to explore varying the number and architecture of the expert networks to generate richer and more diverse priors, enhancing the effectiveness of the MoP strategy. 3D Gaussian Splatting Data Compression with Mixture of Priors , , References [1] 2022. VVC test model (VTM). https://jvet.hhi.fraunhofer.de/, accessed:2024. [2] Johannes Ballé, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick John- ston. 2018. Variational image compression with a scale hyperprior. ICLR (2018). [3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. 2022. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In CVPR. 5470–5479. [4] Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, and Jianfei Cai. 2024. Hac: Hash-grid assisted context for 3d gaussian splatting compression. In ECCV. Springer, 422–438. [5] Zicong Chen, Zhenghao Chen, Wei Jiang, Wei Wang, Lei Liu, and Dong Xu. 2025. 4DGS-CC: A Contextual Coding Framework for 4D Gaussian Splatting Data Compression. arXiv preprint arXiv:2504.18925 (2025). [6] Zhenghao Chen, Shuhang Gu, Guo Lu, and Dong Xu. 2022. Exploiting intra- slice and inter-slice redundancy for learning-based lossless volumetric image compression. IEEE TIP 31 (2022), 1697–1707. [7] Zhenghao Chen, Guo Lu, Zhihao Hu, Shan Liu, Wei Jiang, and Dong Xu. 2022. LSVC: A Learning-Based Stereo Video Compression Framework. In CVPR. 6073– 6082. [8] Zhenghao Chen, Lucas Relic, Roberto Azevedo, Yang Zhang, Markus Gross, Dong Xu, Luping Zhou, and Christopher Schroers. 2023. Neural video compression with spatio-temporal cross-covariance transformers. In ACM MM. 8543–8551. [9] Zhenghao Chen, Luping Zhou, Zhihao Hu, and Dong Xu. 2024. Group-aware parameter-efficient updating for content-adaptive neural video compression. In ACM MM. 11022–11031. [10] Zhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro Katto. 2020. Learned image compression with discretized gaussian mixture likelihoods and attention modules. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 7939–7948. [11] Jinyoung Choi and Bohyung Han. 2020. Task-aware quantization network for jpeg image compression. In ECCV. Springer, 309–324. [12] Thomas M Cover. 1999. Elements of information theory. John Wiley & Sons. [13] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. Glam: Efficient scaling of language models with mixture-of-experts. PMLR, 5547–5569. [14] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, and Zhangyang Wang. 2023. Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps. arXiv preprint arXiv:2311.17245 (2023). [15] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research 23, 120 (2022), 1–39. [16] Haisheng Fu, Jie Liang, Zhenman Fang, Jingning Han, Feng Liang, and Guohe Zhang. 2024. WeConvene: Learned Image Compression with Wavelet-Domain Convolution and Entropy Model. In ECCV. Springer, 37–53. [17] Sharath Girish, Kamal Gupta, and Abhinav Shrivastava. 2023. Eagles: Effi- cient accelerated 3d gaussians with lightweight encodings. arXiv preprint arXiv:2312.04564 (2023). [18] Tao Han, Zhenghao Chen, Song Guo, Wanghan Xu, and Lei Bai. 2024. Cra5: Extreme compression of era5 for portable global climate and weather research via an efficient variational transformer. arXiv preprint arXiv:2405.03376 (2024). [19] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. 2018. Deep blending for free-viewpoint image-based ren- dering. ACM Trans. Graph. 37, 6 (2018), 1–15. [20] Zhihao Hu, Zhenghao Chen, Dong Xu, Guo Lu, Wanli Ouyang, and Shuhang Gu. 2020. Improving deep video compression by resolution-adaptive flow coding. In ECCV. Springer, 193–209. [21] Zhihao Hu, Guo Lu, and Dong Xu. 2021. FVC: A new framework towards deep video compression in feature space. In CVPR. 1502–1511. [22] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive mixtures of local experts. Neural computation 3, 1 (1991), 79–87. [23] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical reparametrization with gumble-softmax. In ICLR. [24] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 2023. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Trans. Graph. 42, 4 (2023), 139–1. [25] Diederik P Kingma. 2015. Adam: A method for stochastic optimization. ICLR (2015). [26] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. 2017. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Trans. Graph. 36, 4 (2017), 1–13. [27] Jooyoung Lee, Seyoon Jeong, and Munchurl Kim. 2022. Selective compression learning of latent representations for variable-rate image compression. NeurIPS 35 (2022), 13146–13157. [28] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. 2024. Compact 3d gaussian representation for radiance field. In CVPR. 21719–21728. [29] Zhongyue Lei, Xuemin Hong, Jianghong Shi, Minxian Su, Chaoheng Lin, and Wei Xia. 2023. Quantization-Based Adaptive Deep Image Compression Using Semantic Information. IEEE Access 11 (2023), 118061–118077. [30] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2021. Gshard: Scaling giant models with conditional computation and automatic sharding. ICLR (2021). [31] Jiahao Li, Bin Li, and Yan Lu. 2021. Deep Contextual Video Compression. Advances in Neural Information Processing Systems 34 (2021). [32] Jiahao Li, Bin Li, and Yan Lu. 2023. Neural Video Compression with Diverse Contexts. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, Canada, June 18-22, 2023. [33] Jiahao Li, Bin Li, and Yan Lu. 2024. Neural Video Compression with Feature Modulation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 17-21, 2024. [34] Lei Liu, Zhenghao Chen, Zhihao Hu, and Dong Xu. 2025. An Efficient Adaptive Compression Method for Human Perception and Machine Vision Tasks. arXiv preprint arXiv:2501.04329 (2025). [35] Lei Liu, Zhihao Hu, and Zhenghao Chen. 2024. Towards point cloud compression for machine perception: A simple and strong baseline by learning the octree depth level predictor. In International Joint Conference on Artificial Intelligence WorkShop. Springer, 3–17. [36] Lei Liu, Zhihao Hu, Zhenghao Chen, and Dong Xu. 2023. Icmh-net: Neural image compression towards both machine vision and human vision. In ACM MM. 8047–8056. [37] Lei Liu, Zhihao Hu, and Jing Zhang. 2023. PCHM-Net: A New Point Cloud Compression Framework for Both Human Vision and Machine Vision. In ICME. 1997–2002. [38] Xiangrui Liu, Xinju Wu, Pingping Zhang, Shiqi Wang, Zhu Li, and Sam Kwong. 2024. CompGS: Efficient 3D Scene Representation via Compressed Gaussian Splatting. In ACM MM. [39] Guo Lu, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Chunlei Cai, and Zhiyong Gao. 2019. Dvc: An end-to-end deep video compression framework. In CVPR. 11006–11015. [40] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. 2024. Scaffold-gs: Structured 3d gaussians for view-adaptive rendering. In CVPR. 20654–20664. [41] Jixiang Luo, Yan Wang, and Hongwei Qin. 2024. Super-high-fidelity image compression via hierarchical-roi and adaptive quantization. arXiv preprint arXiv:2403.13030 (2024). [42] David Minnen, Johannes Ballé, and George D Toderici. 2018. Joint autoregressive and hierarchical priors for learned image compression. NeurIPS 31 (2018). [43] Wieland Morgenstern, Florian Barthel, Anna Hilsmann, and Peter Eisert. 2024. Compact 3d scene representation via self-organizing gaussian grids. ECCV (2024). [44] KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. 2023. Compact3d: Compressing gaussian splat radiance field models with vector quantization. arXiv preprint arXiv:2311.18159 (2023). [45] Simon Niedermayr, Josef Stumpfegger, and Rüdiger Westermann. 2024. Com- pressed 3d gaussian splatting for accelerated novel view synthesis. In CVPR. 10349–10358. [46] Panagiotis Papantonakis, Georgios Kopanas, Bernhard Kerbl, Alexandre Lanvin, and George Drettakis. 2024. Reducing the Memory Footprint of 3D Gaussian Splatting. Proceedings of the ACM on Computer Graphics and Interactive Techniques 7, 1 (2024), 1–17. [47] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. ICLR (2017). [48] Gary J Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. 2012. Overview of the high efficiency video coding (HEVC) standard. IEEE TCSVT 22, 12 (2012), 1649–1668. [49] Kedeng Tong, Yaojun Wu, Yue Li, Kai Zhang, Li Zhang, and Xin Jin. 2023. QVRF: A Quantization-Error-Aware Variable Rate Framework for Learned Image Com- pression. In ICIP. 1310–1314. [50] Henan Wang, Hanxin Zhu, Tianyu He, Runsen Feng, Jiajun Deng, Jiang Bian, and Zhibo Chen. 2024. End-to-end rate-distortion optimized 3d gaussian repre- sentation. In ECCV. Springer, 76–92. [51] Yufei Wang, Zhihao Li, Lanqing Guo, Wenhan Yang, Alex Kot, and Bihan Wen. 2024. ContextGS : Compact 3D Gaussian Splatting with Anchor Level Context Model. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/forum?id=W2qGSMl2Uu [52] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. 2004. Image quality assessment: from error visibility to structural similarity. IEEE TIP 13, 4 (2004), 600–612. [53] Guoqing Xiang, Huizhu Jia, Mingyuan Yang, Yuan Li, and Xiaodong Xie. 2018. A novel adaptive quantization method for video coding. Multimedia Tools and Applications 77 (2018), 14817–14840. , , [54] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin. 2022. Bungeenerf: Progressive neural radiance field for extreme multi-scale scene rendering. In ECCV. Springer, 106–122. [55] Shuzhao Xie, Weixiang Zhang, Chen Tang, Yunpeng Bai, Rongwei Lu, Shijia Ge, and Zhi Wang. 2024. Mesongs: Post-training compression of 3d gaussians via efficient attribute transformation. In ECCV. Springer, 434–452. [56] Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu, and Yang You. 2022. Go wider instead of deeper. In AAAI, Vol. 36. 8779–8787. [57] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR. 586–595. [58] Simiao Zuo, Qingru Zhang, Chen Liang, Pengcheng He, Tuo Zhao, and Weizhu Chen. 2022. MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 